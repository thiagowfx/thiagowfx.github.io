[{"content":"You can always use kubectl logs -n {namespace} {pod} [-c {container}] -f to inspect logs from a specific pod1.\nNonetheless that doesn\u0026rsquo;t scale when you don\u0026rsquo;t know which pod you want in the first place.\nYou could start with deployments, dive into replica sets, and then into individual pods, one by one, but\u0026hellip; that is tedious and slow.\nWe can do better with stern:\n⎈ Multi pod and container log tailing for Kubernetes\nUsage:\nstern pod-query [flags] The pod-query is a regular expression or a Kubernetes resource in the form /.\nExample:\n% stern -n argocd argocd We can also grep for specific strings in log lines with --include:\n% stern -n argocd argocd --include level=error Or grep -v with --exclude.\nIt\u0026rsquo;s better to use --include / --exclude than to | grep, because it preserves color attributes (each pod has a different color in the aggregated log output).\nThe logs stream by default (like tail -f, or kubectl logs -f).\n-c to specify a container within it, and -f to stream logs à la tail -f.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/12/kubernetes-tail-logs-from-pods-with-stern/","summary":"\u003cp\u003eYou can always use \u003ccode\u003ekubectl logs -n {namespace} {pod} [-c {container}] -f\u003c/code\u003e to\ninspect logs from a specific pod\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e\n\u003cp\u003eNonetheless that doesn\u0026rsquo;t scale when you don\u0026rsquo;t know which pod you want in the\nfirst place.\u003c/p\u003e","title":"Kubernetes: tail logs from pods with stern"},{"content":"I am deploying a new helm chart to our staging environments, and needed a way to ensure its dependencies in values.yaml are properly configured.\nFrom the official kubernetes documentation:\nAn issue that comes up rather frequently for new installations of Kubernetes is that a Service is not working properly. You\u0026rsquo;ve run your Pods through a Deployment (or other workload controller) and created a Service, but you get no response when you try to access it. This document will hopefully help you to figure out what\u0026rsquo;s going wrong.\nRun commands in a brand new pod % kubectl run -it --rm --restart=Never busybox --image=gcr.io/google-containers/busybox sh \u0026hellip;quite verbose, but it works reliably, spawning a basic pod with busybox.\nI prefer to have a package manager at my fingertips, hence alpine is arguably a better choice:\n% kubectl run -it --rm --restart=Never busybox --image=alpine ash You don\u0026rsquo;t need to remember that alpine uses ash by default, sh works as well. In fact:\n/ # ls -al /bin | grep sh lrwxrwxrwx 1 root root 12 Dec 5 12:17 ash -\u0026gt; /bin/busybox lrwxrwxrwx 1 root root 12 Dec 5 12:17 fdflush -\u0026gt; /bin/busybox lrwxrwxrwx 1 root root 12 Dec 5 12:17 sh -\u0026gt; /bin/busybox Then I could do, for example, apk update + apk add mtr nmap.\nAlpine is marvellous for this because apk is very lightweight (and, hence, fast!) and the available packages from upstream are quite comprehensive.\nRun commands in an already existing pod % kubectl exec \u0026lt;POD-NAME\u0026gt; -c \u0026lt;CONTAINER-NAME\u0026gt; -- \u0026lt;COMMAND\u0026gt; \u0026hellip;however you cannot choose a custom image this way. You\u0026rsquo;re stuck with whatever the pod is running.\nThe way around this is with the use of ephemeral debug containers. Newer versions of kubernetes have the kubectl debug command:\n% kubectl debug -it \u0026lt;POD-NAME\u0026gt; --image=alpine [--target=\u0026lt;POD-NAME\u0026gt;] Happy debugging!\n","permalink":"https://www.perrotta.dev/2024/12/kubernetes-debugging-services/","summary":"\u003cp\u003eI am deploying a new helm chart to our staging environments, and needed a way to\nensure its dependencies in \u003ccode\u003evalues.yaml\u003c/code\u003e are properly configured.\u003c/p\u003e\n\u003cp\u003eFrom the official kubernetes\n\u003ca href=\"https://kubernetes.io/docs/tasks/debug/debug-application/debug-service/\"\u003edocumentation\u003c/a\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eAn issue that comes up rather frequently for new installations of Kubernetes\nis that a Service is not working properly. You\u0026rsquo;ve run your Pods through a\nDeployment (or other workload controller) and created a Service, but you get\nno response when you try to access it. This document will hopefully help you\nto figure out what\u0026rsquo;s going wrong.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Kubernetes: debugging services"},{"content":"Just is a command runner, a modern replacement for GNU Make.\nIt is written in Rust, has sensible defaults, and lots of syntactic sugar. A good analogy is fish versus bash when comparing just to make.\nIt\u0026rsquo;s very easy to learn from its README.md alone as it\u0026rsquo;s quite comprehensive. There\u0026rsquo;s also a gitbook.\nSimon Willison prompted me to try it out.\nAs an exercise I decided to convert the Makefile used to manage this blog into a Justfile.\nThe original Makefile:\n# Sitemap URL SITEMAP = https://www.perrotta.dev/sitemap.xml # Hugo port PORT := 1313 # Abort if hugo is not installed. ifeq (, $(shell which hugo)) $(error \u0026#34;No hugo in $$PATH, install it first\u0026#34;) endif all: hugo server --bind=\u0026#34;0.0.0.0\u0026#34; --buildDrafts --port $(PORT) --watch build: hugo --environment production --gc --minify clean: $(RM) -r public/ resources/ ping: # Ping Google about changes in the sitemap curl -sS -o /dev/null \u0026#34;https://www.google.com/ping?sitemap=$(SITEMAP)\u0026#34; # Ping Bing (DuckDuckGo, etc) about changes in the sitemap curl -sS -o /dev/null \u0026#34;https://www.bing.com/ping?sitemap=$(SITEMAP)\u0026#34; .PHONY: all build clean ping Initially I asked ChatGPT to convert it to a Justfile but it was a disaster, even after a couple of iterations. Then I did it myself1. The Justfile:\nset dotenv-load watch: hugo server --buildDrafts --port ${PORT:-1313} --watch build: hugo --environment production --gc --minify # Create a new post. Usage: `just new \u0026#34;advent of code day 8\u0026#34;` new post: hugo new content/posts/`date \u0026#34;+%Y-%m-%d\u0026#34;`-{{ kebabcase(post) }}.md clean: rm -rf public/ resources/ # Ping Google and Bing about changes in the sitemap ping sitemap=\u0026#34;https://www.perrotta.dev/sitemap.xml\u0026#34;: curl -sS -o /dev/null \u0026#34;https://www.google.com/ping?sitemap={{ sitemap }}\u0026#34; curl -sS -o /dev/null \u0026#34;https://www.bing.com/ping?sitemap={{ sitemap }}\u0026#34; The main differences:\nEnvironment variables: use {{ foo }} instead of $(FOO) Exception: environment variables loaded from .env (via set dotenv-load) use $FOO or ${FOO} instead, like POSIX shell variables Use rm -rf instead of $(RM) -r Rules accept parameters. Look at new post as an example. Example usage: just new \u0026quot;advent of code day 8\u0026quot; Run shell commands within rules with backticks. $(cmd) does not work. Some handy out-of-the-box functions such as kebabcase(). No need to implement this kind of string manipulation in plain shell script! Documentation comments above rules are recognized. They are displayed as help / usage text when running just -l. No need for hacky self-documented Makefile setups! % just -l Available recipes: build clean new post # Create a new post. Usage: `just new \u0026#34;advent of code day 8\u0026#34;` ping sitemap=\u0026#34;https://www.perrotta.dev/sitemap.xml\u0026#34; # Ping Google and Bing about changes in the sitemap watch With a bit of LLM prompting in lieu of Google or Stack Overflow searches.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/12/just/","summary":"\u003cp\u003e\u003ca href=\"https://github.com/casey/just\"\u003eJust\u003c/a\u003e is a command runner, a modern replacement\nfor GNU Make.\u003c/p\u003e","title":"★ Just"},{"content":"Link to Day #7 puzzle.\nPart one: dynamic programming!\ndef calibrate_one(test_value, operands): @lru_cache(maxsize=None) def dp_calibrate(acc, index): if acc == test_value and index == len(operands): return True if acc \u0026gt; test_value or index == len(operands): return False return dp_calibrate(acc + operands[index], index + 1) or dp_calibrate(acc * operands[index], index + 1) return dp_calibrate(operands[0], 1) I learned this style from NeetCode. An inner function to drive the memoization alongside lru_cache. Beautiful!\nIn my initial solution I passed the whole operands tuple as the second argument. Later on I changed it to pass the current index in the tuple, which is simpler.\nPart two is a natural extension of part one, with an extra operator:\ndef calibrate_two(test_value, operands): @lru_cache(maxsize=None) def dp_calibrate(acc, index): if index == len(operands): return acc == test_value if acc \u0026gt; test_value: return False return dp_calibrate(acc + operands[index], index + 1) or dp_calibrate(acc * operands[index], index + 1) or dp_calibrate(int(str(acc) + str(operands[index])), index + 1) return dp_calibrate(operands[0], 1) The full solution:\n#!/usr/bin/env python3 import sys from functools import lru_cache def calibrate_one(test_value, operands): @lru_cache(maxsize=None) def dp_calibrate(acc, index): if acc == test_value and index == len(operands): return True if acc \u0026gt; test_value or index == len(operands): return False return dp_calibrate(acc + operands[index], index + 1) or dp_calibrate(acc * operands[index], index + 1) return dp_calibrate(operands[0], 1) def calibrate_two(test_value, operands): @lru_cache(maxsize=None) def dp_calibrate(acc, index): if index == len(operands): return acc == test_value if acc \u0026gt; test_value: return False return dp_calibrate(acc + operands[index], index + 1) or dp_calibrate(acc * operands[index], index + 1) or dp_calibrate(int(str(acc) + str(operands[index])), index + 1) return dp_calibrate(operands[0], 1) def main(): with open(sys.argv[1]) as input: lines = input.read().splitlines() total_one = total_two = 0 for line in lines: test_value, operands = line.split(\u0026#39;:\u0026#39;) test_value = int(test_value) operands = [int(x) for x in operands.split()] if calibrate_one(test_value, operands): total_one += test_value if calibrate_two(test_value, operands): total_two += test_value # part one print(total_one) # part two print(total_two) if __name__ == \u0026#39;__main__\u0026#39;: main() ","permalink":"https://www.perrotta.dev/2024/12/advent-of-code-day-7/","summary":"\u003cp\u003eLink to \u003ca href=\"https://adventofcode.com/2024/day/7\"\u003eDay #7\u003c/a\u003e puzzle.\u003c/p\u003e","title":"Advent of Code: Day 7"},{"content":"Assuming you are following this year\u0026rsquo;s advent of code, you are welcome to join our Telegram discussion group here1. It is a supergroup, one of its channels is #adventofcode.\nIt\u0026rsquo;s a moderated group, and this invitation link will expire within 2 weeks of the time of this post.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/12/advent-of-code-discussion-group/","summary":"Assuming you are following this year\u0026rsquo;s advent of code, you are welcome to join our Telegram discussion group here1. It is a supergroup, one of its channels is #adventofcode.\nIt\u0026rsquo;s a moderated group, and this invitation link will expire within 2 weeks of the time of this post.\u0026#160;\u0026#x21a9;\u0026#xfe0e;","title":"Advent of Code: discussion group"},{"content":"Link to Day #11 puzzle.\nPart one can be done with a simulation.\nIt is very delightful to do it in python: lists are quite flexible, and converting from integers to strings and vice-versa is seamless. Counting the number of digits of x is just a matter of len(str(x)). In C++ it\u0026rsquo;s a bit kludgier with std::string(x).size() and std::stoi(s), but then you need to remember which header to import1.\nI thought of using reduce to do blink(blink(stone))... and so on:\nprint(len(reduce(lambda stone: blink(stone), range(25), stones))) \u0026hellip;nonetheless it\u0026rsquo;s more readable to simply use a plain for-range loop:\nfor _ in range(25): stones = blink(stones) print(len(stones)) The secret sauce is in blink:\ndef blink(stones): stones_next = [] for stone in stones: s = str(stone) if stone == 0: stones_next.append(1) elif len(s) % 2 == 0: index = len(s) // 2 stones_next.append(int(s[:index])) stones_next.append(int(s[index:])) else: stones_next.append(stone * 2024) return stones_next For part two we need to be cleverer. In principle the same approach would work, however it takes too long to process due to its exponential nature. In my laptop I can get up to the 42nd blink iteration without losing my patience to wait even longer.\nThe main observation to account for is that we only care about the length of the stone sequence, hence the original task transforms into a simple 2D dynamic programming problem.\nI call:\nprint(dp_blink(stones, 75)) Which is defined this way:\ndef dp_blink(stones, times): from functools import lru_cache @lru_cache(maxsize=None) def dp(stone: int, times: int) -\u0026gt; int: if times == 0: return 1 return sum([dp(stone, times - 1) for stone in blink([stone])]) return sum([dp(stone, times) for stone in stones]) The DP consists of the stone, and how many times are left for you to blink at it.\nThe full solution:\n#!/usr/bin/env python3 import sys def blink(stones): stones_next = [] for stone in stones: s = str(stone) if stone == 0: stones_next.append(1) elif len(s) % 2 == 0: index = len(s) // 2 stones_next.append(int(s[:index])) stones_next.append(int(s[index:])) else: stones_next.append(stone * 2024) return stones_next def dp_blink(stones, times): from functools import lru_cache @lru_cache(maxsize=None) def dp(stone: int, times: int) -\u0026gt; int: if times == 0: return 1 return sum([dp(stone, times - 1) for stone in blink([stone])]) return sum([dp(stone, times) for stone in stones]) def main(): with open(sys.argv[1]) as input: lines = input.read().splitlines() stones = [int(stone) for stone in lines[0].split()] for _ in range(25): stones = blink(stones) # part one print(len(stones)) # This is very slow, with an exponential complexity runtime. # What did you expect? # # for i in range(50): # 50 = 75 - 25 # print(i) # stones = blink(stones) # # part two # print(len(stones)) stones = [int(stone) for stone in lines[0].split()] # part two print(dp_blink(stones, 75)) if __name__ == \u0026#39;__main__\u0026#39;: main() It\u0026rsquo;s #include \u0026lt;string\u0026gt;.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/12/advent-of-code-day-11/","summary":"\u003cp\u003eLink to \u003ca href=\"https://adventofcode.com/2024/day/11\"\u003eDay #11\u003c/a\u003e puzzle.\u003c/p\u003e","title":"Advent of Code: Day 11"},{"content":"Link to Day #6 puzzle.\nPart one is a classic 2D matrix (graph) traversal problem.\nTo store state I created a visited set with the (x, y) coordinates. Alternatively I could have changed the input inplace, but I didn\u0026rsquo;t want to deal with the immutability of python strings, i.e. given:\nl = [\u0026#34;.....\u0026#34;] \u0026hellip;you can\u0026rsquo;t simply do l[0][0] = 'X', because python strings are immutable. We could define a new string and assign it to l[0], or we could change the input to:\nl = [\u0026#39;.\u0026#39;, \u0026#39;.\u0026#39;, \u0026#39;.\u0026#39;, \u0026#39;.\u0026#39;, \u0026#39;.\u0026#39;] \u0026hellip;so that replacing characters becomes trivial.\nThe full solution:\n#!/usr/bin/env python3 import sys def find(lines, c): for i, line in enumerate(lines): if c in line: return i, line.index(c) raise ValueError(f\u0026#39;Could not find {c} in lines\u0026#39;) def move(pos, dir, lines, visited): dirs_clockwise = ((1, 0), (0, -1), (-1, 0), (0, 1)) while True: next_pos = pos[0] + dir[0], pos[1] + dir[1] if next_pos[0] \u0026lt; 0 or next_pos[0] \u0026gt;= len(lines) or next_pos[1] \u0026lt; 0 or next_pos[1] \u0026gt;= len(lines[0]): break if lines[next_pos[0]][next_pos[1]] in \u0026#39;.^\u0026#39;: pos = next_pos visited.add(pos) elif lines[next_pos[0]][next_pos[1]] == \u0026#39;#\u0026#39;: dir = dirs_clockwise[(dirs_clockwise.index(dir) + 1) % 4] def main(): with open(sys.argv[1]) as input: lines = input.read().splitlines() pos = find(lines, \u0026#39;^\u0026#39;) dir = (-1, 0) # up visited = set((pos,)) move(pos, dir, lines, visited) # part one print(len(visited)) if __name__ == \u0026#39;__main__\u0026#39;: main() I did not solve part two yet. I know how to do it, but my initial approach is too brute force to my taste. Perhaps I\u0026rsquo;ll come up with something clever later on.\n","permalink":"https://www.perrotta.dev/2024/12/advent-of-code-day-6/","summary":"\u003cp\u003eLink to \u003ca href=\"https://adventofcode.com/2024/day/6\"\u003eDay #6\u003c/a\u003e puzzle.\u003c/p\u003e","title":"Advent of Code: Day 6"},{"content":"When working with semantic versioning a.k.a. semver, it\u0026rsquo;s often useful to use range constraints1 to express the versions you\u0026rsquo;re interested in.\nThe most useful ones are the tilde (~) and the circumflex / hat / caret (^)2:\n~4.2.1: picks up 4.2.x for x \u0026gt;= 1 ^5.0.2: picks up 5.x.y for x \u0026gt;= 0 and y \u0026gt;= 2 Popular software tooling that uses them include:\nnodejs / npm packages terraform ruby gems (here ~\u0026gt; is used instead of simply ~) Sometimes I use https://jubianchi.github.io/semver-check to double-check3, or to teach teammates how the range constraints work.\nOr constraint ranges? 🤔\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIn Portuguese we say \u0026ldquo;acento circunflexo\u0026rdquo; (circumflex accent). In English, AFAIK, \u0026ldquo;caret\u0026rdquo; is the most precise term however people typically refer to it as \u0026ldquo;hat\u0026rdquo;.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nExample.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/12/semantic-versioning-constraints/","summary":"\u003cp\u003eWhen working with semantic versioning a.k.a. \u003ca href=\"https://semver.org/\"\u003esemver\u003c/a\u003e, it\u0026rsquo;s\noften useful to use range constraints\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e to express the versions you\u0026rsquo;re\ninterested in.\u003c/p\u003e\n\u003cp\u003eThe most useful ones are the tilde (\u003ccode\u003e~\u003c/code\u003e) and the circumflex / hat / caret\n(\u003ccode\u003e^\u003c/code\u003e)\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e~4.2.1\u003c/code\u003e: picks up \u003ccode\u003e4.2.x\u003c/code\u003e for \u003ccode\u003ex \u0026gt;= 1\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e^5.0.2\u003c/code\u003e: picks up \u003ccode\u003e5.x.y\u003c/code\u003e for \u003ccode\u003ex \u0026gt;= 0\u003c/code\u003e and \u003ccode\u003ey \u0026gt;= 2\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ePopular software tooling that uses them include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003enodejs / npm packages\u003c/li\u003e\n\u003cli\u003eterraform\u003c/li\u003e\n\u003cli\u003eruby gems (here \u003ccode\u003e~\u0026gt;\u003c/code\u003e is used instead of simply \u003ccode\u003e~\u003c/code\u003e)\u003c/li\u003e\n\u003c/ul\u003e","title":"Semantic versioning constraints"},{"content":".gitignore is the canonical way to exclude files from your git repository.\nIn some situations, however, you may want to exclude files without adding them to .gitignore, because they are only relevant to you, as opposed to your teammates.\nCommon examples:\n.ackrc: exclude file patterns from search with ack – not everyone in your team may use ack at all .envrc: direnv integration to automatically run a couple of commands whenever cd\u0026lsquo;ing to within the repository – not everyone in your team may use direnv at all \u0026hellip;and so on.\nIs there a way to have a \u0026ldquo;personal\u0026rdquo; .gitignore file? Yes, in fact, many ways!\n1) Per repository Use the .git/info/exclude file instead of .gitignore. Edits in this file are not tracked by version control. The documentation says:\n# git ls-files --others --exclude-from=.git/info/exclude # Lines that start with \u0026#39;#\u0026#39; are comments. # For a project mostly in C, the following would be a good set of # exclude patterns (uncomment them if you want to use them): # *.[oa] # *~ .ackrc .envrc For example, I could add .ackrc to it.\n2) git update-index % git update-index --assume-unchanged .ackrc .envrc If you make a mistake, it can be reversed with --no-assume-unchanged.\n3) Globally This approach takes effect in all repositories.\nSet core.excludesFile in your ~/.gitconfig:\ngit config --global core.excludesFile \u0026#39;~/.gitignore\u0026#39; Now populate it as you normally would your repo .gitignore.\nReferences https://stackoverflow.com/questions/653454/how-do-you-make-git-ignore-files-without-using-gitignore https://gist.github.com/subfuzion/db7f57fff2fb6998a16c ","permalink":"https://www.perrotta.dev/2024/12/.gitignore-without-.gitignore/","summary":"\u003cp\u003e\u003ccode\u003e.gitignore\u003c/code\u003e is the canonical way to exclude files from your git repository.\u003c/p\u003e\n\u003cp\u003eIn some situations, however, you may want to exclude files without adding them\nto \u003ccode\u003e.gitignore\u003c/code\u003e, because they are only relevant to you, as opposed to your\nteammates.\u003c/p\u003e\n\u003cp\u003eCommon examples:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e.ackrc\u003c/code\u003e: exclude file patterns from search with \u003ccode\u003eack\u003c/code\u003e – not everyone in your\nteam may use \u003ccode\u003eack\u003c/code\u003e at all\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e.envrc\u003c/code\u003e: \u003ccode\u003edirenv\u003c/code\u003e integration to automatically run a couple of commands\nwhenever \u003ccode\u003ecd\u003c/code\u003e\u0026lsquo;ing to within the repository – not everyone in your team may use\n\u003ccode\u003edirenv\u003c/code\u003e at all\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u0026hellip;and so on.\u003c/p\u003e","title":".gitignore without .gitignore"},{"content":"Alpine Linux 3.21.0 got released last Thursday.\nSo what? I am credited as an aports commit contributor! :-)\nHere is a list of all the packages I currently maintain, excluding dependencies:\nargocd autotrash bkt fpp gliderlabs-sigil kubeseal tfupdate typos urlwatch yamlfmt \u0026hellip;a total of 10 packages.\n","permalink":"https://www.perrotta.dev/2024/12/alpine-linux-3.21/","summary":"\u003cp\u003e\u003ca href=\"https://alpinelinux.org/posts/Alpine-3.21.0-released.html\"\u003eAlpine Linux 3.21.0\u003c/a\u003e\ngot released last Thursday.\u003c/p\u003e","title":"Alpine Linux 3.21"},{"content":"To build a docker image completely from scratch, without reusing cache layers on your system:\ndocker build --no-cache -f Dockerfile . -t {image name}:{image tag} --platform linux/amd64 Pass an explicit platform in case e.g. you\u0026rsquo;re building on a Mac M1 (arm64) with the intent of running it on Linux in the cloud (amd64 typically). The docker image full specification is: image = {registry}/{repository}:{tag}. For example, the registry could be an AWS ECR URL, whereas the repository is something like \u0026ldquo;bitnami/sealed-secrets\u0026rdquo;, and the tag typically follows semantic versioning. ","permalink":"https://www.perrotta.dev/2024/12/docker-build-ignore-cache/","summary":"\u003cp\u003eTo build a docker image completely from scratch, without reusing cache layers on\nyour system:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edocker build --no-cache -f Dockerfile . -t \u003cspan style=\"color:#f92672\"\u003e{\u003c/span\u003eimage name\u003cspan style=\"color:#f92672\"\u003e}\u003c/span\u003e:\u003cspan style=\"color:#f92672\"\u003e{\u003c/span\u003eimage tag\u003cspan style=\"color:#f92672\"\u003e}\u003c/span\u003e --platform\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003elinux/amd64\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"Docker build: ignore cache"},{"content":"Assume you make a huge change to your git repository, that spawns several file formats – cpp, java, javascript, python, etc.\nIn the end you want to revert the javascript changes, for the sake of splitting your commit into self-contained chunks1.\nI like the following approach2:\n% git checkout -- **/*.js Caveat: It does not include hidden files, or files in hidden directories. Unless\u0026hellip;you set the dotglob option:\n% shopt -s dotglob Note that shopt works in bash, alas not in zsh.\nhttps://sscce.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIn theory, this is a pre-requisite: shopt -s globstar. In practice, it should be the default behavior.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/12/bash-glob-hidden-files-recursively/","summary":"\u003cp\u003eAssume you make a huge change to your git repository, that spawns several file\nformats – cpp, java, javascript, python, etc.\u003c/p\u003e\n\u003cp\u003eIn the end you want to revert the javascript changes, for the sake of splitting\nyour commit into self-contained chunks\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e\n\u003cp\u003eI like the following approach\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% git checkout -- **/*.js\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cstrong\u003eCaveat\u003c/strong\u003e: It does not include hidden files, or files in hidden directories.\nUnless\u0026hellip;you set the \u003ccode\u003edotglob\u003c/code\u003e option:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% shopt -s dotglob\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eNote that \u003ccode\u003eshopt\u003c/code\u003e works in \u003ccode\u003ebash\u003c/code\u003e, alas not in \u003ccode\u003ezsh\u003c/code\u003e.\u003c/p\u003e","title":"bash: glob hidden files recursively"},{"content":"A no-brainer and effective way to increase code quality in an organization is by the means of incorporating linters into your CI/CD pipeline.\nBut where can you find them?\nSome sources of inspiration include curated \u0026ldquo;super-mega-hyper\u0026rdquo; linter packages, such as:\nhttps://megalinter.io https://github.com/super-linter/super-linter I am not generally a fan of employing these collections directly because it\u0026rsquo;s not possible to exert tight control over them, and there\u0026rsquo;s no security or reproducibility guarantee of their pipeline.\nInstead, I cherry-pick interesting / useful linters from their packs, effectively using them as serendipity sources for linters.\nBonus points whenever there\u0026rsquo;s out-of-the-box integration with the pre-commit.com framework i.e. whenever there\u0026rsquo;s a .pre-commit-config.yaml file present at the root of the git repo.\n","permalink":"https://www.perrotta.dev/2024/12/finding-linters/","summary":"\u003cp\u003eA no-brainer and effective way to increase code quality in an organization is by\nthe means of incorporating linters into your CI/CD pipeline.\u003c/p\u003e\n\u003cp\u003eBut where can you find them?\u003c/p\u003e","title":"Finding linters"},{"content":"Link to Day #5 puzzle.\nIt is a topological sort problem, plain and simple.\nIn part one all we care about is whether certain input sequences are valid, within the sort constraints. It\u0026rsquo;s very straightforward to verify that by exhaustively checking all constraints (\u0026ldquo;edges\u0026rdquo;):\nimport sys def is_correct(update, edges): position = defaultdict(lambda: sys.maxsize, {node: i for (i, node) in enumerate(update)}) for (first, second) in edges: if first in update and second in update and position[first] \u0026gt; position[second]: return False return True position is a dictionary representing in which index (position) each element occurs. I make use of a defaultdict with a very large value set by default (instead of a vanilla dict) to avoid the need to explicitly check for element presence.\nIn part two we need to perform the actual topological sort. Or\u0026hellip;do we? Doing toposort would be the most efficient way to resolve it, however, in this case, plain brute force is good enough:\ndef toposort(update, edges): position = defaultdict(lambda: sys.maxsize, {node: i for (i, node) in enumerate(update)}) change = True while change: change = False for (first, second) in edges: if first in update and second in update and position[first] \u0026gt;= position[second]: position[first] = position[second] - 1 change = True return sorted(update, key=lambda x: position[x]) Once again, we iterate through all the input constraints until we find a violation. Whenever we find one, we fix the position of the element in the wrong order by updating it to occur before the other element. We repeat this procedure until there are no more violations.\nThe full source:\n#!/usr/bin/env python3 from collections import defaultdict import sys def is_correct(update, edges): position = defaultdict(lambda: sys.maxsize, {node: i for (i, node) in enumerate(update)}) for (first, second) in edges: if first in update and second in update and position[first] \u0026gt; position[second]: return False return True def toposort(update, edges): position = defaultdict(lambda: sys.maxsize, {node: i for (i, node) in enumerate(update)}) change = True while change: change = False for (first, second) in edges: if first in update and second in update and position[first] \u0026gt;= position[second]: position[first] = position[second] - 1 change = True return sorted(update, key=lambda x: position[x]) def main(): with open(sys.argv[1]) as input: lines = input.read().splitlines() edges = [] updates = [] for line in lines: if \u0026#34;|\u0026#34; in line: edges.append(list(map(int, line.split(\u0026#34;|\u0026#34;)))) elif len(line) == 0: continue else: updates.append(list(map(int, line.split(\u0026#34;,\u0026#34;)))) total_one = total_two = 0 for update in updates: if is_correct(update, edges): total_one += update[len(update) // 2] else: sorted_update = toposort(update, edges) total_two += sorted_update[len(sorted_update) // 2] # part one print(total_one) # part two print(total_two) if __name__ == \u0026#39;__main__\u0026#39;: main() ","permalink":"https://www.perrotta.dev/2024/12/advent-of-code-day-5/","summary":"\u003cp\u003eLink to \u003ca href=\"https://adventofcode.com/2024/day/5\"\u003eDay #5\u003c/a\u003e puzzle.\u003c/p\u003e","title":"Advent of Code: Day 5"},{"content":"Link to Day #4 puzzle.\nIt\u0026rsquo;s a pretty typical 2D matrix search problem, or a graph search problem, if you will.\nThe problem is naturally unraveled into the following searches:\nhorizontally horizontally, reversed vertically vertically, reversed diagonally, all 4 directions (NW, NE, SW, SE) It\u0026rsquo;s possible to write a single pair of for loops that addresses the general case. The (classic) idea is to think of all 8 compass directions to move along the matrix:\n(1, 0) (-1, 0) (0, 1) (0, -1) (1, 1) (-1, -1) (-1, 1) (1, -1) Within the inner iteration, change x += dx and y += dy (or i += di, j += dj, naming is hard). I did this many times in C++ though, and I want to write elegant Python code.\nTherefore I came up with the following solution instead, with nested list comprehensions:\ndef search_horizontal(matrix, keyword): return sum((True for row in matrix for i in range(len(row) - len(keyword) + 1) if \u0026#34;\u0026#34;.join(row[i:i + len(keyword)]) in [keyword, keyword[::-1]])) It follows the same principle as the original intent, however it leverages list slices so that we can omit the dx/dy step.\nThe vertical search is pretty straightforward: it is just a matter of running the horizontal search in the transposed matrix (zip(*matrix)).\nI must confess that using zip to transpose matrices always felt magical and a mere coincidence that it just works™. Ruby has a .transpose method, which is more readable.\nFor the diagonal search, I couldn\u0026rsquo;t think of an elegant list comprehension manner to address it. Is it even possible to \u0026ldquo;2D slice\u0026rdquo; in Python?\nAfter-the-fact I decided to ask ChatGPT, and it is indeed possible, but it requires NumPy:\nIf a is 2-D, returns the diagonal of a with the given offset, i.e., the collection of elements of the form a[i, i+offset]. If a has more than two dimensions, then the axes specified by axis1 and axis2 are used to determine the 2-D sub-array whose diagonal is returned. The shape of the resulting array can be determined by removing axis1 and axis2 and appending an index to the right equal to the size of the resulting diagonals.\nThe method call resembles numpy.array([[1, 2], [3, 4]]).diagonal(offset=1), perhaps with the aid of .flip() to account for the other direction.\nAnyway, my plain diagonal search is:\ndef search_diagonal(matrix, keyword): rows = len(matrix) cols = len(matrix[0]) count = 0 for i in range(rows): for j in range(cols): if i + len(keyword) \u0026lt;= rows and j + len(keyword) \u0026lt;= cols: if \u0026#34;\u0026#34;.join(matrix[i + k][j + k] for k in range(len(keyword))) in [keyword, keyword[::-1]]: count += 1 if i + len(keyword) \u0026lt;= rows and j - len(keyword) \u0026gt;= -1: if \u0026#34;\u0026#34;.join(matrix[i + k][j - k] for k in range(len(keyword))) in [keyword, keyword[::-1]]: count += 1 return count Part two is fundamentally a different problem.\nOne way to address it is to search for all 'A' characters, and then look around its \u0026ldquo;edges\u0026rdquo; to see if they contain exactly two 'M' and two 'S', and that they are properly arranged:\ndef search_double_mas(matrix): rows = len(matrix) cols = len(matrix[0]) count = 0 for i in range(1, rows - 1): for j in range(1, cols - 1): if matrix[i][j] != \u0026#39;A\u0026#39;: continue # look at a QWERTY keyboard to make sense of these variable names q = matrix[i - 1][j - 1] e = matrix[i - 1][j + 1] z = matrix[i + 1][j - 1] c = matrix[i + 1][j + 1] edges = [q, e, z, c] if edges.count(\u0026#39;M\u0026#39;) != 2 or edges.count(\u0026#39;S\u0026#39;) != 2: continue if q == e or q == z: count += 1 return count I couldn\u0026rsquo;t find an opportunity for reuse of the solution from part one.\nThe full solution:\n#!/usr/bin/env python3 import sys def search_horizontal(matrix, keyword): return sum((True for row in matrix for i in range(len(row) - len(keyword) + 1) if \u0026#34;\u0026#34;.join(row[i:i + len(keyword)]) in [keyword, keyword[::-1]])) def search_vertical(matrix, keyword): return search_horizontal(zip(*matrix), keyword) def search_diagonal(matrix, keyword): rows = len(matrix) cols = len(matrix[0]) count = 0 for i in range(rows): for j in range(cols): if i + len(keyword) \u0026lt;= rows and j + len(keyword) \u0026lt;= cols: if \u0026#34;\u0026#34;.join(matrix[i + k][j + k] for k in range(len(keyword))) in [keyword, keyword[::-1]]: count += 1 if i + len(keyword) \u0026lt;= rows and j - len(keyword) \u0026gt;= -1: if \u0026#34;\u0026#34;.join(matrix[i + k][j - k] for k in range(len(keyword))) in [keyword, keyword[::-1]]: count += 1 return count def search_double_mas(matrix): rows = len(matrix) cols = len(matrix[0]) count = 0 for i in range(1, rows - 1): for j in range(1, cols - 1): if matrix[i][j] != \u0026#39;A\u0026#39;: continue # look at a QWERTY keyboard to make sense of these variable names q = matrix[i - 1][j - 1] e = matrix[i - 1][j + 1] z = matrix[i + 1][j - 1] c = matrix[i + 1][j + 1] edges = [q, e, z, c] if edges.count(\u0026#39;M\u0026#39;) != 2 or edges.count(\u0026#39;S\u0026#39;) != 2: continue if q == e or q == z: count += 1 return count def main(): with open(sys.argv[1]) as input: lines = input.read().splitlines() keyword = \u0026#34;XMAS\u0026#34; # [\u0026#39;abcd\u0026#39;, \u0026#39;efgh\u0026#39;, \u0026#39;ijkl\u0026#39;] -\u0026gt; [[\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;], [\u0026#39;e\u0026#39;, \u0026#39;f\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;h\u0026#39;], [\u0026#39;i\u0026#39;, \u0026#39;j\u0026#39;, \u0026#39;k\u0026#39;, \u0026#39;l\u0026#39;]] matrix = [list(line) for line in lines] # part one print(search_horizontal(matrix, keyword) + search_vertical(matrix, keyword) + search_diagonal(matrix, keyword)) # part two print(search_double_mas(matrix)) if __name__ == \u0026#39;__main__\u0026#39;: main() ","permalink":"https://www.perrotta.dev/2024/12/advent-of-code-day-4/","summary":"\u003cp\u003eLink to \u003ca href=\"https://adventofcode.com/2024/day/4\"\u003eDay #4\u003c/a\u003e puzzle.\u003c/p\u003e","title":"Advent of Code: Day 4"},{"content":"When trying to optimize docker images in terms of size, the workflow is as follows:\nTerminal 1 % docker build -f Dockerfile . -t argocd-download-tools \u0026hellip;possibly paired up with entr or fswatch depending on your eagerness for automatic builds.\nTerminal 2 % docker image inspect argocd-download-tools | jq -r \u0026#39;.[0].Size\u0026#39; | numfmt --to=iec 134M \u0026hellip;possibly prepended with watch.\nI wasn\u0026rsquo;t aware of numfmt:\nnumfmt - Convert numbers from/to human-readable strings\n\u0026hellip;instead of displaying plain bytes, it converts numbers to human-readable strings (à la df -h or free -h).\nRegarding the units: they don\u0026rsquo;t matter much, because we are only interested in an approximation, no need to shave bytes off here. SI has 1k = 1000, whereas IEC has 1Ki = 1024.\n","permalink":"https://www.perrotta.dev/2024/12/docker-image-size/","summary":"\u003cp\u003eWhen trying to optimize docker images in terms of size, the workflow is as\nfollows:\u003c/p\u003e","title":"Docker image size"},{"content":"When downloading software from the interwebs, it can come in many popular archive formats: .zip, .tar.gz, .tar.xz, .rar, .7z, etc.\nIf you extract them via the CLI, normally you have to remember the right command and flags to pass for each different format, e.g.:\n% unzip foo.zip % tar xzvf foo.tar.gz [...] It\u0026rsquo;s not too bad but it\u0026rsquo;s unnecessary overhead that can be abstracted away. And there are several ways to do so.\nAt some point I used dtrx:\n% dtrx vault_1.7.2_linux_amd64.zip \u0026hellip;but it used to be Python 2 only. These days there is a Python 3 version but it was too late, at some point I switched to atool:\n% brew ls atool /opt/homebrew/Cellar/atool/0.39.0/bin/acat /opt/homebrew/Cellar/atool/0.39.0/bin/adiff /opt/homebrew/Cellar/atool/0.39.0/bin/als /opt/homebrew/Cellar/atool/0.39.0/bin/apack /opt/homebrew/Cellar/atool/0.39.0/bin/arepack /opt/homebrew/Cellar/atool/0.39.0/bin/atool /opt/homebrew/Cellar/atool/0.39.0/bin/aunpack /opt/homebrew/Cellar/atool/0.39.0/share/man/ (7 files) The only command I use from the package is aunpack, which behaves similarly to dtrx:\n% aunpack vault_1.7.2_linux_amd64.zip atool is globally available in all software repositories I care about.\nAnd then I can free up my mind. With that said, it\u0026rsquo;s still useful to remember the unpacking commands for at the very least .zip and .tar.gz.\n","permalink":"https://www.perrotta.dev/2024/12/atools-unpack-file-archives-agnostically/","summary":"\u003cp\u003eWhen downloading software from the interwebs, it can come in many popular\narchive formats: \u003ccode\u003e.zip\u003c/code\u003e, \u003ccode\u003e.tar.gz\u003c/code\u003e, \u003ccode\u003e.tar.xz\u003c/code\u003e, \u003ccode\u003e.rar\u003c/code\u003e, \u003ccode\u003e.7z\u003c/code\u003e, etc.\u003c/p\u003e\n\u003cp\u003eIf you extract them via the CLI, normally you have to remember the right command\nand flags to pass for each different format, e.g.:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% unzip foo.zip\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% tar xzvf foo.tar.gz\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003e[\u003c/span\u003e...\u003cspan style=\"color:#f92672\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eIt\u0026rsquo;s not too bad but it\u0026rsquo;s unnecessary overhead that can be abstracted away. And\nthere are several ways to do so.\u003c/p\u003e","title":"atools: unpack file archives agnostically"},{"content":"Link to Day #3 puzzle.\nIt\u0026rsquo;s a pretty typical regex problem. To choose not to use regex is to endeavour in pain.\nThe regex for part one to extract all occurrences of mul:\nr\u0026#39;mul\\(\\d+,\\d+\\)\u0026#39; Note that with r there is no need to escape the backslashes in Python.\nLater on I extract the numbers with r'\\d+'.\nIf we really wanted we could do everything with a single regex by using capturing groups, however it would become less readable.\nOnce the numbers are captured, it\u0026rsquo;s just a matter of accumulating their product.\nI craft and test my regex with the support of https://regex101.com/ and then follow up with the Python interpreter in my laptop.\nPart two adds two more operators, which we can easily account for with an or (|).\nThe full solution:\n#!/usr/bin/env python3 import re import sys def main(): with open(sys.argv[1]) as input: lines = input.read().splitlines() prod = prod_two = 0 for memory in lines: ops = re.findall(r\u0026#39;mul\\(\\d+,\\d+\\)\u0026#39;, memory) for op in ops: (f1, f2) = map(int, re.findall(r\u0026#39;\\d+\u0026#39;, op)) prod += f1 * f2 # part one print(prod) enabled = True for memory in lines: ops = re.findall(r\u0026#34;mul\\(\\d+,\\d+\\)|do\\(\\)|don\u0026#39;t\\(\\)\u0026#34;, memory) for op in ops: if \u0026#34;don\u0026#39;t\u0026#34; in op: enabled = False elif \u0026#34;do\u0026#34; in op: enabled = True elif \u0026#39;mul\u0026#39; in op: (f1, f2) = map(int, re.findall(r\u0026#39;\\d+\u0026#39;, op)) if enabled: prod_two += f1 * f2 # part two print(prod_two) if __name__ == \u0026#39;__main__\u0026#39;: main() I intended to use match merely for style points however it\u0026rsquo;s only available from Python 3.10+, thus I sticked with a mere if-elif construct.\n","permalink":"https://www.perrotta.dev/2024/12/advent-of-code-day-3/","summary":"\u003cp\u003eLink to \u003ca href=\"https://adventofcode.com/2024/day/3\"\u003eDay #3\u003c/a\u003e puzzle.\u003c/p\u003e","title":"Advent of Code: Day 3"},{"content":" Me: So where is the Dockerfile source for that image? I can\u0026rsquo;t seem to find it in our git repos.\nCoworker 1: links to an internal page on Confluence, which has the Dockerfile definition1\nCoworker 2: In this case IaC = Infrastructure as Confluence 😄\nto be fair, it\u0026rsquo;s a pretty old image\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/12/iac/","summary":"Me: So where is the Dockerfile source for that image? I can\u0026rsquo;t seem to find it in our git repos.\nCoworker 1: links to an internal page on Confluence, which has the Dockerfile definition1\nCoworker 2: In this case IaC = Infrastructure as Confluence 😄\nto be fair, it\u0026rsquo;s a pretty old image\u0026#160;\u0026#x21a9;\u0026#xfe0e;","title":"IaC"},{"content":"Link to Day #2 puzzle.\nThe first part is straightforward. It felt right to use pairwise to compute the differences between each adjacent pair:\ndiffs = [(b - a) for (a,b) in pairwise(map(int, line.split(\u0026#39; \u0026#39;)))] Then we combine it with all:\ndef is_safe(diffs): return all(1 \u0026lt;= n \u0026lt;= 3 for n in diffs) or all(-3 \u0026lt;= n \u0026lt;= -1 for n in diffs) Note that it is necessary to use two all expressions. It feels tempting to do:\nall(1 \u0026lt;= n \u0026lt;= 3 or -3 \u0026lt;= n \u0026lt;= -1 for n in diffs) \u0026hellip;however that\u0026rsquo;s incorrect. For example: diffs = [1, -1, 1, -1] with an input such as [1, 2, 1, 2] would pass the test even though it shouldn\u0026rsquo;t.\nIt also feels tempting to use abs() but then an additional check would be necessary to ensure the diffs are either all positive or all negative.\nThe second part was trickier.\nInitially I was doing:\nlist(1 \u0026lt;= n \u0026lt;= 3 for n in diffs).count(False) \u0026lt;= 1 or list(-3 \u0026lt;= n \u0026lt;= -1 for n in diffs).count(False) \u0026lt;= 1 \u0026hellip;but then I realized I misunderstood the problem.\nThe 1 2 7 8 9 line, whose diff is [1, 5, 1, 1], illustrates it well: in principle it would pass the test by dropping \u0026ldquo;5\u0026rdquo; from the diff. However, that cannot be correct, because 2 -\u0026gt; 8 is too big of a jump.\nThe brute force way is to drop elements one by one, splitting the original list into two, and then checking is_safe in the merged sublists. That would require computing diffs every time, which would yield an O(n^2) solution.\nWe can do better by pre-computing diffs only once, and then adding a bit of manipulation to reconstruct what the merged diffs would be. The end goal is to compute this:\nis_safe(diffs[:i-1] + [l[i+1] - l[i-1]] + diffs[i+1:]): \u0026hellip;i.e. the left part of diffs, the right part of diffs, and a rolling diff element in the middle.\nThe full solution:\n#!/usr/bin/env python3 import sys from itertools import pairwise def main(): with open(sys.argv[1]) as input: lines = input.read().splitlines() safe = 0 safe_damp = 0 def is_safe(diffs): return all(1 \u0026lt;= n \u0026lt;= 3 for n in diffs) or all(-3 \u0026lt;= n \u0026lt;= -1 for n in diffs) for line in lines: l = list(map(int, line.split(\u0026#39; \u0026#39;))) diffs = [(b - a) for (a,b) in pairwise(l)] is_this_safe = is_safe(diffs) if is_this_safe: safe += 1 safe_damp += 1 continue for i in range(len(l)): if i == 0: if is_safe(diffs[1:]): safe_damp += 1 break elif i == len(l) - 1: if is_safe(diffs[:-1]): safe_damp += 1 break else: if is_safe(diffs[:i-1] + [l[i+1] - l[i-1]] + diffs[i+1:]): safe_damp += 1 break # part one print(safe) # part two print(safe_damp) if __name__ == \u0026#39;__main__\u0026#39;: main() A few notes:\nthere\u0026rsquo;s no need for pairwise; a plain for-range loop would have done the job just fine; pairwise is stylish though 1 \u0026lt;= n \u0026lt;= 3 is syntactic sugar for 1 \u0026lt;= n and n \u0026lt;= 3. Python is sweet. part two could become a bit more elegant by introducing another helper function naming is hard pairwise is only available from Python 3.10+. macOS 15 (Sequoia) ships with Python 3.9. Oh well\u0026hellip;I needed to use the Python binary from homebrew. ","permalink":"https://www.perrotta.dev/2024/12/advent-of-code-2024-day-2/","summary":"\u003cp\u003eLink to \u003ca href=\"https://adventofcode.com/2024/day/2\"\u003eDay #2\u003c/a\u003e puzzle.\u003c/p\u003e","title":"Advent of Code 2024: Day 2"},{"content":"Refer to the previous post about AoC, and to the git repository with my solutions in Python 3.\nLink to Day #1 puzzle.\nThis is just a warm-up.\nGiven two lists of integers, iterate over them and sum the absolute difference between each pair. zip + sum is the perfect pair1 for the job.\nPart two: iterate over the left list whilst accumulating how often the element appears in the right list. \u0026ldquo;How often\u0026rdquo; has, almost always, the smell of a Counter.\nThe full solution2:\n#!/usr/bin/env python3 import sys from collections import Counter def main(): with open(sys.argv[1]) as input: lines = input.read().splitlines() left = [] right = [] for line in lines: l, r = map(int, line.split()) left.append(l) right.append(r) left.sort() right.sort() # part one print(sum(abs(l - r) for (l, r) in zip(left, right))) freqs = Counter(right) # part two print(sum(l * freqs[l] for l in left)) if __name__ == \u0026#39;__main__\u0026#39;: main() I\u0026rsquo;m not sure I like it, And I‘m so tired of fighting\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe git repository is ever-evolving and the source of truth, whereas the blog post is a snapshot. I\u0026rsquo;ll experiment with cross-posting solutions here even though it duplicates the repository ones.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/12/advent-of-code-2024-day-1/","summary":"\u003cp\u003eRefer to the \u003ca href=\"https://www.perrotta.dev/2022/01/advent-of-code/\"\u003eprevious post\u003c/a\u003e about AoC,\nand to the \u003ca href=\"https://github.com/thiagowfx/adventofcode\"\u003egit repository\u003c/a\u003e with my\nsolutions in Python 3.\u003c/p\u003e\n\u003cp\u003eLink to \u003ca href=\"https://adventofcode.com/2024/day/1\"\u003eDay #1\u003c/a\u003e puzzle.\u003c/p\u003e","title":"Advent of Code 2024: Day 1"},{"content":"All hail to Apple Documentation.\nThe instructions are very simple:\nopen the .pdf file in Preview.app export it (File \u0026gt; Export) in \u0026ldquo;quartz filter\u0026rdquo;, select the \u0026ldquo;reduce file size\u0026rdquo; option save I observed a scanned document with ~3.3MiB of file size be compressed to ~350KiB. Not bad! And although not very customizable, it\u0026rsquo;s very user-friendly and easy to remember.\nThe alternative would have been to use GhostScript (gs). For example, ChatGPT suggests:\n% gs -sDEVICE=pdfwrite -dCompatibilityLevel=1.4 -dPDFSETTINGS=/screen -dNOPAUSE -dQUIET -dBATCH -sOutputFile=output.pdf input.pdf ","permalink":"https://www.perrotta.dev/2024/11/compress-a-.pdf-in-macos-with-preview.app/","summary":"\u003cp\u003eAll hail to \u003ca href=\"https://support.apple.com/en-ca/guide/preview/prvw1509/mac\"\u003eApple\nDocumentation\u003c/a\u003e.\u003c/p\u003e","title":"Compress a .pdf in macOS with Preview.app"},{"content":"Today I learned: what is a LKV mit ABS?\nDepending on where you are in Germany, it means exactly one of these:\na truck with ABS brakes (anti-lock braking system) a Leberkäsesemmel mit ein bisschen Senf1 – or, if you will, \u0026ldquo;LeberKäsWecken mit ABissleSenf\u0026rdquo; And, of course, there\u0026rsquo;s lore.\nEdit: And, of course, there\u0026rsquo;s a typo. Because it\u0026rsquo;s much easier to write LKV instead of LKW.\na liver cheese roll with a bit of mustard\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/11/lkv-mit-abs/","summary":"\u003cp\u003eToday I learned: what is a LKV mit ABS?\u003c/p\u003e","title":"LKV mit ABS"},{"content":"We often do set -euo pipefail in shell scripts1. It\u0026rsquo;s an overall good habit in defensive programming.\nNonetheless, sometimes the default behavior for pipefail is desired.\nRecently we had an example involving pre-commit.com and mdsh, consider this README.md file:\n`\u0026gt; files=$(grep -rl \u0026#39;/spec/sources/0/targetRevision\u0026#39; --include=\u0026#34;*.yaml\u0026#34; apps/overlays | sort -d -u) \u0026amp;\u0026amp; if [ -z \u0026#34;$files\u0026#34; ]; then echo \u0026#34;None\u0026#34;; else echo \u0026#34;$files\u0026#34; | sed -e \u0026#39;s/^/- /\u0026#39;; fi` The intention is to find all .yaml files that match the given string within the apps/overlays directory.\nThe result is added to the README.md file with a git pre-commit hook, which runs mdsh underneath, proudly showcasing a glimpse of literate programming.\nWhat happens when there are no matches, and grep returns with exit code 1?\n`\u0026gt; files=$(grep -rl \u0026#39;/spec/sources/0/targetRevision\u0026#39; --include=\u0026#34;*.yaml\u0026#34; apps/overlays | sort -d -u) \u0026amp;\u0026amp; if [ -z \u0026#34;$files\u0026#34; ]; then echo \u0026#34;None\u0026#34;; else echo \u0026#34;$files\u0026#34; | sed -e \u0026#39;s/^/- /\u0026#39;; fi` ERROR: some commands failed: `\u0026gt; files=$(grep -rl \u0026#39;/spec/syncPolicy/automated\u0026#39; --include=\u0026#34;*.yaml\u0026#34; apps/overlays | sort -d -u) \u0026amp;\u0026amp; if [ -z \u0026#34;$files\u0026#34; ]; then echo \u0026#34;None\u0026#34;; else echo \u0026#34;$files\u0026#34; | sed -e \u0026#39;s/^/- /\u0026#39;; fi` failed with exit status: 1. Of course it fails, because mdsh sets -o pipefail.\nThe desired behavior is to disable it: set +o pipefail. Then it works properly:\n\u0026gt; set +o pipefail \u0026amp;\u0026amp; files=$(grep -rl \u0026#39;/spec/syncPolicy/automated\u0026#39; --include=\u0026#34;*.yaml\u0026#34; apps/overlays | sort -d -u) \u0026amp;\u0026amp; if [ -z \u0026#34;$files\u0026#34; ]; then echo \u0026#34;None\u0026#34;; else echo \u0026#34;$files\u0026#34; | sed -e \u0026#39;s/^/- /\u0026#39;; fi A gist with an explanation.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/11/bash-disable-pipefail/","summary":"\u003cp\u003eWe often do \u003ccode\u003eset -euo pipefail\u003c/code\u003e in shell scripts\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e.\nIt\u0026rsquo;s an overall good habit in defensive programming.\u003c/p\u003e\n\u003cp\u003eNonetheless, sometimes the default behavior for \u003ccode\u003epipefail\u003c/code\u003e is desired.\u003c/p\u003e","title":"bash: disable pipefail"},{"content":"Assume you use TODO and DOING for workflow tracking in Logseq.\nAssume you use an #instrospect tag.\nProblem statement: how to define a single page / live query that aggregates all of the above?\n{{query (OR (task TODO DOING) [[introspect]])}} Unraveling:\n[[introspect]] refers to the #introspect tag A live query can be added with /query, which is represented as {{query }} under the hood. More information: check out the logseq hub.\n","permalink":"https://www.perrotta.dev/2024/11/logseq-backlog/","summary":"\u003cp\u003eAssume you use \u003ccode\u003eTODO\u003c/code\u003e and \u003ccode\u003eDOING\u003c/code\u003e for workflow tracking in\n\u003ca href=\"https://logseq.com/\"\u003eLogseq\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAssume you use an \u003ccode\u003e#instrospect\u003c/code\u003e tag.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eProblem statement\u003c/strong\u003e: how to define a single page / live query that aggregates\nall of the above?\u003c/p\u003e","title":"logseq: backlog"},{"content":"A productivity booster whenever trying out new pre-commit.com hooks from the wild:\nUsage: pre-commit try-repo {git-repo} [--all-files]\nExample:\n% pre-commit try-repo https://github.com/mrtazz/checkmake [INFO] Initializing environment for https://github.com/mrtazz/checkmake. =============================================================================== Using config: =============================================================================== repos: - repo: https://github.com/mrtazz/checkmake rev: bd26d7905e47713ff0bf3b0e5e7b9c55f0d24e53 hooks: - id: checkmake - id: checkmake-system =============================================================================== [WARNING] Unstaged files detected. [INFO] Stashing unstaged files to /var/folders/yr/6sw3yylx6gjcy5jr38d6j6000000gn/T/tmpakaoxt10/patch1732620344-2186. [INFO] Installing environment for https://github.com/mrtazz/checkmake. [INFO] Once installed this environment will be reused. [INFO] This may take a few minutes... Makefile linter/analyzer.............................(no files to check)Skipped Makefile linter/analyzer.............................(no files to check)Skipped [INFO] Restored changes from /var/folders/yr/6sw3yylx6gjcy5jr38d6j6000000gn/T/tmpakaoxt10/patch1732620344-2186. Previously I would manually add the repo: entry with rev: HEAD and then run pre-commit run --all-files {hook-id}, one by one, adjusting as needed.\nThis new workflow is much faster though, and it is a native pre-commit command!\nInspiration to adopt more pre-commit hooks: all-repos.yaml\n","permalink":"https://www.perrotta.dev/2024/11/pre-commit-try-repo/","summary":"\u003cp\u003eA productivity booster whenever trying out new\n\u003ca href=\"https://pre-commit.com/\"\u003epre-commit.com\u003c/a\u003e hooks from the wild:\u003c/p\u003e","title":"pre-commit: try-repo"},{"content":"Deeper into the rabbit hole:\n% fd Dockerfile.dockerignore | xargs -n 1 rename \u0026#39;s/Dockerfile\\.dockerignore/.dockerignore/\u0026#39; The pre-commit identify library currently mistags these files as Dockerfiles, even though they are not1, which creates all sorts of issues.\nThey are akin to .gitignore files in terms of structure.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/11/rename-files-in-bulk-cont/","summary":"\u003cp\u003eDeeper into the \u003ca href=\"https://www.perrotta.dev/2024/06/rename-files-in-bulk/\"\u003erabbit hole\u003c/a\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e% fd Dockerfile.dockerignore | xargs -n \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e rename \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;s/Dockerfile\\.dockerignore/.dockerignore/\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThe pre-commit \u003ca href=\"https://github.com/pre-commit/identify\"\u003eidentify\u003c/a\u003e library\ncurrently mistags these files as Dockerfiles, even though they are not\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e,\nwhich creates all sorts of issues.\u003c/p\u003e","title":"Rename files in bulk (cont)"},{"content":"To illustrate, consider TypeScript files (*.ts).\nRun:\n% fd -e .ts -x chmod -x References:\nfd: find on steroids: https://github.com/sharkdp/fd -x: execute the given command on all matched files You could also use classic find:\n% find . -type f -name \u0026#39;*.ts\u0026#39; -exec chmod -x {} \\; Or:\n% find . -type f -name \u0026#39;*.ts\u0026#39; | xargs chmod -x Or, with more style safety:\n% find . -type f -name \u0026#39;*.ts\u0026#39; -print0 | xargs -0 -n 1 chmod -x ","permalink":"https://www.perrotta.dev/2024/11/remove-the-executable-bit-from-all-files-with-a-given-extension/","summary":"To illustrate, consider TypeScript files (*.ts).\nRun:\n% fd -e .ts -x chmod -x References:\nfd: find on steroids: https://github.com/sharkdp/fd -x: execute the given command on all matched files You could also use classic find:\n% find . -type f -name \u0026#39;*.ts\u0026#39; -exec chmod -x {} \\; Or:\n% find . -type f -name \u0026#39;*.ts\u0026#39; | xargs chmod -x Or, with more style safety:\n% find . -type f -name \u0026#39;*.","title":"Remove the executable bit from all files with a given extension"},{"content":"When working with git, sometimes we make PEBKAC mistakes and end up losing a given checkout or commit or worktree.\ngit reflog can often help recover from these mistakes. It is a bit difficult to understand, but quite straightforward to use:\n% git reflog | grep {branch_name} For example:\n% git reflog | grep -i \u0026#39;thiagowfx/global-services\u0026#39; Enjoy a real example:\n% git reflog | grep -i \u0026#39;thiagowfx/global-services\u0026#39; 7ca9ac6de77 HEAD@{0}: checkout: moving from 8882c1a128dd2cf35aa2188def3d176c7c15d2f1 to thiagowfx/global-services-docker 8882c1a128d HEAD@{1}: checkout: moving from thiagowfx/global-services-docker to 8882c1a12 7fcc128d2ea HEAD@{15}: rebase (finish): returning to refs/heads/thiagowfx/global-services-docker 16fcb83d888 HEAD@{20}: rebase (finish): returning to refs/heads/thiagowfx/global-services-docker 431509d9d0a HEAD@{24}: rebase (abort): returning to refs/heads/thiagowfx/global-services-docker ee408b6e38c HEAD@{30}: checkout: moving from master to thiagowfx/global-services-docker 3babf6588f3 HEAD@{55}: checkout: moving from thiagowfx/global-services-docker to thiagowfx/base-global-services-image-tag ee408b6e38c HEAD@{57}: rebase (abort): returning to refs/heads/thiagowfx/global-services-docker ee408b6e38c HEAD@{60}: checkout: moving from master to thiagowfx/global-services-docker 3ef5b7c3844 HEAD@{77}: checkout: moving from thiagowfx/global-services-docker to thiagowfx/check-executables-have-shebangs ee408b6e38c HEAD@{78}: checkout: moving from master to thiagowfx/global-services-docker 87f6bb2d95d HEAD@{86}: checkout: moving from thiagowfx/global-services-docker to thiagowfx/actionlint 8d07b53fd47 HEAD@{89}: rebase (finish): returning to refs/heads/thiagowfx/global-services-docker c21089ab78c HEAD@{95}: rebase (finish): returning to refs/heads/thiagowfx/global-services-docker 70941a98ad0 HEAD@{104}: checkout: moving from master to thiagowfx/global-services-docker 2dd03ecdd09 HEAD@{149}: checkout: moving from thiagowfx/global-services-docker to lts13.1 1d62e88a8e2 HEAD@{151}: rebase (finish): returning to refs/heads/thiagowfx/global-services-docker c9c2b36b058 HEAD@{166}: checkout: moving from master to thiagowfx/global-services-docker 0faa6f637b6 HEAD@{281}: checkout: moving from thiagowfx/global-services-docker to thiagowfx/pre-commit-trailing-4 c9c2b36b058 HEAD@{282}: checkout: moving from master to thiagowfx/global-services-docker b0521a1a0f7 HEAD@{284}: checkout: moving from thiagowfx/global-services-docker to master 72b137aa4c7 HEAD@{286}: checkout: moving from master to thiagowfx/global-services-docker 81ac79b0c90 HEAD@{316}: checkout: moving from thiagowfx/global-services-docker to master baef8a5eda1 HEAD@{319}: rebase (finish): returning to refs/heads/thiagowfx/global-services-docker 81ac79b0c90 HEAD@{343}: checkout: moving from master to thiagowfx/global-services-docker How to find the correct checkout?\nThe easiest (albeit tedious) way is by brute-forcing your search in the output:\n% git show 7ca9ac6de77 % git show 8882c1a128d [...] \u0026hellip;until you find the diff you are looking for1.\nFrom there, either git checkout (perhaps with the aid of git worktree) or copy-and-paste (maybe with git apply).\nI am using the term \u0026ldquo;snowflake commit\u0026rdquo; in the title but it\u0026rsquo;s not really idiomatic. The meaning is roughly a sand grain in the beach, or a snowflake in the snow.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/11/git-recover-a-snowflake-commit/","summary":"\u003cp\u003eWhen working with \u003ccode\u003egit\u003c/code\u003e, sometimes we make\n\u003ca href=\"https://en.wikipedia.org/wiki/User_error\"\u003ePEBKAC\u003c/a\u003e mistakes and end up losing a\ngiven checkout or commit or worktree.\u003c/p\u003e","title":"git: recover a snowflake commit"},{"content":"When working with pre-commit.com and specifying language: golang for a given hook, you may want to install dependencies as part of the hook bootstrapping process.\nRecently I needed to do so for yq1:\nThe .pre-commit-config.yaml looked like this:\nrepos: - repo: local hooks: - id: helm-dirname-must-match-chart-name name: Helm chart directory name must match the chart name files: /Chart\\.(yml|yaml)$ entry: ci/helm_check_match_dirname_chart_name.sh language: golang additional_dependencies: - https://github.com/mikefarah/yq Context for the hook and the script: https://stackoverflow.com/q/79166730/1745064.\nIt didn\u0026rsquo;t work. It\u0026rsquo;s necessary to drop the https:// prefix.\nrepos: - repo: local hooks: - id: helm-dirname-must-match-chart-name name: Helm chart directory name must match the chart name files: /Chart\\.(yml|yaml)$ entry: ci/helm_check_match_dirname_chart_name.sh language: golang additional_dependencies: - github.com/mikefarah/yq That didn\u0026rsquo;t work either. Then I realized I needed to specify an exact version2:\nrepos: - repo: local hooks: - id: helm-dirname-must-match-chart-name name: Helm chart directory name must match the chart name files: /Chart\\.(yml|yaml)$ entry: ci/helm_check_match_dirname_chart_name.sh language: golang additional_dependencies: - github.com/mikefarah/yq@v4.44.3 It also didn\u0026rsquo;t work! There was an error message about the need to specify /v4 in the path for whatever reason:\nrepos: - repo: local hooks: - id: helm-dirname-must-match-chart-name name: Helm chart directory name must match the chart name files: /Chart\\.(yml|yaml)$ entry: ci/helm_check_match_dirname_chart_name.sh language: golang additional_dependencies: - github.com/mikefarah/yq/v4@v4.44.3 That worked! Test it:\npre-commit run --all-files helm-dirname-must-match-chart-name [--verbose] yq is like jq for YAML instead of JSON.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n@latest would also work. However, for the sake of reproducibility, pinning is more reliable.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/11/pre-commit-additional-dependencies-in-golang/","summary":"\u003cp\u003eWhen working with \u003ca href=\"https://pre-commit.com/\"\u003epre-commit.com\u003c/a\u003e and specifying\n\u003ccode\u003elanguage: golang\u003c/code\u003e for a given hook, you may want to install dependencies as\npart of the hook bootstrapping process.\u003c/p\u003e","title":"pre-commit: additional dependencies in golang"},{"content":"I am a big fan of zoxide:\nA smarter cd command. Supports all major shells.\nThe idea is simple: as you keep cding during the day, it builds up a local database of your most frequently accessed directories.\nPerhaps you cd often to ~/projects/company/major/area/foo.\nNext time, run z foo, and it will magically cd into it.\nWhat if you have ~/Downloads/foo as well? Then it will cd to the one you access more frequently.\nIt does so transparently and dynamically, out-of-the-box, without any extra intervention.\nSometimes though it cds to a directory you didn\u0026rsquo;t intend.\nTo instruct it to cd to a subdirectory within $PWD (the current working directory), just pass a slash (/) as its second argument:\n~/projects/company$ z foo / In the previous example, it would not cd to ~/Downloads.\nThere are many similar projects to zoxide. In the past, I used autojump.\n","permalink":"https://www.perrotta.dev/2024/11/zoxide-cd-within-the-same-directory/","summary":"\u003cp\u003eI am a big fan of \u003ca href=\"https://github.com/ajeetdsouza/zoxide\"\u003e\u003ccode\u003ezoxide\u003c/code\u003e\u003c/a\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA smarter cd command. Supports all major shells.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThe idea is simple: as you keep \u003ccode\u003ecd\u003c/code\u003eing during the day, it builds up a local\ndatabase of your most frequently accessed directories.\u003c/p\u003e","title":"zoxide: cd within the same directory"},{"content":"Helm charts support the inclusion of a values.schema.json file to validate values.yaml. Documentation: https://helm.sh/docs/topics/charts/#schema-files\nA JSON schema is akin to defining the structure of and type-annotating a JSON file. It helps to \u0026ldquo;shift left\u0026rdquo; the lifecycle of your helm releases.\nFor a pre-existing values.yaml file, it may be quite tedious to generate the schema from scratch, by hand.\nCan we automate this process?\nOf course we can.\nOne possibility is to use GenAI / LLMs. This is out of scope of this post.\nAnother possibility is to leverage a helm plug-in to do so.\nI had a good experience with https://github.com/karuppiah7890/helm-schema-gen:\nhelm plugin install https://github.com/karuppiah7890/helm-schema-gen.git cd path/to/helm/chart helm schema-gen values.yaml | tee values.schema.json Afterwards, test the generated schema with:\nhelm lint path/to/helm/chart Usually I need to do some follow-up edits to the generated file, including:\nadd string enum types for a tighter validation of strings mark certain fields as required mark certain fields with additionalProperties: \u0026quot;false\u0026quot; augment [\u0026quot;null\u0026quot;] to [\u0026quot;null\u0026quot;, \u0026quot;string\u0026quot;] for keys that do not have a default value It\u0026rsquo;s worth to observe that, at the time of this publication, the following notice is present in the repository of the plug-in:\nThis repository has been archived by the owner on Aug 31, 2021. It is now read-only.\nNonetheless, it still works well.\n","permalink":"https://www.perrotta.dev/2024/11/helm-json-schema-generation/","summary":"\u003cp\u003eHelm charts support the inclusion of a \u003ccode\u003evalues.schema.json\u003c/code\u003e file to validate\n\u003ccode\u003evalues.yaml\u003c/code\u003e. Documentation: \u003ca href=\"https://helm.sh/docs/topics/charts/#schema-files\"\u003ehttps://helm.sh/docs/topics/charts/#schema-files\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eA JSON schema is akin to defining the structure of and type-annotating a JSON\nfile. It helps to \u0026ldquo;shift left\u0026rdquo; the lifecycle of your helm releases.\u003c/p\u003e\n\u003cp\u003eFor a pre-existing \u003ccode\u003evalues.yaml\u003c/code\u003e file, it may be quite tedious to generate the\nschema from scratch, by hand.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCan we automate this process?\u003c/strong\u003e\u003c/p\u003e","title":"Helm: JSON schema generation"},{"content":"We had the following code in a CI pipeline:\nfind apps/base/ -type d -exec basename {} \\; | sort | sed -e \u0026#39;s/^/- / It lists all directories in apps/base. We add sort to make the output canonical. The sed part is just to make an unordered list out of it.\nThere was an issue though.\nIn my machine, and in CI (GitHub Actions), the following output was produced:\ngarden-info-export gardenia I use macOS + sort from GNU coreutils via homebrew:\n% which sort /opt/homebrew/opt/coreutils/libexec/gnubin/sort % sort --version sort (GNU coreutils) 9.5 Copyright (C) 2024 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later \u0026lt;https://gnu.org/licenses/gpl.html\u0026gt;. This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Written by Mike Haertel and Paul Eggert. In a coworker’s machine (Ubuntu Linux), it produced the following instead:\ngardenia garden-info-export We had the same locale (en_US.UTF-8), and the coworker was also using sort from GNU coreutils. Puzzling.\nIn order to force a deterministic output, I proposed to use -d. From sort(1):\n-d, --dictionary-order Consider only blank spaces and alphanumeric characters in comparisons. And therein reproducibility was achieved1.\nI do not really know why the outputs were different even with the same locale (LANG, LC_COLLATE, LC_ALL, etc). For future reference, my current locale:\n% locale LANG=\u0026#34;en_US.UTF-8\u0026#34; LC_COLLATE=\u0026#34;en_US.UTF-8\u0026#34; LC_CTYPE=\u0026#34;en_US.UTF-8\u0026#34; LC_MESSAGES=\u0026#34;en_US.UTF-8\u0026#34; LC_MONETARY=\u0026#34;en_US.UTF-8\u0026#34; LC_NUMERIC=\u0026#34;en_US.UTF-8\u0026#34; LC_TIME=\u0026#34;en_US.UTF-8\u0026#34; LC_ALL= And my coworker’s:\n$ locale LANG=en_US.UTF-8 LANGUAGE=en_US LC_CTYPE=\u0026#34;en_US.UTF-8\u0026#34; LC_NUMERIC=en_US.UTF-8 LC_TIME=en_US.UTF-8 LC_COLLATE=\u0026#34;en_US.UTF-8\u0026#34; LC_MONETARY=en_US.UTF-8 LC_MESSAGES=\u0026#34;en_US.UTF-8\u0026#34; LC_PAPER=en_US.UTF-8 LC_NAME=en_US.UTF-8 LC_ADDRESS=en_US.UTF-8 LC_TELEPHONE=en_US.UTF-8 LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=en_US.UTF-8 LC_ALL= In this case, the coworker’s version became the canonical one.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/11/sort-deterministically/","summary":"\u003cp\u003eWe had the following code in a CI pipeline:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003efind apps/base/ -type d -exec basename \u003cspan style=\"color:#f92672\"\u003e{}\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\;\u003c/span\u003e | sort | sed -e \u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u0026#39;\u003c/span\u003es/^/- /\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eIt lists all directories in \u003ccode\u003eapps/base\u003c/code\u003e.\nWe add \u003ccode\u003esort\u003c/code\u003e to make the output canonical.\nThe \u003ccode\u003esed\u003c/code\u003e part is just to make an unordered list out of it.\u003c/p\u003e\n\u003cp\u003eThere was an issue though.\u003c/p\u003e","title":"sort deterministically"},{"content":"Problem statement: Given a helm chart called foo, enforce that its Chart.yaml file lives in a directory called foo1.\nBackground In 2016, this used to be the default behavior in Helm:\nfix(helm): produce error if package name is inconsistent\nIn 2018, this enforcement was removed:\nremove dirname constraint on helm package\nWe would like to reintroduce this requirement in our Helm charts codebase, as a best practice, to prevent chart name collisions.\nWhat would be the most native way to accomplish that?\nI would probably write a git pre-commit hook if there is no native way (e.g. via some helm lint flag).\nSolution Use the following script with pre-commit:\n#!/usr/bin/env bash # # Check that the directory name matches the chart name in Chart.yaml. # # Examples: # - foo/Chart.yaml with \u0026#34;name: hey-foo\u0026#34; fails the check. # - foo/Chart.yaml with \u0026#34;name: foo\u0026#34; passes the check. # # Usage: $0 [path/to/chart/Chart.yaml ...] for chart in \u0026#34;$@\u0026#34;; do dirname=\u0026#34;$(basename \u0026#34;$(dirname \u0026#34;$chart\u0026#34;)\u0026#34;)\u0026#34; # Remove trailing slash. dirname=\u0026#34;${dirname%/}\u0026#34; # Fetch chart name from Chart.yaml. chart_name=\u0026#34;$(yq e \u0026#39;.name\u0026#39; \u0026#34;$chart\u0026#34;)\u0026#34; if [[ $dirname != \u0026#34;$chart_name\u0026#34; ]]; then echo \u0026#34;error: directory name \u0026#39;${dirname}\u0026#39; does not match chart name \u0026#39;${chart_name}\u0026#39;\u0026#34; exit 1 fi done Reference I asked and self-answered this question on Stack Overflow.\nWhy? For ease of management, simplicity, consistency \u0026amp; uniformity.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/11/helm-enforce-the-directory-name-matches-the-chart-name/","summary":"\u003cp\u003e\u003cstrong\u003eProblem statement\u003c/strong\u003e: Given a helm chart called \u003ccode\u003efoo\u003c/code\u003e, enforce that its \u003ccode\u003eChart.yaml\u003c/code\u003e file lives in a directory called \u003ccode\u003efoo\u003c/code\u003e\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e","title":"Helm: enforce the directory name matches the chart name"},{"content":"Given:\n% aws eks --profile global --region us-east-1 list-clusters { \u0026#34;clusters\u0026#34;: [ \u0026#34;aws-us-east-1-global-0\u0026#34; ] } Query:\n% aws eks --profile global --region us-east-1 list-clusters | jq \u0026#39;.clusters\u0026#39; [ \u0026#34;aws-us-east-1-global-0\u0026#34; ] Further:\n% aws eks --profile global --region us-east-1 list-clusters | jq \u0026#39;.clusters[0]\u0026#39; \u0026#34;aws-us-east-1-global-0\u0026#34; What if we wanted to remove the quotes? Add -r (--raw-output):\n% aws eks --profile global --region us-east-1 list-clusters | jq -r \u0026#39;.clusters[0]\u0026#39; aws-us-east-1-global-0 ","permalink":"https://www.perrotta.dev/2024/11/jq-remove-quotes-from-output/","summary":"Given:\n% aws eks --profile global --region us-east-1 list-clusters { \u0026#34;clusters\u0026#34;: [ \u0026#34;aws-us-east-1-global-0\u0026#34; ] } Query:\n% aws eks --profile global --region us-east-1 list-clusters | jq \u0026#39;.clusters\u0026#39; [ \u0026#34;aws-us-east-1-global-0\u0026#34; ] Further:\n% aws eks --profile global --region us-east-1 list-clusters | jq \u0026#39;.clusters[0]\u0026#39; \u0026#34;aws-us-east-1-global-0\u0026#34; What if we wanted to remove the quotes? Add -r (--raw-output):\n% aws eks --profile global --region us-east-1 list-clusters | jq -r \u0026#39;.clusters[0]\u0026#39; aws-us-east-1-global-0 ","title":"jq: remove quotes from output"},{"content":"Oneshot:\n% gem update rubocop That won\u0026rsquo;t update the .gemspec file though. To do so:\nbundler update rubocop Tip: Omitting the gem name has the effect of updating all gems.\n","permalink":"https://www.perrotta.dev/2024/11/ruby-update-gemfile-dependencies/","summary":"Oneshot:\n% gem update rubocop That won\u0026rsquo;t update the .gemspec file though. To do so:\nbundler update rubocop Tip: Omitting the gem name has the effect of updating all gems.","title":"Ruby: update Gemfile dependencies"},{"content":"Problem statement: Given a monorepo on Github with multiple docker images in it, write a github workflow to build and push all of them.\nHere is an abridged version of the resulting workflow. The images are listed one by one for fine-grained control purposes, but it would also be possible to glob them with a single command.\nname: Global services permissions: id-token: write contents: read # Replace with workflow trigger conditions. on: {} jobs: global-services: name: ${{ matrix.image }} runs-on: ubuntu-latest strategy: fail-fast: false matrix: include: # keep-sorted start - dockerfile: path/to/one/Dockerfile image: org/one - dockerfile: path/to/two/Dockerfile image: org/two # keep-sorted end steps: - name: Check out source code uses: actions/checkout@v4 - name: Get git SHA id: git-sha run: | ./scripts/get-versions.sh echo \u0026#34;sha=$(grep \u0026#39;^services_sha\u0026#39; versions.yaml | cut -d\u0026#39; \u0026#39; -f 2)\u0026#34; \u0026gt;\u0026gt; \u0026#34;$GITHUB_OUTPUT\u0026#34; - uses: ./.github/actions/setup-ecr-buildx id: setup-ecr-buildx - name: Build and push global service images uses: docker/build-push-action@6.9.0 with: cache-from: type=gha cache-to: type=gha,mode=max context: path/to file: ${{ matrix.dockerfile }} provenance: false push: true tags: ${{ steps.setup-ecr-buildx.outputs.ecr_registry }}/${{ matrix.image }}:${{ steps.git-sha.outputs.sha }} A matrix strategy kicks off independent multiple build jobs all at once.\nDocs: https://github.com/docker/build-push-action\n","permalink":"https://www.perrotta.dev/2024/11/github-actions-multiple-docker-images/","summary":"\u003cp\u003e\u003cstrong\u003eProblem statement\u003c/strong\u003e: Given a monorepo on Github with multiple docker images in\nit, write a github workflow to build and push all of them.\u003c/p\u003e","title":"Github Actions: multiple docker images"},{"content":"Whenever I open VSCode, there are some extension updates. Every time, I need to manually click \u0026ldquo;Extensions\u0026rdquo; in the left-side navigation bar and then \u0026ldquo;Restart\u0026rdquo; so that the upgrades take effect.\nThis should be automated.\nThere is a solution: add the following to your settings.json (Cmd + ,)\n\u0026#34;extensions.autoRestart\u0026#34;: true Documentation:\n// If activated, extensions will automatically restart following an update if the window is not in focus.\nSource: https://code.visualstudio.com/docs/getstarted/settings\n","permalink":"https://www.perrotta.dev/2024/11/vscode-auto-restart-extensions/","summary":"\u003cp\u003eWhenever I open VSCode, there are some extension updates. Every time, I need to\nmanually click \u0026ldquo;Extensions\u0026rdquo; in the left-side navigation bar and then \u0026ldquo;Restart\u0026rdquo;\nso that the upgrades take effect.\u003c/p\u003e\n\u003cp\u003eThis should be automated.\u003c/p\u003e","title":"VSCode: auto restart extensions"},{"content":"Trakt for Jellyfin Plugin:\nAvailable for install through the plugin catalog, Trakt for Jellyfin allows you to synchronize your watch states with ease.\nI’ve been enjoying this free plug-in. When watching content in Jellyfin (e.g. via Android TV, or via your laptop), it automatically scrobbles1 TV shows and movies to your Jellyfin account.\nPerhaps only we millennials peruse this term, which comes from LastFM.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/11/jellyfin-trakt-plug-in/","summary":"\u003cp\u003e\u003ca href=\"https://github.com/jellyfin/jellyfin-plugin-trakt\"\u003eTrakt for Jellyfin Plugin\u003c/a\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eAvailable for install through the plugin catalog, Trakt for Jellyfin allows you to synchronize your watch states with ease.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Jellyfin Trakt plug-in"},{"content":"When writing documentation in markdown e.g. for projects stored in git, adding a system architecture or design diagram is a great way to boost its readability, especially to unfamiliar readers.\nMy favorite way to generate diagrams is with GraphViz. It is widely available and easy to use.\nThat said, MermaidJS has native integration with GitHub and GitLab circa 2 years ago, and that\u0026rsquo;s hard to beat. Graphviz, sadly, does not.\nThere is a live editor for quick prototyping and iteration: https://mermaid.live/\nOnce you are happy with the result, you can easily embed it in markdown1:\n```mermaid flowchart TD A[Christmas] --\u0026gt;|Get money| B(Go shopping) B --\u0026gt; C{Let me think} C --\u0026gt;|One| D[Laptop] C --\u0026gt;|Two| E[iPhone] C --\u0026gt;|Three| F[fa:fa-car Car] ``` \u0026hellip;and it will get automatically rendered in the aforementioned git forges.\nMermaidJS is quite intuitive to use. There is plenty of documentation for it2, e.g. for graphs / flowcharts: https://mermaid.js.org/syntax/flowchart.html.\nThis is one of the examples in the official documentation.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAnd, as increasingly common these days, you can always resort to GenAI for an extra push.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/10/mermaid-rich-diagrams-in-markdown/","summary":"\u003cp\u003eWhen writing documentation in markdown e.g. for projects stored in git, adding\na system architecture or design diagram is a great way to boost its readability,\nespecially to unfamiliar readers.\u003c/p\u003e\n\u003cp\u003eMy favorite way to generate diagrams is with \u003ca href=\"https://graphviz.org/\"\u003eGraphViz\u003c/a\u003e.\nIt is widely available and easy to use.\u003c/p\u003e\n\u003cp\u003eThat said, \u003ca href=\"https://mermaid.js.org/\"\u003eMermaidJS\u003c/a\u003e has native integration with\n\u003ca href=\"https://github.blog/developer-skills/github/include-diagrams-markdown-files-mermaid/\"\u003eGitHub\u003c/a\u003e\nand \u003ca href=\"https://docs.gitlab.com/ee/user/markdown.html#mermaid\"\u003eGitLab\u003c/a\u003e circa \u003ca href=\"https://github.com/github/roadmap/issues/372\"\u003e2\nyears ago\u003c/a\u003e, and that\u0026rsquo;s hard to\nbeat. Graphviz, sadly, \u003ca href=\"https://forum.graphviz.org/t/github-adding-support-for-mermaid-diagrams/998\"\u003edoes\nnot\u003c/a\u003e.\u003c/p\u003e","title":"Mermaid: rich diagrams in markdown"},{"content":" Me: Would you like to play board games next week?\nAcquaintance 1: What kind of board games?\nAcquaintance 2: Which board games?\nAcquaintance 3: What games are you playing?\nPeople, have some room for serendipity and spontaneity, come on. Just say yes!\n","permalink":"https://www.perrotta.dev/2024/10/board-games/","summary":"Me: Would you like to play board games next week?\nAcquaintance 1: What kind of board games?\nAcquaintance 2: Which board games?\nAcquaintance 3: What games are you playing?\nPeople, have some room for serendipity and spontaneity, come on. Just say yes!","title":"Board games"},{"content":"Conventional commits: https://www.conventionalcommits.org/:\nA specification for adding human and machine readable meaning to commit messages\n[\u0026hellip;] a lightweight convention on top of commit messages. It provides an easy set of rules for creating an explicit commit history; which makes it easier to write automated tools on top of. This convention dovetails with SemVer, by describing the features, fixes, and breaking changes made in commit messages.\nIt looks like the following:\n\u0026lt;type\u0026gt;[optional scope]: \u0026lt;description\u0026gt; [optional body] [optional footer(s)] In practice, it looks like the following:\nfeat(sessions): add integration with github sso So that developers no longer need to log in with the shared admin account. Follow-up-of: #4242 Tested: `make test` One way to enforce it in a git project is with its pre-commit.com integration:\nrepos: - repo: https://github.com/compilerla/conventional-pre-commit rev: v3.4.0 hooks: # https://conventionalcommits.org - id: conventional-pre-commit stages: [commit-msg] args: # keep-sorted start - base - build - chore - ci - docs - feat - fix - meta - refactor - test # keep-sorted end The above hook only runs in the commit-msg stage, which is not installed by default via pre-commit install. You\u0026rsquo;ll need to pass -t commit-msg to it.\nI have mixed experiences and opinions with conventional commits in a team setting.\nIn a previous team, it was successfully adopted. It was very easy to generate changelogs this way.\nIn other teams, only a handful of members adopted it. In this setting there isn\u0026rsquo;t much value in enforcing a half-adopted convention, it must be all or nothing.\nIn repositories that I own, I tend to adopt something in-between, writing a prefix in the commit message title.\n","permalink":"https://www.perrotta.dev/2024/10/conventional-commits/","summary":"\u003cp\u003eConventional commits: \u003ca href=\"https://www.conventionalcommits.org/\"\u003ehttps://www.conventionalcommits.org/\u003c/a\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA specification for adding human and machine readable meaning to commit\nmessages\u003c/p\u003e\n\u003cp\u003e[\u0026hellip;] a lightweight convention on top of commit messages. It provides an easy\nset of rules for creating an explicit commit history; which makes it easier to\nwrite automated tools on top of. This convention dovetails with SemVer, by\ndescribing the features, fixes, and breaking changes made in commit messages.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Conventional commits"},{"content":"Given a docker registry, image and tag, run the following command:\n% docker manifest inspect 9876543210.foo.ecr.us-east-1.amazonaws.com/org/image:123456789 If it does not exist, you\u0026rsquo;ll see:\nno such manifest: 9876543210.foo.ecr.us-east-1.amazonaws.com/org/image:123456789 Ensure you are authenticated and/or connected to the VPN in case of a private registry.\nWhen using ECR (AWS\u0026rsquo;s registry), you can authenticate this way:\n% which docker_login docker_login () { aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 9876543210.foo.ecr.us-east-1.amazonaws.com } ","permalink":"https://www.perrotta.dev/2024/10/check-if-docker-image-exists-in-registry/","summary":"\u003cp\u003eGiven a docker registry, image and tag, run the following command:\u003c/p\u003e","title":"Check if docker image exists in registry"},{"content":"During performance reviews1, it\u0026rsquo;s handy to obtain an overview of your accomplishments that are stored as artifacts in source control systems.\nUsually accomplishments for a software engineer span more than just code: design documents, documentation, bug triage and fixes, product health initiatives, tackling of technical debt, processes\u0026hellip;the list is endless.\nNonetheless in this post I\u0026rsquo;ll focus only in contributions in the form of code.\n99.999% of the time this means git commits (and/or pull requests, if you will).\nOther than git, and the very very very occasional mercurial (hg), the only other VCS I used significantly was Google\u0026rsquo;s Perforce / Piper.\nPiper Although there are command-line tools to summarize your accomplishments, I find the easiest way to do so is via Critique.\nYour CL submissions Use a query like:\n(author:me OR pair:me) is:submitted since:2024-01-03 to:2024-01-09 Update since and to according to the current PERF / GRAD cycle.\nIf you are a high-performer and/or send lots of LSCs (large-scale changes), the output can be noisy. Filter it out with d:{description} as needed.\nFor example, to call out LSC changes in a separate section in your packet, you may want to exclude them from your overall contributions with -d:LSC or -d:Rosie or similar.\npair is for crediting pair programming.\nDisclaimer: There is a possibility the syntax is incorrect, as I am not able to test it at the moment.\nYour CL reviews r:me is:submitted since:2024-01-03 to:2024-01-09 You\u0026rsquo;ll likely want to call out readability reviews separately. Use something like -cc:typescript-readability-approvers.\ngit One option is to use the web UI of your forge (GitHub, GitLab, etc).\nI strive for a forge-agnostic solution though.\nYour pull requests / commits git shortlog is your friend!\nExample for this blog:\n% git shortlog --author=\u0026#34;Thiago Perrotta\u0026#34; --since=\u0026#34;6 months ago\u0026#34; # alt: --since=\u0026#34;2024-03-01\u0026#34; Thiago Perrotta (499): Initial commit add gitignore: go,hugo,vim hugo: add config.toml and default archetype [...] If there are multiple repositories, combine the command with myrepos:\n% mr run git shortlog --author=\u0026#34;Thiago Perrotta\u0026#34; To filter out irrelevant commits, use grep -v. Example:\n% git shortlog --author=\u0026#34;Thiago Perrotta\u0026#34; | grep -Ev \u0026#39;^\\s*ci|^.*pre-commit\u0026#39; \u0026hellip;filters out commits that contain \u0026ldquo;ci\u0026rdquo; or \u0026ldquo;pre-commit\u0026rdquo; in the message summary.\nYour PR reviews PR reviews are a concept from forges. From a git CLI perspective it would only be possible to do so when automation adds git tags to commits e.g. Reviewed-by: Thiago Perrotta \u0026lt;thiago@example.com\u0026gt;.\nGerrit does that. Here is an example in Chromium. In this case, one could just grep for Reviewed-by in git log.\nOtherwise: on GitHub, use a query such as is:pr reviewed-by:@me in the Pull Request search tab.\nPERF, GRAD, TPG: there are all sorts of naming schemes for them.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/10/performance-reviews/","summary":"\u003cp\u003eDuring performance reviews\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e, it\u0026rsquo;s handy to obtain an overview of your\naccomplishments that are stored as artifacts in source control systems.\u003c/p\u003e","title":"Performance reviews"},{"content":"At Google we had a prodaccess command used to log in and authenticate in all sorts of internal corp systems1 for 20 hours. It became a daily ritual to run prodaccess first thing in the morning when logging in upon unlocking our physical workstations2.\nDuring the pandemic, when heavily using our virtual workstations (dubbed cloudtops, think of an EC2 instance or a VPS) it was common to hook prodaccess into ssh. There was a prodcertstatus command that would tell you when prodaccess would expire. A common pattern was to modify your ssh startup command to automatically run prodaccess whenever prodcertstatus indicated that access would expire within 2-4 hours, depending on the user\u0026rsquo;s preference.\nGiven my trained muscle memory, I wanted to continue to use this alias even in environments other than Google:\n% cat ~/.profile.d/functions_corp.sh [...] prodaccess() { echo \u0026#34;--\u0026gt; VPN\u0026#34; vpn_login echo \u0026#34;--\u0026gt; Teleport\u0026#34; teleport_login echo \u0026#34;--\u0026gt; AWS\u0026#34; aws_login echo \u0026#34;--\u0026gt; Azure\u0026#34; azure_login case \u0026#34;$1\u0026#34; in -u|--update) echo \u0026#34;--\u0026gt; mr\u0026#34; # Update all well-known corp repositories (cd ~/Corp \u0026amp;\u0026amp; mr update \u0026amp;\u0026amp; mr run git world) # Package manager upgrade echo \u0026#34;--\u0026gt; sd-world\u0026#34; sd-world shift ;; -a|--all) echo \u0026#34;--\u0026gt; Atlas Mongo\u0026#34; atlas_login shift ;; esac } [...] Notes:\nmr is https://myrepos.branchable.com/ (\u0026ldquo;a tool to manage all your version control repositories\u0026rdquo;)3 sd-world was introduced in a previous post: it performs a full system upgrade --all holds services I only need / use from time to time. shift is not really needed, but IMHO it\u0026rsquo;s a good practice (and another ingrained habit of mine), especially when inside for loops git world is an alias in my ~/.gitconfig that runs git fetch --all \u0026amp;\u0026amp; git remote prune origin Every morning I invoke prodaccess -u when starting my work day. A few password prompts and touch IDs later and I do not need to worry about logging in anymore during the middle of the day, which reduces overall context switching and stress.\nUntil someone in CorpEng decided it was a good idea to deprecate it in favour of a new gcert command, destroying many SWE-years of developed muscle memory.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAh, the good ol\u0026rsquo; times.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI should write a post about it sometime.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/10/prodaccess/","summary":"\u003cp\u003eAt Google we had a \u003ccode\u003eprodaccess\u003c/code\u003e command used to log in and authenticate in all\nsorts of internal corp systems\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e for 20 hours. It became a daily ritual to run\n\u003ccode\u003eprodaccess\u003c/code\u003e first thing in the morning when logging in upon unlocking our\nphysical workstations\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e\n\u003cp\u003eDuring the pandemic, when heavily using our virtual workstations (dubbed\n\u003cem\u003ecloudtops\u003c/em\u003e, think of an EC2 instance or a VPS) it was common to hook\n\u003ccode\u003eprodaccess\u003c/code\u003e into \u003ccode\u003essh\u003c/code\u003e. There was a \u003ccode\u003eprodcertstatus\u003c/code\u003e command that would tell\nyou when \u003ccode\u003eprodaccess\u003c/code\u003e would expire. A common pattern was to modify your \u003ccode\u003essh\u003c/code\u003e\nstartup command to automatically run \u003ccode\u003eprodaccess\u003c/code\u003e whenever \u003ccode\u003eprodcertstatus\u003c/code\u003e\nindicated that access would expire within 2-4 hours, depending on the user\u0026rsquo;s\npreference.\u003c/p\u003e\n\u003cp\u003eGiven my trained muscle memory, I wanted to continue to use this alias even in\nenvironments other than Google:\u003c/p\u003e","title":"★ prodaccess"},{"content":"Problem statement: Given a Makefile within ~/git/scaffolding/Makefile, and a command that needs to run from the scaffolding/ directory, create an all target that works from any directory.\nSolution:\n# The directory wherein the Makefile resides. ROOT_DIR := $(patsubst %/,%,$(dir $(realpath $(lastword $(MAKEFILE_LIST))))) all: @echo $(ROOT_DIR) kickstart $(ROOT_DIR)/app .PHONY: all Explanation:\nThe echo is used only for debugging, therefore it should be removed in prod. The kickstart command will properly run having ~/git/scaffolding as $PWD whether you invoke it from ~/git/scaffolding or from ~/git (via make -C). pathsubst is needed to remove the trailing slash (/) from the directory, so that $(ROOT_DIR)/ does not yield a double slash, which works but it is ugly. Source (adapted): https://stackoverflow.com/questions/18136918/how-to-get-current-relative-directory-of-your-makefile\n","permalink":"https://www.perrotta.dev/2024/10/makefile-path-to-root-dir/","summary":"\u003cp\u003e\u003cstrong\u003eProblem statement\u003c/strong\u003e: Given a \u003ccode\u003eMakefile\u003c/code\u003e within \u003ccode\u003e~/git/scaffolding/Makefile\u003c/code\u003e,\nand a command that needs to run from the \u003ccode\u003escaffolding/\u003c/code\u003e directory, create an\n\u003ccode\u003eall\u003c/code\u003e target that works from any directory.\u003c/p\u003e","title":"Makefile: path to root dir"},{"content":"Recipe to list all pods that belong to a given node:\n$ kubectl get pod -o wide --field-selector spec.nodeName={node_name} -A Source: https://stackoverflow.com/questions/39231880/kubernetes-api-get-pods-on-specific-nodes\nThis ought to be easier to remember\u0026hellip;\n","permalink":"https://www.perrotta.dev/2024/10/kubectl-list-all-node-pods/","summary":"\u003cp\u003eRecipe to list all pods that belong to a given node:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e$ kubectl get pod -o wide --field-selector spec.nodeName\u003cspan style=\"color:#f92672\"\u003e={\u003c/span\u003enode_name\u003cspan style=\"color:#f92672\"\u003e}\u003c/span\u003e -A\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"kubectl: list all node pods"},{"content":"Our installation of traefik via helm got stuck today in such a way that its pods did not terminate, even with successive kubectl delete.\n--force did not work either :mildshock:\n--grace-period=0 (alongside --force) did the trick:\nkubectl delete pod traefik-{...} --grace-period=0 --force --namespace kube-system Source: https://stackoverflow.com/questions/35453792/pods-stuck-in-terminating-status\n","permalink":"https://www.perrotta.dev/2024/10/kubectl-force-delete-pods/","summary":"\u003cp\u003eOur installation of \u003ccode\u003etraefik\u003c/code\u003e via \u003ccode\u003ehelm\u003c/code\u003e got stuck today in such a way that its\npods did not terminate, even with successive \u003ccode\u003ekubectl delete\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e--force\u003c/code\u003e did not work either :mildshock:\u003c/p\u003e","title":"kubectl: force delete pods"},{"content":"Intro I have two raspberry pi1 units sitting idle at home, a 3B and a 4.\nFor a long time I’ve been wanting to do something useful with them, while increasing my DevOps toolkit knowledge.\nThe roadblock to do so was the most classic excuse: lack of non-interrupted time.\nThere’s plenty of motivation, and ideas. These will likely never run out, any time soon.\nThat said: It’s very easy to get distracted and lose focus. There is so much information today, in the form of YouTube videos, blog posts, forums and communities (e.g. the likes of Hacker News and Reddit), podcasts, books…the list goes on, and it does not end.\nTherefore, for the sake of fixing a North Star path, I wanted to make an initial blog post with some of the ideas that are currently in my head.\nThere’s no guarantee I will follow up on all of these ideas but, as long as their spirit is ingrained in semi-permanent written form, I figured that shall be enough to make me accountable to myself.\nGuiding principles a.k.a. rules of engagement\nIt must run Unix No Windows. No macOS. Anything else is fair game. Corollary: It must run either Linux or BSD.\nIt must be vanilla / upstream No spin-offs. For example: for Ubuntu, no Xubuntu. For Arch Linux, no Manjaro. For Gentoo, no Funtoo. And so on. Stick to the core / base Linux distributions. For BSD, this is a non-issue.\nEvery software installation must come from a package If there is no package, I will create one myself. This is easier if I use Arch Linux or Alpine Linux, but I am willing to contribute to other distributions as well.\nIt must have no X11 nor Wayland nor a graphical system It should be a pure server. In the past I ran RaspberryPi OS (neé Raspbian) and it wasn’t very useful, besides being super slow and sluggish.\nSoftware updates must happen with a single command There is no need for auto-updates (these are often not well-supported anyways), but a human operator should be able to upgrade everything in a single shot. It doesn’t have to be a single command (e.g. apt update + apt upgrade is acceptable), but it should be contained within a short script.\nIt should be reasonably popular and well-supported No obscure distributions. I don’t particularly care about a sizeable community (I won’t join their Discord server nor Reddit community anyway), but there should be at least one official support channel, and it would be preferred that it is old-school (BBS / Discourse / forums, mailing lists, IRC / Matrix). Stack Exchange is also acceptable to an extent. The problem with the modern stuff (Discord, Reddit) is that it is too proprietary, can / will disappear at any moment, and will be heavily used to train LLMs with no scrutiny. Commercial support is fine. For example, Red Hat backing Fedora, Canonical backing Ubuntu, and SUSE backing openSUSE is a non-issue. I would just avoid commercial enterprises that suffocate their open counterparts.\nIt should support Raspberry Pi (the ARM architecture, for that matter) as a first-class citizen If Raspberry Pi support is considered experimental, I would avoid the trouble at this time.\nStopping briefly here for a moment, the following list comes to mind, using DistroWatch and Linux Distribution timeline as an aid:\nDebian RaspberryPi OS Ubuntu Fedora openSUSE Alpine Linux Arch Linux (Arch Linux ARM) Void Linux Gentoo NixOS FreeBSD OpenBSD Slackware Linux This list is still quite large. I will need to trim it down further in the following days. Stay tuned.\nSome observations before I make a final decision:\nI have two units, so one decision to make is whether to choose the same distribution for both or distinct distributions for each. One Linux and one BSD, for example. I never used the following distros: Void Linux, NixOS, OpenBSD, Slackware Linux. There’s always an appeal to trying out something new, even if just ephemerally. I am heavily experienced in Debian / Ubuntu, Arch and Alpine (with a tad of Gentoo as well). There’s an appeal to using something I am already familiar with to get the OS out of the way, and thus focus more on DevOps. NixOS is very tempting for the purposes of reproducibility, but every time I look at it I feel lost in its sea of complexity. And it oftentimes feels bloated. Reproducibility comes with a non-trivial upfront cost. OpenBSD feels very tempting as a self-contained, “do one thing and do it well”, KISS \u0026amp; secure system. It lacks on integration with third-party software, but perhaps that’s a feature. Let’s be honest, as much as Slackware has its charm, realistically I am not choosing it. Its ecosystem is too small today. It doesn’t provide any value when compared to the rest of the list. Between Slack and Alpine, I’d easily pick Alpine with no effort. Hence we can already eliminate one: Slackware Linux.\nTo be continued ■\nTwo units is a safer choice than “raspberry pies” or “raspberry pis”.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/10/new-series-raspberry-pi-fun-with-devops/","summary":"\u003ch2 id=\"intro\"\u003eIntro\u003c/h2\u003e\n\u003cp\u003eI have two raspberry pi\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e units sitting idle at home, a 3B and a 4.\u003c/p\u003e\n\u003cp\u003eFor a long time I’ve been wanting to do something useful with them,\nwhile increasing my DevOps toolkit knowledge.\u003c/p\u003e","title":"★ New series: Raspberry Pi fun with DevOps"},{"content":"Shell:\nalias cdg=\u0026#39;cd \u0026#34;$(git root)\u0026#34;\u0026#39; ~/.gitconfig:\n[alias] root = rev-parse --show-toplevel Sadly it is not possible to do so with a git alias, c.f. https://stackoverflow.com/questions/19032372/git-alias-for-shell-command-to-cd-into-git-root-not-working-as-expected:\nYour shell is invoking Git, and Git is invoking another shell in which to run your cd command. This command is successful, and this changes the working directory of the child shell, but it does not change the working directory of Git, nor of the parent shell.\nIn order to do this you need to run the command in your current shell, which means that invoking Git will not be able to accomplish this. You will have to continue using a shell alias.\nSince we cannot change directory with a git alias, then at least we can use one to print the repository root.\n","permalink":"https://www.perrotta.dev/2024/10/cdg-change-directory-to-the-git-root/","summary":"\u003cp\u003eShell:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ealias cdg\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;cd \u0026#34;$(git root)\u0026#34;\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003ccode\u003e~/.gitconfig\u003c/code\u003e:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e[alias]\n  root = rev-parse --show-toplevel\n\u003c/code\u003e\u003c/pre\u003e","title":"cdg: change directory to the git root"},{"content":"Given a YAML file that is effectively JSON, convert it to YAML format.\nInput % cat file.yaml { \u0026#34;title\u0026#34;: \u0026#34;The Big Bang Theory\u0026#34;, \u0026#34;characters\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Sheldon\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;Leonard\u0026#34; } ] } Output % yq -i -P file.yaml % cat file.yaml title: The Big Bang Theory characters: - name: Sheldon - name: Leonard yq is like jq for YAML.\n-i is for in-place modification1, -P is for pretty-printing.\nIf you don’t know about -i, you can always use sponge:\n% yq -P file.yaml | sponge file.yaml Happy YAML’ing!\nIn this context avoid using \u0026gt; output redirection because it would mangle the input file. Unless you redirect to another file, of course.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/10/convert-json-to-yaml/","summary":"\u003cp\u003eGiven a YAML file that is effectively JSON, convert it to YAML format.\u003c/p\u003e","title":"Convert JSON to YAML"},{"content":"Buried into a ~/.bashrc far away\u0026hellip;1\n# misspellings for git alias t=git alias it=git alias gi=git alias gt=git alias gti=git History makes no mistake: all these have occurred at least once.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/10/git-misspellings/","summary":"Buried into a ~/.bashrc far away\u0026hellip;1\n# misspellings for git alias t=git alias it=git alias gi=git alias gt=git alias gti=git History makes no mistake: all these have occurred at least once.\u0026#160;\u0026#x21a9;\u0026#xfe0e;","title":"git misspellings"},{"content":"ArgoCD is a widely used GitOps software for Kubernetes Continuous Delivery (see USERS.md).\nI am quite surprised no one bothered to create an Alpine Linux package for it.\nUntil\u0026hellip;now, by yours truly.\nThis APKBUILD took a bit longer to create than the usual. There were a couple of issues with -buildmode=pie, addressed with export CGO_ENABLED=1 (via make CGO_FLAG=1).\nAlso, not every architecture is compatible with it. The following error message appears in ARM builds:\ncannot use math.MaxInt64 (untyped int constant 9223372036854775807) as int value in argument to env.ParseNumFromEnv (overflows) Anyway, once it is merged upstream, enjoy!\n% doas apk add argocd ","permalink":"https://www.perrotta.dev/2024/10/new-apkbuild-argocd/","summary":"\u003cp\u003e\u003ca href=\"https://argo-cd.readthedocs.io/en/stable/\"\u003eArgoCD\u003c/a\u003e is a widely used GitOps\nsoftware for Kubernetes Continuous Delivery (see\n\u003ca href=\"https://github.com/argoproj/argo-cd/blob/master/USERS.md\"\u003eUSERS.md\u003c/a\u003e).\u003c/p\u003e\n\u003cp\u003eI am quite surprised no one bothered to create an Alpine Linux package for it.\u003c/p\u003e\n\u003cp\u003eUntil\u0026hellip;\u003ca href=\"https://gitlab.alpinelinux.org/alpine/aports/-/merge_requests/73305\"\u003enow\u003c/a\u003e,\nby yours truly.\u003c/p\u003e","title":"New APKBUILD: argocd"},{"content":"Exponential Idle:\nExponential Idle is a math-inspired incremental game available on Android and iOS. Your goal is to stack up money by taking advantage of exponential growth. To do so, you have to step through time by tapping the equation or simply let the time follow its course. You can perform change of variables to accelerate the process, buy upgrades, get rewards, and unlock achievements while earning virtual money.\nI’ve been in love with this game, it is very fun and chill. The typography is beautiful, with LaTeX equations all over the place.\nIt’s very similar to Universal Paperclips, but with an academic1 tad.\nAs of the time of this post I am a Master’s student, and my thesis proposal has just been accepted!\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/10/exponential-idle/","summary":"\u003cp\u003e\u003ca href=\"https://conicgames.github.io/exponentialidle/\"\u003eExponential Idle\u003c/a\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eExponential Idle is a math-inspired incremental game available on Android and iOS. Your goal is to stack up money by taking advantage of exponential growth. To do so, you have to step through time by tapping the equation or simply let the time follow its course. You can perform change of variables to accelerate the process, buy upgrades, get rewards, and unlock achievements while earning virtual money.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Exponential Idle"},{"content":"To format dates GNU date is often used:\n$ date \u0026#39;+%Y-%m-%d\u0026#39; 2024-10-07 It turns out bash (\u0026gt;=4.2) has this feature built-in as part of printf:\n$ printf \u0026#39;%(%Y-%m-%d)T\\n\u0026#39; 2024-10-07 It does not work on zsh though:\n% printf \u0026#39;%(%Y-%m-%d)T\\n\u0026#39; printf: %(: invalid directive Source: https://blog.marco.ninja/notes/technology/linux/working-with-dates-in-bash-and-other-shells/\n","permalink":"https://www.perrotta.dev/2024/10/bash-built-in-date/","summary":"\u003cp\u003eTo format dates \u003ca href=\"https://www.gnu.org/software/coreutils/manual/html_node/Examples-of-date.html\"\u003eGNU\ndate\u003c/a\u003e\nis often used:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e$ date \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;+%Y-%m-%d\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e2024-10-07\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eIt turns out bash (\u0026gt;=4.2) has this feature built-in as part of \u003ccode\u003eprintf\u003c/code\u003e:\u003c/p\u003e","title":"bash built-in date"},{"content":"Sometimes I need to do random experimentation in a throwaway directory.\nFor that, I have the following cdtmp function in my shell:\ncdtmp () { builtin cd \u0026#34;$(mktemp -d \u0026#34;/tmp/$USER-${1:+$1-}$(date +%Y-%m-%d)-XXXXXX\u0026#34;)\u0026#34; || return builtin pwd } Here are two example usages:\n% cdtmp /tmp/thiago-2024-10-07-V3c3Na thiago@thiagoperrotta-MacBook-Pro /tmp/thiago-2024-10-07-V3c3Na % cdtmp devops /tmp/thiago-devops-2024-10-07-P4W1fh This idea was built upon Alex Kotliarskyi\u0026rsquo;s, which he describes as:\nIt’s a super simple alias that creates a temporary directory and then jumps into it. Here are a few examples of what I use it for:\nClone a random interesting git repo to experiment with I also use it for testing one-off bash, C++, golang or python scripts, for example:\n% cdtmp /tmp/thiago-2024-10-07-F5aFrJ thiago@thiagoperrotta-MacBook-Pro /tmp/thiago-2024-10-07-F5aFrJ % vim main.sh thiago@thiagoperrotta-MacBook-Pro /tmp/thiago-2024-10-07-F5aFrJ % bash main.sh hello world Here is the definition my dotfiles.\n","permalink":"https://www.perrotta.dev/2024/10/cdtmp-change-to-a-temporary-directory/","summary":"\u003cp\u003eSometimes I need to do random experimentation in a throwaway directory.\u003c/p\u003e\n\u003cp\u003eFor that, I have the following \u003ccode\u003ecdtmp\u003c/code\u003e function in my shell:\u003c/p\u003e","title":"cdtmp: change to a temporary directory"},{"content":"Overview I wanted to be notified whenever a flight deal from Munich1 appeared. The destination doesn\u0026rsquo;t really matter.\nThere are several ways to do so semi-automatically, I tried all of them at least once:\nfollow travel blogs sign up for newsletters follow instagram pages or influencers that relay promos visit the websites from airlines directly use bots / scraping ask friends periodically date a flight attendant The list above is overwhelming, and I have no time for all of it. Furthermore, I have zero desire to inundate myself with (even) more social media posts, that are often empty in nature.\nMoreover I wanted to build a permanent and long-lasting solution on top of my personal infrastructure.\nHence the only real options were: (i) travel blogs, (ii) newsletters, (iii) scraping.\nThe first phase is implemented as a Telegram channel: https://t.me/bavariabudgetbirds.\nWhenever there are new deals my server pushes an update to the Telegram channel above for each of them. The deals2 are tracked via a list of curated RSS feed subscriptions, which is filtered to only match keywords that are pertaining to Munich. It is not perfect, but it is good enough for my purposes, virtually marginally free to run, and needs zero maintenance.\nTechnical Details I have a VPS that runs Alpine Linux. The VPS provider is not important.\nThe OS runs the excellent Miniflux app, managed via OpenRC. In this specific deployment, there is no systemd, and there is no virtualization (docker, kubernetes, etc). It is 100% KISS. Miniflux is actively developed and Alpine Linux package releases happen frequently and quickly. Even if they do not, I have the means to create APKBUILDs myself when needed (see previous posts).\nMiniflux is populated with the list of travel blogs I follow, filtering out non-relevant posts, polled roughly every 2 hours. There is a separate Miniflux user for this purpose, so that my own subscriptions do not get mingled in-between.\nI use the Miniflux integration with Telegram to ensure that new posts are immediately pushed to the aforementioned channel with my Telegram bot.\nWishlist for the future:\nAdd the ability to integrate with websites that do not support RSS directly and/or that block RSS readers. Examples: https://lowcost.pro/route/MUC/XXX/EN/ https://www.secretflying.com/posts/category/cities-countries/germany/ https://www.trabber.de/en/flights-from-germany-de/ Convert newsletters-only sources to RSS feeds. This solution can easily be generalized for any location; there is absolutely nothing special about Munich in this context.\nMunich, Memmingen or Nuremberg (Nürnberg)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBlog posts, really.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/10/tracking-cheap-flights-from-munich/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eI wanted to be notified whenever a flight deal from Munich\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e appeared. The\ndestination doesn\u0026rsquo;t really matter.\u003c/p\u003e","title":"Tracking cheap flights from Munich"},{"content":"Today I tried to use my Steam Deck to no avail.\nUpon turning it on, a “Downloading update (…)” splash screen would appear, then the device would quickly turn itself off, in a matter of seconds.\nRepeating this dance a couple of times yielded the same results.\nThen I found out that if you hold the three-dot button (the quick settings menu button) briefly after turning it on, you are prompted with a boot loader screen that resembles GRUB. In this screen it’s possible to roll back to an earlier OS version.\nThe Steam Deck disk partitioning has two partitions (A and B), just choose the second one (i.e. not the latest one).\nThis time upon restarting the device the boot loop was gone.\nCredits: https://www.reddit.com/r/SteamDeck/comments/1bb5767/steam_deck_stuck_at_update_complete_launching/\n","permalink":"https://www.perrotta.dev/2024/10/steam-deck-downloading-update-boot-loop/","summary":"\u003cp\u003eToday I tried to use my Steam Deck to no avail.\u003c/p\u003e\n\u003cp\u003eUpon turning it on, a “Downloading update (…)” splash screen would appear, then the device would quickly turn itself off, in a matter of seconds.\u003c/p\u003e","title":"Steam Deck \"downloading update\" boot loop"},{"content":"In the google3 codebase, when working with Piper directly (i.e. not Fig[-on-CitC]), I often liked to use this tool called erratic (abbrev: er), which was a 20% project of another Software Engineer1.\ngoogle3 workflow The workflow is as follows:\nMake some changes: g4 open, g4 edit, etc Create a CL (ChangeList) g4 change Now let\u0026rsquo;s say you\u0026rsquo;re anticipating some heavy refactoring, in the same CL2, that you could potentially regret and want to revert.\nThis would be a great moment to use erratic. After installing it (or aliasing it, if using it via X20), run:\n% er explain \u0026#34;this is working perfectly before bob@ asked me to refactor it\u0026#34; Make additional changes Feel free to make more snapshots (checkpoints), as you see fit If you ever want to roll back, run er list. It will list all explicit snapshots you annotated so far. Locate the one you want to roll back to:\n% er list 42 \u0026#34;this is working perfectly before bob@ asked me to refactor it\u0026#34; 78 \u0026#34;second refactoring\u0026#34; Now just roll back to it:\n% er restore 42 Sometimes you\u0026rsquo;ll need to run g4 add afterwards. And we\u0026rsquo;re done!\nColophon: Why use erratic at all, since CitC already does automatic periodic snapshots out-of-the-box? Because these automatic snapshots are meant for machines, not humans; they are not very developer-friendly. You could get your way around rsync + finding the correct timestamps to copy from, but that\u0026rsquo;s not fun at all, and not a good use of time either.\ngit workflow How to replicate a similar workflow in a non-Google world?\nFor most of us, non-Google necessarily means git. Let\u0026rsquo;s ignore hg (mercurial) in this context.\nThere is not much to do, actually: https://github.blog/open-source/git/commits-are-snapshots-not-diffs/\ngit commits are already snapshots. The workflow is:\nMake some changes, then git add Create a branch (git switch --create), commit your changes To make a snapshot, just make a new commit and annotate it (git commit -m \u0026quot;foo\u0026quot;) To list your snapshots, run git rev-list $(git show-branch --merge-base HEAD)^..HEAD --pretty. This will list all commits since your branch diverted. You could add an alias to it in your ~/.gitconfig. To roll back, run git reset --hard {commit}. Caveat: You will lose track of all commits after {commit}. Although it would still be possible to recover them with git reflog (in case of a mistake), that is not a developer-friendly workflow.\nThen the follow-up question is: how to roll back without losing track of intermediate work?\nOne way is to create a new branch that points out to {commit} instead of hard-resetting your entire worktree:\n% git checkout -b mynewbranch {commit} Once you are satisfied, just delete the previous branch and then rename the current one to it.\nAnother possibility is the use of git worktrees to divert (spin-off) branches.\nThis becomes a very natural workflow once you repeat it a couple of times.\nI have gladly peer bonused him. You should always peer bonus those folks who helped you become more productive.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nArguably you should create a separate CL for that (go/small-cls), but who am I to judge? Unless I am your teammate\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/10/erratic-human-friendly-google3-piper-snapshots/","summary":"\u003cp\u003eIn the google3 codebase, when working with Piper directly (i.e. not\nFig[-on-CitC]), I often liked to use this tool called \u003ccode\u003eerratic\u003c/code\u003e (abbrev: \u003ccode\u003eer\u003c/code\u003e),\nwhich was a 20% project of another Software Engineer\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e","title":"★ Erratic: human-friendly google3 piper snapshots"},{"content":"When using fd(1), only the filename is matched by default.\nTo match the full path, use -p. It is often useful to combine it with --type file.\n% fd -p clustermon --type file apps/base/clustermon/clustermon.yaml apps/base/clustermon/kustomization.yaml apps/overlays/g02/clustermon/patches.yaml apps/overlays/g02/clustermon/values.yaml A natural extension is to pipe it to | ifne xargs -n 1 gsed -i -e '{expression}'. Changes in the entire codebase at your fingertips!\n","permalink":"https://www.perrotta.dev/2024/10/fd-match-full-path/","summary":"\u003cp\u003eWhen using \u003ca href=\"https://github.com/sharkdp/fd\"\u003e\u003ccode\u003efd(1)\u003c/code\u003e\u003c/a\u003e, only the filename is\nmatched by default.\u003c/p\u003e\n\u003cp\u003eTo match the full path, use \u003ccode\u003e-p\u003c/code\u003e. It is often useful to combine it with \u003ccode\u003e--type file\u003c/code\u003e.\u003c/p\u003e","title":"fd: match full path"},{"content":"Context: Helm charts use .Values.{foo} in templates/ for templating from values.yaml.\nGoal: Return all unique values used in templates/. The intention for it is to add them all as defaults to values.yaml.\nFirst attempt:\n$ ack \u0026#39;\\.Values\\.\u0026#39; One could proceed manually from here, but this is not very ergonomic in big helm charts.\nA better (and more precise) attempt is:\n$ ack \u0026#39;\\.Values[\\.\\w+]+\u0026#39; This will match .Values.config.foo, in addition to .Values.foo.\nTo return only the matched strings, add -o:\n$ ack -o \u0026#39;\\.Values[\\.\\w+]+\u0026#39; Here is a real example:\n$ ack -o \u0026#39;\\.Values[\\.\\w+]+\u0026#39; [...] values.yaml 349:29:.Values.global.prometheus_scrape_interval 863:29:.Values.global.prometheus_scrape_interval 903:20:.Values.global.global_grafana_ingest_url To return only the values, add -h (it will hide the filenames):\n$ ack -o -h \u0026#39;\\.Values[\\.\\w+]+\u0026#39; [...] .Values.global.prometheus_scrape_interval .Values.global.prometheus_scrape_interval .Values.global.global_grafana_ingest_url Now all that is left to do is to pipe it to sort -u.\n$ ack -o -h \u0026#39;\\.Values[\\.\\w+]+\u0026#39; | sort -u [...] .Values.global.prometheus_scrape_interval .Values.global.global_grafana_ingest_url It turns out that grep also has a -o flag with a similar behavior.\n","permalink":"https://www.perrotta.dev/2024/10/ack-match-and-return-only-the-matched-string/","summary":"\u003cp\u003e\u003cstrong\u003eContext\u003c/strong\u003e: Helm charts use \u003ccode\u003e.Values.{foo}\u003c/code\u003e in \u003ccode\u003etemplates/\u003c/code\u003e for templating from\n\u003ccode\u003evalues.yaml\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eGoal\u003c/strong\u003e: Return all unique values used in \u003ccode\u003etemplates/\u003c/code\u003e. The intention for it is\nto add them all as defaults to \u003ccode\u003evalues.yaml\u003c/code\u003e.\u003c/p\u003e","title":"ack: match and return only the matched string"},{"content":"Problem statement: Given a kubernetes cluster with many helm charts in different namespaces, how to use helm to query the list of the last recently \u0026ldquo;touched\u0026rdquo; charts?\nBy \u0026ldquo;touched\u0026rdquo; we mean either installed (via helm install) or upgraded (via helm upgrade).\nSomething like: helm ls --all --sort-by updated. Using kubectl directly would also be OK.\nSolution The brute force way:\nhelm ls --max 99999 -A -o json | jq -r \u0026#39;.[] | \u0026#34;\\(.updated)\\t\\(.name)\u0026#34;\u0026#39; | sort | tail | column -t -A for all namespaces --max 99999 to \u0026ldquo;disable\u0026rdquo; paging (there\u0026rsquo;s no better way as of 2024-09-24) jq to filter out on the \u0026ldquo;updated\u0026rdquo; field Note that a sample json entry looks like the following:\n{ \u0026#34;name\u0026#34;: \u0026#34;hoth-cb7f8a327\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;revision\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;updated\u0026#34;: \u0026#34;2024-06-14 15:02:16.775174131 +0000 UTC\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;deployed\u0026#34;, \u0026#34;chart\u0026#34;: \u0026#34;hoth-0.1.0\u0026#34;, \u0026#34;app_version\u0026#34;: \u0026#34;1.0.0\u0026#34; } Sample output of the aforementioned command:\n2024-09-24 10:56:15.655674586 +0000 UTC geonosis 2024-09-24 10:56:15.731630075 +0000 UTC coruscant 2024-09-24 11:01:35.156973247 +0000 UTC endor 2024-09-24 11:02:30.314014351 +0000 UTC hoth ","permalink":"https://www.perrotta.dev/2024/09/helm-list-recent-chart-upgrades/","summary":"\u003cp\u003e\u003cstrong\u003eProblem statement\u003c/strong\u003e: Given a kubernetes cluster with many helm charts in\ndifferent namespaces, how to use \u003ccode\u003ehelm\u003c/code\u003e to query the list of the last recently\n\u0026ldquo;touched\u0026rdquo; charts?\u003c/p\u003e\n\u003cp\u003eBy \u0026ldquo;touched\u0026rdquo; we mean either installed (via \u003ccode\u003ehelm install\u003c/code\u003e) or upgraded (via\n\u003ccode\u003ehelm upgrade\u003c/code\u003e).\u003c/p\u003e\n\u003cp\u003eSomething like: \u003ccode\u003ehelm ls --all --sort-by updated\u003c/code\u003e. Using \u003ccode\u003ekubectl\u003c/code\u003e directly\nwould also be OK.\u003c/p\u003e","title":"helm: list recent chart upgrades"},{"content":"Every macOS upgrade comes with disappointment and sorrow in the form of some tool, dependency / library or system-wide configuration breaking1.\nThis time: the upgrade to macOS Sequoia (15.0) broke ack (Perl).\n% ack zsh: /opt/homebrew/bin/ack: bad interpreter: /usr/bin/perl5.30: no such file or directory I desperately needed to use ack, however there was no internet connection. And I didn\u0026rsquo;t have any of its alternatives installed (e.g. ag, ripgrep / rg).\nThen the best way is to fall back to good ol\u0026rsquo; grep2:\n% grep [-l] \u0026#39;\\bref\\b\u0026#39; **/* Alternatively, within a git repository, there is git grep:\n% git grep [-l] \u0026#39;\\bref\\b\u0026#39; With git grep there is no need to specify which files to grep, as it entails all files that belong to the repository\u0026hellip;which happens to be a caveat for newly created files that were not yet git add-ed.\nHomebrew (brew) being almost always one of them.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIf grep ever breaks, we are screwed.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/09/life-without-ack/","summary":"\u003cp\u003eEvery macOS upgrade comes with disappointment and sorrow in the form of some\ntool, dependency / library or system-wide configuration breaking\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e\n\u003cp\u003eThis time: the upgrade to macOS Sequoia (15.0) broke \u003ccode\u003eack\u003c/code\u003e (Perl).\u003c/p\u003e","title":"Life without ack"},{"content":"This post builds upon the previous Anki: custom language card.\nSearching word boundaries When searching for a word such as Mann, results such as Mannschaft come up.\nTo search accounting for the word boundary, like a typical \\bMann\\b regex, use w:. In this example: w:Mann.\nSearching ignoring accents When searching for a word with Umlauts such as Über, you should type it exactly as is.\nIf you are lazy or do not recall where the umlaut falls, perhaps you would think of searching for uber, however that does not work.\nTo make it work, prepend nc: to the word. In this case: nc:uber will properly match über. NC stands for non-combining.\nSearching word boundaries whilst ignoring accents Is it possible to combine the two aforementioned operators? For example, to match für. Unfortunately, it is not. Anki does not support w:nc:fur nor nc:w:fur, which is unfortunate. Schade!\nIn this case the best compromise would be to search for w:für.\nAdding images to flashcards It is pretty straightforward: Right click the image in your favorite web browser, copy it, and then paste it (Ctrl/Cmd + V) into Anki. The image is automatically imported.\nNo-brainer image sources:\ndict.cc Google Images Nowadays it would also be natural1 to use \u0026ldquo;AI\u0026rdquo; to generate images.\nProviding examples: style To illustrate with a concrete example:\n1: Ja, genau.\n2: Das ist ja komisch.\nAffirmative\nThe structure:\nBold numbers to disambiguate Underscore the word in question to emphasize it Italicize everything that is meta or an explanation about the word To highlight substrings, e.g. Mann in Mannschaft, use underscores2.\nTags I seldom tag notes because there is no need for categorization, all cards are treated the same way. Tagging is only useful in two scenarios:\nfor grouping / retrieval: for example, if you want to make custom study sessions for a specific domain / area (e.g. animals, programming, trips)\nfor provenance annotations: to make it easier to remember where a given note came from (e.g. \u0026ldquo;textbook\u0026rdquo;, \u0026ldquo;my german teacher\u0026rdquo;, \u0026ldquo;blog\u0026rdquo;, \u0026ldquo;podcast\u0026rdquo;, \u0026ldquo;work\u0026rdquo;)\nAdding tags creates the burden of managing and standardizing them. You do not want to spend valuable mental effort derailing from your main task.\nSources https://docs.ankiweb.net/searching.html Albeit deeply unnecessary and wasteful (energy-wise).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis principle is not strict. I often interchange bold and underscore. Choose whichever feels more natural in the appropriate context.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/09/anki-custom-language-card-cont./","summary":"\u003cp\u003eThis post builds upon the previous \u003ca href=\"https://www.perrotta.dev/2024/02/anki-custom-language-card/\"\u003eAnki: custom language card\u003c/a\u003e.\u003c/p\u003e","title":"★ Anki: custom language card (cont.)"},{"content":" Coworker: Do you have an AI plug-in for Slack?\nCoworker: I see that you use things like this: AI: Exclude \u0026ldquo;cert-manager\u0026rdquo; from the alert (manually)\nMe: lol, this is just an abbrev. for \u0026ldquo;Action Item\u0026rdquo;. Might as well have been \u0026ldquo;TODO:\u0026rdquo;\nCoworker: AI is such a buzzword nowadays, looks like I\u0026rsquo;m starting to get brainwashed to see it in everything\n","permalink":"https://www.perrotta.dev/2024/09/slack-ai-plug-in/","summary":"Coworker: Do you have an AI plug-in for Slack?\nCoworker: I see that you use things like this: AI: Exclude \u0026ldquo;cert-manager\u0026rdquo; from the alert (manually)\nMe: lol, this is just an abbrev. for \u0026ldquo;Action Item\u0026rdquo;. Might as well have been \u0026ldquo;TODO:\u0026rdquo;\nCoworker: AI is such a buzzword nowadays, looks like I\u0026rsquo;m starting to get brainwashed to see it in everything","title":"Slack: AI plug-in"},{"content":"TL;DR: Shift + \u0026lt;DEL\u0026gt;.\nPerhaps you don\u0026rsquo;t have a DEL key in your keyboard. Macbooks can emit DEL with Fn + Delete (Delete here is the traditional Backspace).\n","permalink":"https://www.perrotta.dev/2024/09/chromium-delete-entry-in-url-bar/","summary":"\u003cp\u003e\u003cstrong\u003eTL;DR\u003c/strong\u003e: \u003ccode\u003eShift + \u0026lt;DEL\u0026gt;\u003c/code\u003e.\u003c/p\u003e","title":"Chromium: Delete entry in URL bar"},{"content":"less(1): you use SPC to scroll forward. How to scroll back?\nI would think Shift + SPC would be the sought shortcut, but it isn\u0026rsquo;t.\nPress b (short for \u0026ldquo;backward(s)\u0026rdquo;).\n\u0026hellip;which also implies: use f to scroll forward.\n","permalink":"https://www.perrotta.dev/2024/09/less1-scroll-back-one-page/","summary":"\u003cp\u003e\u003ccode\u003eless(1)\u003c/code\u003e: you use \u003ccode\u003eSPC\u003c/code\u003e to scroll forward. How to scroll back?\u003c/p\u003e","title":"less(1): scroll back one page"},{"content":"I wanted to create a new secure passphrase for my WLAN (Wi-Fi).\nIt\u0026rsquo;s always an option to use the password generator built into my password manager, however I wanted to take this opportunity to employ Diceware:\nDiceware Password Generator Generate high-entropy passwords the easy way!\nThe original source has the best explanation: https://diceware.dmuth.org/.\nOr you could read Wikipedia as well.\nFor example, for this post:\nScratchObscureTrendHumiliate\nNumber of possible passwords: 3 quadrillion\nGood (and memorable!) enough for me.\n","permalink":"https://www.perrotta.dev/2024/09/diceware-passphrases/","summary":"\u003cp\u003eI wanted to create a new secure passphrase for my WLAN (Wi-Fi).\u003c/p\u003e","title":"Diceware passphrases"},{"content":"https://qifi.org/ is a pure JS Wi-Fi QR Code generator.\nInput:\nSSID Passphrase Output: QR Code with the appropriate information your friends can use from Android / iOS to log into your Wi-Fi.\nPreferably use this for your guest network for better isolation / security.\n","permalink":"https://www.perrotta.dev/2024/09/wi-fi-qr-code-generator/","summary":"https://qifi.org/ is a pure JS Wi-Fi QR Code generator.\nInput:\nSSID Passphrase Output: QR Code with the appropriate information your friends can use from Android / iOS to log into your Wi-Fi.\nPreferably use this for your guest network for better isolation / security.","title":"Wi-Fi QR code generator"},{"content":"https://endoflife.date/ is like Repology but, instead of package versions, it tracks the end of life of various packages, linux distributions, frameworks, etc.\nFor example, Alpine Linux: https://endoflife.date/alpine\n","permalink":"https://www.perrotta.dev/2024/09/end-of-life-package-info/","summary":"\u003cp\u003e\u003ca href=\"https://endoflife.date/\"\u003ehttps://endoflife.date/\u003c/a\u003e is like \u003ca href=\"https://repology.org/\"\u003eRepology\u003c/a\u003e but, instead\nof package versions, it tracks the \u003cem\u003eend of life\u003c/em\u003e of various packages, linux\ndistributions, frameworks, etc.\u003c/p\u003e","title":"End of life package info"},{"content":"Given an AWS S3 bucket, how to recursively list all objects within it that match a given suffix?\nThe following example searches for all objects that end with thiagowfx.\naws s3 rm --profile {aws_profile} s3://example.com/my/path/ --recursive --dryrun --exclude \u0026#39;*\u0026#39; --include \u0026#34;*thiagowfx\u0026#34; This is a hack with a dry-run deletion operation.\nAlternatively, use aws s3 ls1 plus grep / awk / sed:\naws s3 ls --profile {aws_profile} s3://example.com/my/path | awk -F \u0026#39; \u0026#39; \u0026#39;{print $4}\u0026#39; | grep \u0026#39;thiagowfx$\u0026#39; Why would you use rm in lieu of ls? Mostly because of the built-in --include / --exclude options. If you happen to have millions of objects in your S3 bucket, then you do not need to list them all.\nhttps://docs.aws.amazon.com/cli/latest/reference/s3/ls.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/09/aws-s3-search-by-suffix/","summary":"\u003cp\u003eGiven an AWS S3 bucket, how to recursively list all objects within it that match\na given suffix?\u003c/p\u003e","title":"AWS S3: search by suffix"},{"content":"Let\u0026rsquo;s say you want to submit a quick, perhaps even trivial, one-off pull request (PR).\nOne step you can do to minimize the number of decisions1 to make is to have a git alias that creates a branch with an arbitrary name.\n% cat ~/.gitconfig # ... [alias] nbt = !git nb \\\u0026#34;thiagowfx/$(shuf -n1 /usr/share/dict/words | tr \\\u0026#34;[:upper:]\\\u0026#34; \\\u0026#34;[:lower:]\\\u0026#34;)\\\u0026#34; Running git nbt2 will create a branch such as thiagowfx/foo.\nThe {username}/ prefix is to make it easier to attribute your branches to yourself, which can be handy when working collaboratively with other engineers in a team.\nThe word after the slash is sourced from dictionary words on your system.\nSee https://en.wikipedia.org/wiki/Decision_fatigue, https://en.wikipedia.org/wiki/The_Paradox_of_Choice.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nnbt stands for \u0026ldquo;new branch throwaway\u0026rdquo;.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/09/git-create-a-throwaway-branch/","summary":"\u003cp\u003eLet\u0026rsquo;s say you want to submit a quick, perhaps even trivial, one-off pull request\n(PR).\u003c/p\u003e\n\u003cp\u003eOne step you can do to minimize the number of decisions\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e to make is to have\na git alias that creates a branch with an arbitrary name.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e% cat ~/.gitconfig\n# ...\n[alias]\nnbt = !git nb \\\u0026#34;thiagowfx/$(shuf -n1 /usr/share/dict/words | tr \\\u0026#34;[:upper:]\\\u0026#34; \\\u0026#34;[:lower:]\\\u0026#34;)\\\u0026#34;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eRunning \u003ccode\u003egit nbt\u003c/code\u003e\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e will create a branch such as \u003ccode\u003ethiagowfx/foo\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003e{username}/\u003c/code\u003e prefix is to make it easier to attribute your branches to\nyourself, which can be handy when working collaboratively with other engineers\nin a team.\u003c/p\u003e\n\u003cp\u003eThe word after the slash is sourced from dictionary words on your system.\u003c/p\u003e","title":"git: create a throwaway branch"},{"content":"I keep forgetting what the correct syntax is.\nThis reference is the best: https://yaml-multiline.info/\nSummary:\n\u0026gt; eats newlines, | keeps them. Appending a - to either of these eats the newline(s) at the end. Omitting it keeps one. Appending a + preserves all newlines at the end. ","permalink":"https://www.perrotta.dev/2024/09/yaml-multiline-strings/","summary":"\u003cp\u003eI keep forgetting what the correct syntax is.\u003c/p\u003e\n\u003cp\u003eThis reference is the best: \u003ca href=\"https://yaml-multiline.info/\"\u003ehttps://yaml-multiline.info/\u003c/a\u003e\u003c/p\u003e","title":"YAML multiline strings"},{"content":"In theory this ought to be really easy:\n$ helm ls -h [...] -a, --all show all releases without any filter applied -A, --all-namespaces list releases across all namespaces So\u0026hellip;one of these should work, right?\nhelm ls -a helm ls -A I have a sms-service running in the cluster in the default namespace. If I grep any of the previous commands to sms, it does not show up though.\nThen\u0026hellip;maybe helm ls -a -A? Nope.\nHere\u0026rsquo;s what works:\n$ helm ls -a --max 9999 | grep sms sms-service default 356 2024-08-26 14:09:50.705277885 +0000 UTC\tdeployed\tsms-v0.1.0 Kinda ridiculous, eh?\n-m, --max int maximum number of releases to fetch (default 256) From https://helm.sh/docs/helm/helm_list/:\nBy default, up to 256 items may be returned. To limit this, use the \u0026lsquo;\u0026ndash;max\u0026rsquo; flag. Setting \u0026lsquo;\u0026ndash;max\u0026rsquo; to 0 will not return all results. Rather, it will return the server\u0026rsquo;s default, which may be much higher than 256. Pairing the \u0026lsquo;\u0026ndash;max\u0026rsquo; flag with the \u0026lsquo;\u0026ndash;offset\u0026rsquo; flag allows you to page through results.\nTherefore I settle with this form:\n$ helm ls -a -A --max 9999 | grep sms Happy helming.\n","permalink":"https://www.perrotta.dev/2024/08/helm-list-all-installed-charts-in-the-cluster/","summary":"\u003cp\u003eIn theory this ought to be really easy:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e$ helm ls -h\n[...]\n  -a, --all                  show all releases without any filter applied\n  -A, --all-namespaces       list releases across all namespaces\n\u003c/code\u003e\u003c/pre\u003e","title":"Helm: list all installed charts in the cluster"},{"content":"Using browser tabs as rolling TODO lists is always a terrible idea.\nBookmarks (favorites) are slightly better, but also not a great idea.\n","permalink":"https://www.perrotta.dev/2024/08/browser-tabs-are-not-todo-lists/","summary":"Using browser tabs as rolling TODO lists is always a terrible idea.\nBookmarks (favorites) are slightly better, but also not a great idea.","title":"Browser tabs are not TODO lists"},{"content":"Few things in life spark joy so consistently as finishing a book you\u0026rsquo;ve been reading for the past two months. Books are magical.\n","permalink":"https://www.perrotta.dev/2024/08/accomplishment/","summary":"Few things in life spark joy so consistently as finishing a book you\u0026rsquo;ve been reading for the past two months. Books are magical.","title":"Accomplishment"},{"content":"There are many ways to diff two individual files:\ndiff colordiff icdiff delta But how can you diff two individual directories?\nEnter diffoscope.\nInstall it with your favorite package manager. Usage is as simple as:\ndiffoscope /path/to/dir1 /path/to/dir2 There\u0026rsquo;s also a webapp: https://try.diffoscope.org/\nIt is particularly handy in the context of Reproducible Builds.\n","permalink":"https://www.perrotta.dev/2024/08/diffoscope-recursive-diffs/","summary":"\u003cp\u003eThere are many ways to \u003ccode\u003ediff\u003c/code\u003e two individual files:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003ediff\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.colordiff.org/\"\u003e\u003ccode\u003ecolordiff\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/jeffkaufman/icdiff\"\u003e\u003ccode\u003eicdiff\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/dandavison/delta\"\u003e\u003ccode\u003edelta\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBut how can you \u003ccode\u003ediff\u003c/code\u003e two individual directories?\u003c/p\u003e","title":"Diffoscope: recursive diffs"},{"content":"Problem statement: Upon running doas apk upgrade on Alpine Linux, select packages with binaries backed by system services should be automatically restarted.\nDeb-based systems have checkrestart(8).\nOn Alpine, the best available approach as of today is https://github.com/jirutka/apk-autoupdate/:\ndoas apk add apk-autoupdate $EDITOR /etc/apk/autoupdate.conf Then make the two following changes1:\n# Because the default is \u0026#39;*\u0026#39;, which will prevent all services from restarting. services_blacklist=\u0026#34;\u0026#34; # List of services that should be restarted upon package upgrades. services_whitelist=\u0026#34;miniflux tailscale\u0026#34; From this point on, whenever there are system upgrades for the aforementioned services (doas apk upgrade), they will be automatically restarted. There\u0026rsquo;s no need for doas /etc/init.d/miniflux restart.\nh/t to @fossdd for replying to my https://github.com/jirutka/apk-autoupdate/issues/8 thread.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/08/apk-autoupdate-on-alpine-linux/","summary":"\u003cp\u003e\u003cstrong\u003eProblem statement\u003c/strong\u003e: Upon running \u003ccode\u003edoas apk upgrade\u003c/code\u003e on Alpine Linux, select\npackages with binaries backed by system services should be automatically\nrestarted.\u003c/p\u003e","title":"apk autoupdate on alpine linux"},{"content":"JSON Schema is a powerful validation tool to enforce a given structure and/or data type in JSON and YAML files.\nProblem statement: Disallow aws.iam_role1, with the implicit goal of allowing aws.iamRole. Because naming and sticking to conventions is hard.\nIllustration:\naws: iamRole: fooArn iam_role: barArn # \u0026lt;-- disallow We can accomplish this with the following schema (% cat values.schema.json):\n{ \u0026#34;$schema\u0026#34;: \u0026#34;http://json-schema.org/draft-07/schema#\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Schema that disallows aws.iam_role\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;additionalProperties\u0026#34;: true, \u0026#34;properties\u0026#34;: { \u0026#34;aws\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;additionalProperties\u0026#34;: true, \u0026#34;not\u0026#34;: { \u0026#34;required\u0026#34;: [\u0026#34;iam_role\u0026#34;] } } } } Test: helm lint should fail that schema validation with the aforementioned input.\nThis is for a helm chart.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/08/json-schema-negation/","summary":"\u003cp\u003e\u003ca href=\"https://json-schema.org/\"\u003eJSON Schema\u003c/a\u003e is a powerful validation tool to enforce\na given structure and/or data type in JSON and YAML files.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eProblem statement\u003c/strong\u003e: Disallow \u003ccode\u003eaws.iam_role\u003c/code\u003e\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e, with the implicit goal of\nallowing \u003ccode\u003eaws.iamRole\u003c/code\u003e. Because naming and sticking to conventions is hard.\u003c/p\u003e\n\u003cp\u003eIllustration:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-yaml\" data-lang=\"yaml\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eaws\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003eiamRole\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003efooArn\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003eiam_role\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003ebarArn \u003c/span\u003e \u003cspan style=\"color:#75715e\"\u003e# \u0026lt;-- disallow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"JSON schema negation"},{"content":" me: walks towards a linux laptop from someone in a party1\nme: runs cat /etc/*release\n\u0026hellip;Gentoo Linux\u0026hellip;\nA smile appears. Enlightenment irradiates.\nWhich kind of party is this?\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/08/release/","summary":"me: walks towards a linux laptop from someone in a party1\nme: runs cat /etc/*release\n\u0026hellip;Gentoo Linux\u0026hellip;\nA smile appears. Enlightenment irradiates.\nWhich kind of party is this?\u0026#160;\u0026#x21a9;\u0026#xfe0e;","title":"Release"},{"content":"Makefiles are often great bash script replacements. Instead of creating a bash script with multiple functions with various dependencies (from a topological graph viewpoint), just create a bunch of Makefile targets.\nRecently I found myself writing the following Makefile:\n# This Makefile is used to bootstrap the ArgoCD installation in the cluster. # It is idempotent. ROOT := $(shell git rev-parse --show-toplevel) TERRAFORM_GITHUB_PATH = \u0026#34;$(ROOT)/terraform/modules/global-github\u0026#34; TERRAFORM := terraform all: webhook # Update helm dependencies. helm helm-dep-update: helm dep update \u0026#34;$(ROOT)/helm/argocd\u0026#34; helm dep update \u0026#34;$(ROOT)/helm/external-secrets\u0026#34; # Edit all files. edit: edit-webhook # Modify github webhooks to the deploy servers. edit-webhook webhook-edit: \u0026#34;$$EDITOR\u0026#34; \u0026#34;$(TERRAFORM_GITHUB_PATH)/main.tf\u0026#34; # Apply github webhooks to the deploy servers. webhook: $(TERRAFORM) -chdir=\u0026#34;$(TERRAFORM_GITHUB_PATH)\u0026#34; apply .PHONY: all edit edit-webhook helm helm-dep-update webhook webhook-edit As you can see, there are a bunch of comments on top of each target. The question is: How to surface these comments to the user?\nA make help command would be great.\nWith a bit of searching, I found:\nhttps://gist.github.com/prwhite/8168133 https://marmelab.com/blog/2016/02/29/auto-documented-makefile.html These were good starting points. The technique was to transform the above Makefile into the following one:\n# This Makefile is used to bootstrap the ArgoCD installation in the cluster. # It is idempotent. ROOT := $(shell git rev-parse --show-toplevel) TERRAFORM_GITHUB_PATH = \u0026#34;$(ROOT)/terraform/modules/global-github\u0026#34; TERRAFORM := terraform all: webhook helm helm-dep-update: ## Update helm dependencies. helm dep update \u0026#34;$(ROOT)/helm/argocd\u0026#34; helm dep update \u0026#34;$(ROOT)/helm/external-secrets\u0026#34; edit: edit-webhook ## Edit all files. edit-webhook webhook-edit: ## Modify github webhooks to the deploy servers. \u0026#34;$$EDITOR\u0026#34; \u0026#34;$(TERRAFORM_GITHUB_PATH)/main.tf\u0026#34; webhook: ## Apply github webhooks to the deploy servers. $(TERRAFORM) -chdir=\u0026#34;$(TERRAFORM_GITHUB_PATH)\u0026#34; apply .PHONY: all edit edit-webhook helm helm-dep-update webhook webhook-edit Then we would add a help target to parse the comments after the ##.\nThe first source suggested:\nhelp: ## Show this help. @fgrep -h \u0026#34;##\u0026#34; $(MAKEFILE_LIST) | fgrep -v fgrep | sed -e \u0026#39;s/\\\\$$//\u0026#39; | sed -e \u0026#39;s/##//\u0026#39; It is simple and gets the job done, but the formatting was poor:\n% make help help: Show this help. helm helm-dep-update: Update helm dependencies. edit: edit-webhook Edit all files. edit-webhook webhook-edit: Modify github webhooks to the deploy servers. webhook: Apply github webhooks to the deploy servers. The second source suggested:\nhelp: @grep -E \u0026#39;^[a-zA-Z_-]+:.*?## .*$$\u0026#39; $(MAKEFILE_LIST) | sort | awk \u0026#39;BEGIN {FS = \u0026#34;:.*?## \u0026#34;}; {printf \u0026#34;\\033[36m%-30s\\033[0m %s\\n\u0026#34;, $$1, $$2}\u0026#39; \u0026hellip;which produces:\n% make help edit Edit all files. webhook Apply github webhooks to the deploy servers. The formatting is great, but alas it does not match multiple targets in a single line.\nI could have modified the targets to be like this:\nedit-webhook: webhook-edit webhook-edit: ## Description here However then I would have to duplicate their comments. I wanted to do better.\nWith a bit of LLM1 magic from GPT-4o, we can have the best of both worlds, supporting both single and multiple targets in the same line:\nhelp: ## Show this help. @grep -E \u0026#39;^[a-zA-Z_-]+([ \\t]+[a-zA-Z_-]+)*:[ \\t]*## .*$$\u0026#39; $(MAKEFILE_LIST) | awk \u0026#39;BEGIN {FS = \u0026#34;:.*?## \u0026#34;}; {n=split($$1, targets, /[ \\t]+/); for (i=1; i\u0026lt;=n; i++) {if (targets[i] != \u0026#34;\u0026#34;) printf \u0026#34;\\033[36m%-30s\\033[0m %s\\n\u0026#34;, targets[i], $$2}}\u0026#39; | sort The output (amazing!):\n% make help edit-webhook Modify github webhooks to the deploy servers. helm Update helm dependencies. helm-dep-update Update helm dependencies. help Show this help. webhook Apply github webhooks the deploy servers. webhook-edit Modify github webhooks to the deploy servers. The chat session: https://chatgpt.com/share/f9872dfa-650e-4a0c-b974-701181c237c6.\nWe could also add:\n.DEFAULT_GOAL := help \u0026hellip;to ensure that a plain make invocation behaves like make help.\nEdit(2024-08-08): I had to make one small adaptation2 to make it work with dependencies, which is the whole point of make:\nhelp: ## Show this help. @grep -E \u0026#39;^[.a-zA-Z_-]+([ \\t]+[.a-zA-Z_-]+)*:[ \\t.a-zA-Z_-]*## .*$$\u0026#39; $(MAKEFILE_LIST) | awk \u0026#39;BEGIN {FS = \u0026#34;:.*?## \u0026#34;}; {n=split($$1, targets, /[ \\t]+/); for (i=1; i\u0026lt;=n; i++) {if (targets[i] != \u0026#34;\u0026#34;) printf \u0026#34;\\033[36m%-30s\\033[0m %s\\n\u0026#34;, targets[i], $$2}}\u0026#39; | sort The previous version would not recognize the following entry:\nall-in-dev: edit webhook ## Run all necessary steps in the development environment.` If I\u0026rsquo;ll start to talk about \u0026ldquo;AI\u0026rdquo; in this blog, the very least I can do is to call them what they really are: LLMs. The \u0026ldquo;AI\u0026rdquo; acronym is currently way too hyped.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNever fully trust LLMs. Well, drop the fully. Just never trust LLMs, period.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/08/self-documented-makefiles/","summary":"\u003cp\u003e\u003ccode\u003eMakefile\u003c/code\u003es are often great \u003ccode\u003ebash\u003c/code\u003e script replacements. Instead of creating a\n\u003ccode\u003ebash\u003c/code\u003e script with multiple functions with various dependencies (from a\ntopological graph viewpoint), just create a bunch of \u003ccode\u003eMakefile\u003c/code\u003e targets.\u003c/p\u003e","title":"★ Self-documented Makefiles"},{"content":"Before checking .png images into git repositories, you should optimize them – mostly for their file size.\nThe Chromium project provides this script to do so. It boils down to installing and running the following executables:\noptipng: https://optipng.sourceforge.net/ pngcrush: https://pmt.sourceforge.io/pngcrush/ pngout: https://www.jonof.id.au/kenutils.html The first two are available via homebrew.\n","permalink":"https://www.perrotta.dev/2024/08/optimize-png-images/","summary":"\u003cp\u003eBefore checking \u003ccode\u003e.png\u003c/code\u003e images into git repositories, you should optimize them –\nmostly for their file size.\u003c/p\u003e","title":"Optimize PNG images"},{"content":"Problem statement: Given a kubernetes secret with more than one key-value pair, print all of them in plain text (i.e. base64-decoded in this context).\nEdit(2024-08-08): It turns out I made a mistake. The effect of the post below is akin to kubectl get [...] -o yaml. I\u0026rsquo;ll keep the post for bookkeeping purposes anyway.\nBasically, something like:\nkubectl cat secret -n infra-services my-cool-secret Except that there is no kubectl cat, what a shame. It would be really great if we had it, for consistency with systemctl cat. Oh well.\nThere is kubectl edit though – amen, just like systemctl edit. This will open your $EDITOR. It\u0026rsquo;s often inconvenient to copy text to the clipboard from your terminal-based editor though, due to intricacies of different terminals, shells, terminal multiplexers, and OSC-52, therefore I don\u0026rsquo;t deem this as an acceptable solution.\nThere is a simple trick though:\nEDITOR=cat kubectl edit secret -n infra-services my-cool-secret \u0026hellip;which works exactly as you would expect kubectl cat to behave. It displays a warning at the end:\nEdit cancelled, no changes made. \u0026hellip;however it can be duly ignored.\nIf we didn\u0026rsquo;t have this trick, we could have:\nwritten a bloated tool in golang to pipe to e.g. https://github.com/ashleyschuett/kubernetes-secret-decode (via) used the kubectl edit trick above, with a decent terminal and OSC-52 setup used jq with its map and base64d constructs (c.f. Stack Overflow) used kubectl describe with -o jsonpath, specifying every single field, one by one (super tedious) c.f. this previous post ","permalink":"https://www.perrotta.dev/2024/08/kubectl-print-all-secret-values-in-plain-text/","summary":"\u003cp\u003e\u003cstrong\u003eProblem statement\u003c/strong\u003e: Given a \u003ca href=\"https://kubernetes.io/docs/concepts/configuration/secret/\"\u003ekubernetes\nsecret\u003c/a\u003e with more\nthan one key-value pair, print \u003cstrong\u003eall\u003c/strong\u003e of them in plain text (i.e.\nbase64-decoded in this context).\u003c/p\u003e","title":"kubectl: print all secret values in plain text"},{"content":"I covered direnv before.\nNew scenario: Given a directory structure with two nested .envrcs:\n% tree -a top top ├── .envrc └── inner └── .envrc It turns out that, surprisingly, cd top/inner will load only top/inner/.envrc. The top-level top/.envrc will not be automatically loaded.\nThis is intentional, meant for security reasons.\nIn order to source the parent .envrc, add source_up to the inner one.\nAlternatively, source_env .. also works. I prefer source_up.\nThere\u0026rsquo;s also source_up_if_exists, which works gracefully even if there\u0026rsquo;s no env file to source upwards.\nThe full list of built-in functions is available here: https://direnv.net/man/direnv-stdlib.1.html\nReference: https://github.com/direnv/direnv/issues/757\nUse Case: Add top-level environment variables to top/.envrc. Add layout to top/inner/.envrc. For example, perhaps inner is a custom git repository / project.\n","permalink":"https://www.perrotta.dev/2024/08/direnv-nested/","summary":"\u003cp\u003eI covered \u003ca href=\"https://www.perrotta.dev/2022/01/direnv-automate-your-environment-variables/\"\u003e\u003ccode\u003edirenv\u003c/code\u003e\u003c/a\u003e before.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNew scenario\u003c/strong\u003e: Given a directory structure with two nested \u003ccode\u003e.envrc\u003c/code\u003es:\u003c/p\u003e","title":"Direnv nested"},{"content":"Although there\u0026rsquo;s never a guarantee, you can attempt to disallow \u0026ldquo;AI\u0026rdquo; from scraping the posts in your blog, in the same spirit of \u0026ldquo;DNT - Do Not Track\u0026rdquo; in modern web browsers.\nIf you have control over your robots.txt1, add something like the following to it:\nUser-agent: GPTBot Disallow: / User-agent: ChatGPT-User Disallow: / User-agent: CCBot Disallow: / User-agent: Google-Extended Disallow: / User-agent: anthropic-ai Disallow: / User-agent: Claude-Web Disallow: / User-agent: ClaudeBot Disallow: / User-agent: cohere-ai Disallow: / User-agent: PerplexityBot Disallow: / User-agent: FacebookBot Disallow: / # Default rule User-agent: * Disallow: The list above is non-exhaustive and will not be kept up-to-date, it\u0026rsquo;s just meant as a reference and/or starting point.\nYou could always use Gen AI itself to help you populate it, credits go to https://grubz.blog/ai-scrapers-post/.\nYou really should.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/08/disallow-ai-scraping/","summary":"\u003cp\u003eAlthough there\u0026rsquo;s never a guarantee, you can attempt to disallow \u0026ldquo;AI\u0026rdquo; from\nscraping the posts in your blog, in the same spirit of \u0026ldquo;DNT - Do Not Track\u0026rdquo; in\nmodern web browsers.\u003c/p\u003e\n\u003cp\u003eIf you have control over your\n\u003ca href=\"https://perrotta.dev/robots.txt\"\u003e\u003ccode\u003erobots.txt\u003c/code\u003e\u003c/a\u003e\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e,\nadd something like the following to it:\u003c/p\u003e","title":"Disallow AI scraping"},{"content":"There are many analytics and tracking offerings for blogs. I don\u0026rsquo;t want any of them.\nIn this umpteenth iteration of my personal blog, I realized from the start that I would be much happier by not having any anxiety related to trying to figure out how many visitors I have.\nIt\u0026rsquo;s such a peace of mind to write just for the sake of writing.\nA public blog is not a journal, yet it plays its own role in the grand scheme of things.\nThe world would be a happier place if we cared less about numbers; or, at least, certain numbers, such as Instagram / Twitter / TikTok followers, or story views, or blog post impressions.\nThe obvious and most sensible exception is when(ever) you have a business or a brand. Then it is important to stay in the know, when your revenue depends on it.\n","permalink":"https://www.perrotta.dev/2024/08/no-analytics/","summary":"\u003cp\u003eThere are many analytics and tracking offerings for blogs. I don\u0026rsquo;t want any of\nthem.\u003c/p\u003e","title":"No analytics"},{"content":"In this post: how to properly start docker on an Alpine Linux server.\nFirst, install docker and friends:\ndoas apk add docker docker-cli docker-compose Then start the docker service:\ndoas service docker start Check if it started successfully:\nservice docker status If not, then look at the logs:\nless /var/log/docker.log I got an error:\nfailed to start daemon: error initializing graphdriver: driver not supported The suggestion was to change the driver to overlay2:\n% $EDITOR /etc/docker/daemon.json { \u0026#34;storage-driver\u0026#34;: \u0026#34;overlay2\u0026#34; } Then restart docker.\nI got another error:\nlevel=error msg=\u0026#34;failed to mount overlay: no such device\u0026#34; storage-driver=overlay2 The suggestion was to reboot:\ndoas reboot Then start docker again:\ndoas service docker start And now everything works!\n","permalink":"https://www.perrotta.dev/2024/07/docker-on-alpine-linux/","summary":"\u003cp\u003eIn this post: how to properly start \u003ccode\u003edocker\u003c/code\u003e on an Alpine Linux server.\u003c/p\u003e","title":"Docker on Alpine Linux"},{"content":"Sometimes, when you chat with people, you notice that they tend to use a certain (sub)set of emojis frequently. The emoji choice is part of their personality.\nAfter a while, you then start to associate the emoji with the person1, even when it is used in other contexts, by other people.\nIt is such a powerful association meaning.\nWhich emoji are you?\nAlso, some people (millennials?) tend to (over)use emoticons, perhaps even more so than GIFs by Gen-Z. ;)\nCaveat: Your exes better not have their own emoji identity, or else you will remember them whenever you see the emoji.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/07/emoji-appropriation/","summary":"Sometimes, when you chat with people, you notice that they tend to use a certain (sub)set of emojis frequently. The emoji choice is part of their personality.\nAfter a while, you then start to associate the emoji with the person1, even when it is used in other contexts, by other people.\nIt is such a powerful association meaning.\nWhich emoji are you?\nAlso, some people (millennials?) tend to (over)use emoticons, perhaps even more so than GIFs by Gen-Z.","title":"Emoji appropriation"},{"content":"The best non-fiction audiobooks are the ones read by the author. Nothing beats the sense of familiarity and passion than its own creator.\n","permalink":"https://www.perrotta.dev/2024/07/audiobooks-read-by-the-author/","summary":"The best non-fiction audiobooks are the ones read by the author. Nothing beats the sense of familiarity and passion than its own creator.","title":"Audiobooks read by the author"},{"content":"Apparently there are two Unix-y ways to run commands in parallel:\nGNU parallel: https://www.gnu.org/software/parallel/ moreutils parallel: https://www.gnu.org/software/parallel/ A simple example with wc -l:\nGNU receives input from stdin:\nfind . -type f | parallel wc -l Moreutils receives input from command-line arguments:\nparallel wc -l -- $(find . -type f) Style aside, the annoyance is that they are not compatible in Debian, Ubuntu and macOS (homebrew), due to both having the parallel executable:\nhttps://superuser.com/questions/917577/how-can-i-install-gnu-parallel-alongside-moreutils-on-ubuntu-debian https://askubuntu.com/questions/1191516/what-happens-to-usr-bin-parallel-if-i-install-the-moreutils-on-top-of-the-paral Debian thread wherein no consensus is achieved: https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=597050\nI will stick to moreutils due to its simplicity and richer suite. Nonetheless, it would be better if we didn\u0026rsquo;t have binary clashes like this. It is really annoying and developer-unfriendly.\nOf course, my favorite Linux distributions do not have this issue:\n% apk info -L moreutils | grep parallel usr/bin/parallel-moreutils \u0026hellip;Alpine just installs parallel from moreutils with another name.\nArch Linux does the same:\nmv \u0026#34;$pkgdir\u0026#34;/usr/bin/parallel \u0026#34;$pkgdir\u0026#34;/usr/bin/parallel-moreutils Moreover, GNU Parallel has an annoying citation notice, which the Arch Linux package helpfully removes1.\nSome context. Not everyone works in academia. Also it\u0026rsquo;s quite weird that the program is called \u0026ldquo;GNU parallel\u0026rdquo;, is published under the GPL, and yet does not seemingly follow the GPL guidelines. This is the official FAQ of the program.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/07/parallel/","summary":"\u003cp\u003eApparently there are two Unix-y ways to run commands in parallel:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGNU parallel: \u003ca href=\"https://www.gnu.org/software/parallel/\"\u003ehttps://www.gnu.org/software/parallel/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003emoreutils parallel: \u003ca href=\"https://www.gnu.org/software/parallel/\"\u003ehttps://www.gnu.org/software/parallel/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","title":"Parallel"},{"content":"When presenting your screen over video calls, always remember to zoom in.\nWhen presenting your terminal, always remember to use a decent color scheme, with enough contrast.\nWhen making demos in conferences, always remember to zoom in.\nIt\u0026rsquo;s not about you, it\u0026rsquo;s about your audience.\n","permalink":"https://www.perrotta.dev/2024/07/zoom/","summary":"\u003cp\u003eWhen presenting your screen over video calls, always remember to zoom in.\u003c/p\u003e","title":"Zoom"},{"content":"This post is a reply to https://tilde.town/~kzimmermann/articles/installing_alpine_manpages.html.\nThe author describes their experience while attempting to install all man pages for all the packages in use in their system.\nThe breakdown progression has some valuable insights on how a typical Unix sysadmin addresses a problem. I tend to adopt a similar approach when entering unknown territory.\nHowever, in Alpine Linux, there is a better way.\nSolution There is a docs metapackage:\n% apk info docs docs-0.2-r6 description: Meta package for pulling in all documentation docs-0.2-r6 webpage: https://alpinelinux.org docs-0.2-r6 installed size: 4096 B All you have to do is:\n% doas apk add docs (1/125) Installing mandoc-doc (1.14.6-r13) (2/125) Installing docs (0.2-r6) (3/125) Installing libseccomp-doc (2.5.5-r1) (4/125) Installing busybox-doc (1.36.1-r31) [...] Likewise, it is trivial to get rid of all man pages:\n% doas apk del docs I would like to give a few other suggestions to the author, if we were to assume there is no docs metapackage:\nStep 2: You could also cat /etc/apk/world (reference).\nStep 4: combine from moreutils is more user-friendly than comm. I need to look up how to use comm every single time, whereas combine is much easier to remember.\nAppendix This was also a typical xyproblem example:\nWhat is the attempt? \u0026ldquo;I want to install, via apk add, all foo-doc packages for every foo package on my system\u0026rdquo;.\nWhat is the end goal? \u0026ldquo;I want to install all man pages for the installed packages on my system\u0026rdquo;.\n","permalink":"https://www.perrotta.dev/2024/07/alpine-linux-how-to-install-all-manpages-idiomatically/","summary":"\u003cp\u003eThis post is a reply to\n\u003ca href=\"https://tilde.town/~kzimmermann/articles/installing_alpine_manpages.html\"\u003ehttps://tilde.town/~kzimmermann/articles/installing_alpine_manpages.html\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe author describes their experience while attempting to install all man pages\nfor all the packages in use in their system.\u003c/p\u003e\n\u003cp\u003eThe breakdown progression has some valuable insights on how a typical Unix\nsysadmin addresses a problem. I tend to adopt a similar approach when entering\nunknown territory.\u003c/p\u003e\n\u003cp\u003eHowever, in Alpine Linux, there is a better way.\u003c/p\u003e","title":"Alpine Linux: How to install all manpages (idiomatically)"},{"content":"I wanted to add a new kind of post to this blog, wherein I transcribe conversations (a dialogue).\nInitially, I formatted conversations like this:\nAlice: Hi Bob: Hello In code:\n- Alice: Hi - Bob: Hello But that\u0026rsquo;s just an ordinary list.\nI thought about using em-dashes next, but they were meh (–).\nThen the lazy web helped me with inspirations, and I settled with the following format:\nAlice: Hi\nBob: Hello\nIn code:\n\u0026gt; **Alice**: Hi \u0026gt; \u0026gt; **Bob**: Hello It combines blockquotes with bolding the speaker. Simple, yet effective.\nSource: https://blog.jakelee.co.uk/markdown-conversation-formatting/\n","permalink":"https://www.perrotta.dev/2024/07/format-dialogues-in-markdown/","summary":"I wanted to add a new kind of post to this blog, wherein I transcribe conversations (a dialogue).\nInitially, I formatted conversations like this:\nAlice: Hi Bob: Hello In code:\n- Alice: Hi - Bob: Hello But that\u0026rsquo;s just an ordinary list.\nI thought about using em-dashes next, but they were meh (–).\nThen the lazy web helped me with inspirations, and I settled with the following format:\nAlice: Hi","title":"Format dialogues in markdown"},{"content":" Coworker 1: We need new fans for the office. You are welcome to send some suggestions.\nCoworker 2: Let\u0026rsquo;s start a thread on Slack and post some links to Amazon.\nCoworker 3: Yes, we could start a new channel there. It should be called OnlyFans.\n","permalink":"https://www.perrotta.dev/2024/07/fans/","summary":"Coworker 1: We need new fans for the office. You are welcome to send some suggestions.\nCoworker 2: Let\u0026rsquo;s start a thread on Slack and post some links to Amazon.\nCoworker 3: Yes, we could start a new channel there. It should be called OnlyFans.","title":"Fans"},{"content":"Frequently:\nkubectl logs -n argocd argocd-repo-server-5d59975687-dxnh7 But how do you know what hash to use after server-?\nOption 1): TAB auto-completion:\nkubectl logs -n argocd argocd-repo-server-\u0026lt;TAB\u0026gt; Caveats:\ntab completion for kubectl isn\u0026rsquo;t always properly set up if there is more than one pod, you would have to type in the first few letters of the hash before hitting TAB again, that\u0026rsquo;s annoying and non-ergonomic Option 2): subshell\nkubectl logs -n argocd $(kubectl get pod -n argocd | grep argocd-repo-server | cut -f1 -d\u0026#39; \u0026#39; | head -1) Caveats:\na lot of typing head -1 is necessary in case there are multiple pods1 Option 3): get all logs from all pods that match a given label\nkubectl logs -n argocd -l app=app.kubernetes.io/instance=argocd [--all-containers] [--ignore-errors] This is much more ergonomic.\nYou still need to figure out what label to use though:\n$ kubectl describe pod -n argocd argocd-repo-server-5d59975687-dxnh7 | grep -A7 -i labels: Labels: app.kubernetes.io/component=repo-server app.kubernetes.io/instance=argocd app.kubernetes.io/managed-by=Helm app.kubernetes.io/name=argocd-repo-server app.kubernetes.io/part-of=argocd app.kubernetes.io/version=v2.11.4 helm.sh/chart=argo-cd-7.3.4 pod-template-hash=5d59975687 Source: https://stackoverflow.com/questions/33069736/how-do-i-get-logs-from-all-pods-of-a-kubernetes-replication-controller\nHappy logging!\nOr tail -1, for that matter.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/07/kubectl-get-all-logs-from-all-pods/","summary":"\u003cp\u003eFrequently:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ekubectl logs -n argocd argocd-repo-server-5d59975687-dxnh7\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eBut how do you know what hash to use after \u003ccode\u003eserver-\u003c/code\u003e?\u003c/p\u003e","title":"kubectl: get all logs from all pods"},{"content":"When retrieving opaque secrets with kubectl, one will often invoke this typical command:\nubuntu@ubuntu:~ $ kubectl get secret -n argocd argocd-github-webhook-secret -o jsonpath=\u0026#39;{.data.value}\u0026#39; eW91IGFyZSBjdXJpb3VzIGFyZW50IHlvdQ==ubuntu@ubuntu:~ $ But isn\u0026rsquo;t this ugly? The prompt is concatenated with the output.\nIt turns out jsonpath can emit a newline for improved readability:\nubuntu@ubuntu:~ $ kubectl get secret -n argocd argocd-github-webhook-secret -o jsonpath=\u0026#39;{.data.value}{\u0026#34;\\n\u0026#34;}\u0026#39; eW91IGFyZSBjdXJpb3VzIGFyZW50IHlvdQ== ubuntu@ubuntu:~ $ Note that the {\u0026quot;\\n\u0026quot;} must be quoted.\nAnd then you could pipe it to base64 -d afterwards, as usual, and it\u0026rsquo;s a no-op:\n$ kubectl get secret -n argocd argocd-github-webhook-secret -o jsonpath=\u0026#39;{.data.value}{\u0026#34;\\n\u0026#34;}\u0026#39; | base64 -d you are curious arent youubuntu@ubuntu:~ $ However, the base64 output also swallows the newline. This can be resolved with a simple echo:\n$ kubectl get secret -n argocd argocd-github-webhook-secret -o jsonpath=\u0026#39;{.data.value}{\u0026#34;\\n\u0026#34;}\u0026#39; | base64 -d \u0026amp;\u0026amp; echo you are curious arent you ubuntu@ubuntu:~ $ Newlines are hard, eh?\n","permalink":"https://www.perrotta.dev/2024/07/kubectl-get-secret-with-jsonpath-and-add-a-newline/","summary":"\u003cp\u003eWhen retrieving opaque secrets with \u003ccode\u003ekubectl\u003c/code\u003e, one will often invoke this\ntypical command:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eubuntu@ubuntu:~ $ kubectl get secret -n argocd argocd-github-webhook-secret -o jsonpath\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;{.data.value}\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eeW91IGFyZSBjdXJpb3VzIGFyZW50IHlvdQ\u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003eubuntu@ubuntu:~ $\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eBut isn\u0026rsquo;t this ugly? The prompt is concatenated with the output.\u003c/p\u003e","title":"kubectl: get secret with jsonpath and add a newline"},{"content":"icdiff is a simple diffing tool written in python that comes with two sensible out-of-the-box defaults:\ncolored diff output side-by-side diff The usual diff tool in Unix system has some basic color support1. Nonetheless, sometimes the output of icdiff is better, with little effort or memorization.\nGiven two files (pardon my lack of creativity, but at least you know it\u0026rsquo;s not AI generated):\n% cat first gustavo fring walter white % cat second walter white jesse pinkman Plain diff output (terrible, IMHO):\n% diff first second 1,2d0 \u0026lt; gustavo \u0026lt; fring 4a3,4 \u0026gt; jesse \u0026gt; pinkman Better diff output:\n% diff --color=auto -uN first second --- first\t2024-07-09 10:50:01 +++ second\t2024-07-09 10:50:08 @@ -1,4 +1,4 @@ -gustavo -fring walter white +jesse +pinkman \u0026hellip;which is why I have alias diff=\u0026quot;diff --color=auto -uN\u0026quot; in my shell config / dotfiles.\nWith icdiff:\n% icdiff first second first second gustavo fring walter walter white white jesse pinkman Two observations:\nin this blog you cannot see the colors, therefore this example is meh icdiff will happily utilize your full terminal width, positioning the second column quite to the right in case, thereby fully utilizing the available space And there\u0026rsquo;s also colordiff.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/07/icdiff-side-by-side-diff/","summary":"\u003cp\u003e\u003ca href=\"https://www.jefftk.com/icdiff\"\u003e\u003ccode\u003eicdiff\u003c/code\u003e\u003c/a\u003e is a simple diffing tool written in\npython that comes with two sensible out-of-the-box defaults:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ecolored diff output\u003c/li\u003e\n\u003cli\u003eside-by-side diff\u003c/li\u003e\n\u003c/ul\u003e","title":"icdiff: side-by-side diff"},{"content":"Since the end of the last year it\u0026rsquo;s possible to add callouts to markdown files on GitHub. They are treated specially and rendered in a visually distinct way:\n\u0026gt; [!NOTE] \u0026gt; Highlights information that users should take into account, even when skimming. \u0026gt; [!TIP] \u0026gt; Optional information to help a user be more successful. \u0026gt; [!IMPORTANT] \u0026gt; Crucial information necessary for users to succeed. \u0026gt; [!WARNING] \u0026gt; Critical content demanding immediate user attention due to potential risks. \u0026gt; [!CAUTION] \u0026gt; Negative potential consequences of an action. In the spirit of xg2xg, this feature has a direct 1:1 mapping with the classic g3doc callouts at Google (except for \u0026ldquo;caution\u0026rdquo;):\nNote: This is a note. Tip: This is a tip. Warning: This is a warning. Important: This is important. Sources:\nhttps://github.blog/changelog/2023-12-14-new-markdown-extension-alerts-provide-distinctive-styling-for-significant-content/ https://github.com/orgs/community/discussions/16925 ","permalink":"https://www.perrotta.dev/2024/07/github-flavoured-markdown-callouts/","summary":"\u003cp\u003eSince the end of the last year it\u0026rsquo;s possible to add callouts to markdown files\non GitHub. They are treated specially and rendered in a visually distinct way:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-md\" data-lang=\"md\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003e\u0026gt; \u003c/span\u003e\u003cspan style=\"font-style:italic\"\u003e[!NOTE]\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"font-style:italic\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003e\u0026gt; \u003c/span\u003e\u003cspan style=\"font-style:italic\"\u003eHighlights information that users should take into account, even when skimming.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"font-style:italic\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003e\u0026gt; \u003c/span\u003e\u003cspan style=\"font-style:italic\"\u003e[!TIP]\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"font-style:italic\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003e\u0026gt; \u003c/span\u003e\u003cspan style=\"font-style:italic\"\u003eOptional information to help a user be more successful.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"font-style:italic\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003e\u0026gt; \u003c/span\u003e\u003cspan style=\"font-style:italic\"\u003e[!IMPORTANT]\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"font-style:italic\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003e\u0026gt; \u003c/span\u003e\u003cspan style=\"font-style:italic\"\u003eCrucial information necessary for users to succeed.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"font-style:italic\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003e\u0026gt; \u003c/span\u003e\u003cspan style=\"font-style:italic\"\u003e[!WARNING]\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"font-style:italic\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003e\u0026gt; \u003c/span\u003e\u003cspan style=\"font-style:italic\"\u003eCritical content demanding immediate user attention due to potential risks.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"font-style:italic\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003e\u0026gt; \u003c/span\u003e\u003cspan style=\"font-style:italic\"\u003e[!CAUTION]\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"font-style:italic\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003e\u0026gt; \u003c/span\u003e\u003cspan style=\"font-style:italic\"\u003eNegative potential consequences of an action.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eIn the spirit of \u003ca href=\"https://github.com/jhuangtw/xg2xg\"\u003exg2xg\u003c/a\u003e, this feature has a\ndirect 1:1 mapping with the classic g3doc callouts at Google (except for\n\u0026ldquo;caution\u0026rdquo;):\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-md\" data-lang=\"md\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eNote: This is a note.\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eTip: This is a tip.\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eWarning: This is a warning.\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eImportant: This is important.\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eSources:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.blog/changelog/2023-12-14-new-markdown-extension-alerts-provide-distinctive-styling-for-significant-content/\"\u003ehttps://github.blog/changelog/2023-12-14-new-markdown-extension-alerts-provide-distinctive-styling-for-significant-content/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/orgs/community/discussions/16925\"\u003ehttps://github.com/orgs/community/discussions/16925\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","title":"Github-flavoured Markdown: Callouts"},{"content":"Yahoo! Pipes is not around anymore, however there is a modern replacement for it: https://www.pipes.digital/.\nIt was on Hacker News recently: https://news.ycombinator.com/item?id=40841980\nHere is a practical use case for it: to filter out unwanted episodes from podcast RSS feeds.\nI follow Jovem Nerd - Nerdcasts. They are a Brazilian podcast.\nHowever, in the recent years, they added a lot of other (unrelated) podcasts from their network to the main NerdCast feed, effectively polluting it. The cleaner and elegant action to do would have been to publish individual RSS feeds for each of their podcast series, and perhaps an additional one to contain all of them, for those who would like to subscribe to everything. But that\u0026rsquo;s not what they do.\nThis is an effective marketing strategy for them, because the smaller podcast series benefit from the exposure in the main feed. At the same time, this is hostile to listeners, as we end up receiving bundling of episodes we are not looking for.\nEnter the new Pipes.\nWith it I can build the following pipeline:\nfetch the nerdcast feed: https://api.jovemnerd.com.br/feed-nerdcast/ filter only the items that contain \u0026ldquo;NerdCast\u0026rdquo; in item.title publish a new feed The end result (\u0026ldquo;NerdCast ONLY\u0026rdquo;):\nPreview: https://www.pipes.digital/feedpreview/VqaEW4qJ Raw feed: https://www.pipes.digital/feed/VqaEW4qJ? You are welcome.\nDocumentation: https://www.pipes.digital/docs\n","permalink":"https://www.perrotta.dev/2024/07/pipes-rss-manipulation/","summary":"\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Yahoo!_Pipes\"\u003eYahoo! Pipes\u003c/a\u003e is not around\nanymore, however there is a modern replacement for it: \u003ca href=\"https://www.pipes.digital/\"\u003ehttps://www.pipes.digital/\u003c/a\u003e.\u003c/p\u003e","title":"Pipes: RSS Manipulation"},{"content":"Apartment hunting in Germany is comparable to dating: it is a numbers game w.r.t. supply and demand1, first impressions matter a lot, you often get ghosted, and once you finally make it through the first few rounds there is a great possibility of rejection for unknown reasons down the road.\nin the straight world, women are akin to landlords\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/07/rejection/","summary":"Apartment hunting in Germany is comparable to dating: it is a numbers game w.r.t. supply and demand1, first impressions matter a lot, you often get ghosted, and once you finally make it through the first few rounds there is a great possibility of rejection for unknown reasons down the road.\nin the straight world, women are akin to landlords\u0026#160;\u0026#x21a9;\u0026#xfe0e;","title":"Rejection"},{"content":"When attending tech conferences, wear T-shirts from previous conferences, or from prior editions of the same conference. It is a good icebreaker when people recognize them.\nOf course, there is no need to buy a T-shirt from every single conference you attend; owning one or two of them is enough.\n","permalink":"https://www.perrotta.dev/2024/07/t-shirt/","summary":"When attending tech conferences, wear T-shirts from previous conferences, or from prior editions of the same conference. It is a good icebreaker when people recognize them.\nOf course, there is no need to buy a T-shirt from every single conference you attend; owning one or two of them is enough.","title":"t-shirt"},{"content":"Given a terraform/modules directory tree, we would like to globally update the minimum required terraform version in all modules.\nOption 1: Use ack or fd + sed. Option 2: Use tfupdate, which can be installed via homebrew or your favorite package manager (c.f. repology). $ tfupdate terraform -r terraform/modules -v \u0026#39;~\u0026gt; 1.6.6\u0026#39; ","permalink":"https://www.perrotta.dev/2024/07/terraform-perform-a-global-update/","summary":"\u003cp\u003eGiven a \u003ccode\u003eterraform/modules\u003c/code\u003e directory tree, we would like to globally update the\nminimum required terraform version in all modules.\u003c/p\u003e","title":"Terraform: perform a global update"},{"content":"Given, for example, 0 0 * * *, how do you figure out when it will run?\nOption 1: Read the docs! The ArchWiki is frequently a great reference. Alternatively, use your favorite search engine. Option 2: Ask ChatGPT! A simple cron: 0 0 * * * prompt is enough. No need to embezzle it with explain what this does or what does this do?. Option 3: Paste it into https://crontab.guru/. ","permalink":"https://www.perrotta.dev/2024/07/explain-a-crontab-expression/","summary":"\u003cp\u003eGiven, for example, \u003ccode\u003e0 0 * * *\u003c/code\u003e, how do you figure out when it will run?\u003c/p\u003e","title":"Explain a crontab expression"},{"content":"If you find yourself in a situation wherein http://localhost:1313 has issues, you can use a domain that redirects to localhost. For example:\nhttp://localdev.me:1313/ http://demo.localdev.me:1313/ When I’m doing local development, I sometimes need a domain name that routes back to localhost. I’ve long run into cases where I need subdomains and ended up modifying my local hosts file. I’ve used this for a variety of situations going back for a long time. From Kubernetes ingress work to web development.\nlocaldev.me DNS is served through amazon. The domain name and any subdomains point to 127.0.0.1.\nThe next time you need a custom domain or subdomain for local development, instead of hancking your hosts file you might consider localdev.me.\nSource: https://codeengineered.com/blog/2022/localdev-me/\n","permalink":"https://www.perrotta.dev/2024/07/localhost-domain/","summary":"\u003cp\u003eIf you find yourself in a situation wherein http://localhost:1313 has issues,\nyou can use a domain that redirects to localhost. For example:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://localdev.me:1313/\"\u003ehttp://localdev.me:1313/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://demo.localdev.me:1313/\"\u003ehttp://demo.localdev.me:1313/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","title":"Localhost domain"},{"content":" Me: Do you speak English?\nFoe: A little / Meh / Ein bisschen / Mixed\n/foe speaks perfect English\nFoe: Sprechen Sie Deutsch?\nMe: Nur ein bisschen / A2\n/me speaks lousy Deutsch\n","permalink":"https://www.perrotta.dev/2024/07/a-little/","summary":"Me: Do you speak English?\nFoe: A little / Meh / Ein bisschen / Mixed\n/foe speaks perfect English\nFoe: Sprechen Sie Deutsch?\nMe: Nur ein bisschen / A2\n/me speaks lousy Deutsch","title":"a little"},{"content":"If you are arranging a meeting with someone in the other side of the globe, it is good courtesy to send your availability in their local timezone.\n","permalink":"https://www.perrotta.dev/2024/07/timezones/","summary":"If you are arranging a meeting with someone in the other side of the globe, it is good courtesy to send your availability in their local timezone.","title":"timezones"},{"content":"Let\u0026rsquo;s say the files you want to copy are in ~/Downloads.\nStart a local HTTP server on your laptop:\n$ cd ~/Downloads $ python3 -m http.server Serving HTTP on :: port 8000 (http://[::]:8000/) ... Find the IP address of your laptop within your LAN:\n$ ifconfig # macOS $ ip addr # linux Now go to your Steam Deck, access http://\u0026lt;ip\u0026gt;:8000 via the installed web browser, and download your files.\nAlternatively, run wget / curl in a terminal.\n","permalink":"https://www.perrotta.dev/2024/06/copy-files-from-laptop-to-steam-deck/","summary":"\u003cp\u003eLet\u0026rsquo;s say the files you want to copy are in \u003ccode\u003e~/Downloads\u003c/code\u003e.\u003c/p\u003e","title":"Copy files from laptop to Steam Deck"},{"content":"Whenever we play badminton and someone hits the shuttle in such a way that it gently touches the net from the top, followed up by a point, the person instantly apologizes. It makes me smile every single time this unspoken tradition happens.\n","permalink":"https://www.perrotta.dev/2024/06/entschuldigung/","summary":"Whenever we play badminton and someone hits the shuttle in such a way that it gently touches the net from the top, followed up by a point, the person instantly apologizes. It makes me smile every single time this unspoken tradition happens.","title":"Entschuldigung"},{"content":"When asking questions or filing bugs / feature requests in the internet, I often refer to these two invaluable resources:\nHow To Ask Questions The Smart Way by Eric S. Raymond XY problem: The XY problem is asking about your attempted solution rather than your actual problem. This leads to enormous amounts of wasted time and energy, both on the part of people asking for help, and on the part of those providing help. Here is a recent example: https://github.com/23andMe/Yamale/issues/250\nhttps://xyproblem.info / Motivation: Somewhat related to #228: It\u0026rsquo;s not currently possible to exclude files from a given directory. In the context of developing Kubernetes GitOps repository this is often an issue. The ability to run yamale on individual files would address it.\n","permalink":"https://www.perrotta.dev/2024/06/xy-problem/","summary":"\u003cp\u003eWhen asking questions or filing bugs / feature requests in the internet, I often\nrefer to these two invaluable resources:\u003c/p\u003e","title":"★ XY problem"},{"content":"Whenever the files are in the same directory, vidir from moreutils is the best interactive tool.\nIf files are scattered across multiple directories, consider using the rename utility from util-linux.\nA simple example to rename all readme.md files to README.md for consistency:\nrename \u0026#39;s/readme\\.md/README.md/\u0026#39; **/* A more recent example (2024-07-25):\nrename \u0026#39;s/promtail-global-testing/promtail-global/g\u0026#39; **/* If there are nested directories that match the expression, you\u0026rsquo;ll need to run rename multiple times.\n","permalink":"https://www.perrotta.dev/2024/06/rename-files-in-bulk/","summary":"\u003cp\u003eWhenever the files are in the same directory, \u003ccode\u003evidir\u003c/code\u003e from \u003ca href=\"https://www.perrotta.dev/2022/05/tools-you-should-know-about-moreutils/\"\u003emoreutils\u003c/a\u003e is the best\ninteractive tool.\u003c/p\u003e\n\u003cp\u003eIf files are scattered across multiple directories, consider using the\n\u003ca href=\"https://man.archlinux.org/man/rename.1.en\"\u003e\u003ccode\u003erename\u003c/code\u003e\u003c/a\u003e utility from \u003ccode\u003eutil-linux\u003c/code\u003e.\u003c/p\u003e","title":"Rename files in bulk"},{"content":"The following commands will make the shell sleep indefinitely:\nsleep inf sleep infinity Previously I would call a command such as a while true loop or the yes utility, but sleep is also handy.\n","permalink":"https://www.perrotta.dev/2024/06/sleep-forever/","summary":"\u003cp\u003eThe following commands will make the shell sleep indefinitely:\u003c/p\u003e","title":"Sleep forever"},{"content":"Given the password correct horse battery staple, we would like to bcrypt-hash it.\nXKCD Courtesy of Randall Munroe\nHere\u0026rsquo;s one way to do so via the command line:\n$ htpasswd -nbBC 10 \u0026#34;\u0026#34; \u0026#39;correct horse battery staple\u0026#39; | tr -d \u0026#39;:\\n\u0026#39; | sed \u0026#39;s/$2y/$2a/\u0026#39; \u0026hellip;which yields:\n$2a$10$HKSHfLu4l7TvOmnLkhUngu2U1pJUUw7hEU0LE1iN84S09fJsZowHm You could verify it matches e.g. via https://bcrypt-generator.com/.\nContext: ArgoCD expects a bcrypt-hashed password in its config file.\n","permalink":"https://www.perrotta.dev/2024/06/bcrypt-hash-a-password/","summary":"\u003cp\u003eGiven the password \u003ccode\u003ecorrect horse battery staple\u003c/code\u003e, we would like to bcrypt-hash\nit.\u003c/p\u003e\n\u003cfigure class=\"align-center \"\u003e\u003ca href=\"https://xkcd.com/936/\"\u003e\n    \u003cimg loading=\"lazy\" src=\"https://imgs.xkcd.com/comics/password_strength.png#center\"\n         alt=\"Through 20 years of effort, we\u0026#39;ve successfully trained everyone to use passwords that are hard for humans to remember, but easy for computers to guess.\"/\u003e \u003c/a\u003e\u003cfigcaption\u003e\n            \u003cp\u003eXKCD Courtesy of Randall Munroe\u003c/p\u003e\n        \u003c/figcaption\u003e\n\u003c/figure\u003e","title":"Bcrypt-hash a password"},{"content":"This post exemplifies an efficient workflow to shorten the edit-refresh loop when dealing with github actions.\nAssumptions You have git, jq and the gh command-line tool installed (brew install gh) You have a github actions file in ~/.github/workflows/package-release-dispatch.yml. The action has the following form: on: workflow_dispatch: inputs: ref: description: \u0026#39;Ref to build from. This can either be a SHA or a branch/tag\u0026#39; required: true type: string push: branches: - master paths: - \u0026#39;.github/workflows/package-release-dispatch.yml\u0026#39; - \u0026#39;helm/**\u0026#39; The goal is to iterate on the jobs: section of the action.\nWorkflow Trigger an action run from the command line:\ngh workflow run package-release-dispatch.yml [--ref master] [-f ref=master] --ref should be the git branch you\u0026rsquo;re working on, for example, thiagowfx/my-cool-feature.\n-f provides an input to the workflow. In this case, there\u0026rsquo;s an input named ref, which is meant to be the branch the action will act upon.\nThe action takes a little while to trigger, we can sleep to give it some time. I found that sleep 3 is a sensible value (3 seconds).\nHow to view the action?\nFirst we need to get its ID. Here\u0026rsquo;s one way to do so:\ngh run list --workflow=package-release-dispatch.yml --json databaseId --jq \u0026#39;.[0].databaseId\u0026#39; Option 1) Via the command-line gh run watch \u0026lt;id\u0026gt; It\u0026rsquo;s analogous to watch, continuously refreshing the action progress in the background, step by step. Example:\n* master Package release dispatch · 12345678 Triggered via workflow_dispatch about 1 minute ago JOBS * build (ID 23456789) ✓ Set up job ✓ Checkout source code ✓ Run azure/setup-helm@v4 ✓ Install yq ✓ Install helm cm-push plugin ✓ Set up Helm repos * Helm package all charts sans blacklist * Publish all helm packages * Post Checkout source code Option 2) Via the web browser gh run view \u0026lt;id\u0026gt; -w It will open the system web browser in the right page, pertaining to the action run.\nPutting everything together Option 1) Via the command-line gh workflow run package-release-dispatch.yml --ref master -f ref=master \u0026amp;\u0026amp; \\ sleep 3 \u0026amp;\u0026amp; \\ gh run watch $(gh run list --workflow=package-release-dispatch.yml --json databaseId --jq \u0026#39;.[0].databaseId\u0026#39;) Option 2) Via the web browser gh workflow run package-release-dispatch.yml --ref master -f ref=master \u0026amp;\u0026amp; \\ sleep 3 \u0026amp;\u0026amp; \\ gh run view $(gh run list --workflow=package-release-dispatch.yml --json databaseId --jq \u0026#39;.[0].databaseId\u0026#39;) -w ","permalink":"https://www.perrotta.dev/2024/05/github-actions-workflow/","summary":"\u003cp\u003eThis post exemplifies an efficient workflow to shorten the edit-refresh loop\nwhen dealing with github actions.\u003c/p\u003e","title":"★ Github actions workflow"},{"content":"A common scenario: there\u0026rsquo;s a new deployment you would like to roll out to AWS. Let\u0026rsquo;s say you pick \u0026ldquo;us-east-1\u0026rdquo; as your cloud region. There are multiple availability zones within it:\nus-east-1a us-east-1b us-east-1c us-east-1d us-east-1e us-east-1f Suppose you want to pick two of them for your service/app, and you don\u0026rsquo;t particularly care about which one. How to proceed?\nOption #1: Hard-coding Pick two arbitrary zones and hard-code them.\nvariable \u0026#34;availability_zones\u0026#34; { type = list(string) default = [\u0026#34;us-east-1a\u0026#34;, \u0026#34;us-east-1b\u0026#34;] } resource \u0026#34;aws_subnet\u0026#34; \u0026#34;private\u0026#34; { vpc_id = aws_vpc.chartmuseum.id cidr_block = element(var.private_subnets, count.index) availability_zone = element(var.availability_zones, count.index) count = length(var.private_subnets) } Caveat: The paradox of choice, unnecessary decision fatigue.\nOption #2: Pick the first two Use the AWS data source to dynamically find all zones, and pick the first two.\ndata \u0026#34;aws_availability_zones\u0026#34; \u0026#34;available\u0026#34; { state = \u0026#34;available\u0026#34; } resource \u0026#34;aws_subnet\u0026#34; \u0026#34;private\u0026#34; { vpc_id = aws_vpc.chartmuseum.id cidr_block = element(var.private_subnets, count.index) availability_zone = element(data.aws_availability_zones.available.names, count.index) count = length(var.private_subnets) } Note that terraform plan should display the full zone list.\nCaveat: Heavily biased towards the first two zones.\nOption #3: Random shuffling Pick two zones at random!\ndata \u0026#34;aws_availability_zones\u0026#34; \u0026#34;available\u0026#34; { state = \u0026#34;available\u0026#34; } resource \u0026#34;random_shuffle\u0026#34; \u0026#34;aws_availability_zone_names\u0026#34; { input = data.aws_availability_zones.available.names result_count = 2 } resource \u0026#34;aws_subnet\u0026#34; \u0026#34;private\u0026#34; { vpc_id = aws_vpc.chartmuseum.id cidr_block = element(var.private_subnets, count.index) availability_zone = element(random_shuffle.aws_availability_zone_names.result, count.index) count = length(var.private_subnets) } Winner: In my opinion, this is the most elegant approach.\nrandom_shuffle will output the selected regions upon running terraform apply.\n","permalink":"https://www.perrotta.dev/2024/05/terraform-aws-deployment-to-random-availability-zones/","summary":"\u003cp\u003eA common scenario: there\u0026rsquo;s a new deployment you would like to roll out to AWS.\nLet\u0026rsquo;s say you pick \u0026ldquo;us-east-1\u0026rdquo; as your cloud region. There are multiple\navailability zones within it:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eus-east-1a\u003c/li\u003e\n\u003cli\u003eus-east-1b\u003c/li\u003e\n\u003cli\u003eus-east-1c\u003c/li\u003e\n\u003cli\u003eus-east-1d\u003c/li\u003e\n\u003cli\u003eus-east-1e\u003c/li\u003e\n\u003cli\u003eus-east-1f\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSuppose you want to pick two of them for your service/app, and you don\u0026rsquo;t\nparticularly care about which one. How to proceed?\u003c/p\u003e","title":"Terraform: AWS deployment to random availability zones"},{"content":"Assume that you have a Chartmuseum container running in AWS Fargate.\nChartmuseum is a repository for helm charts. AWS Fargate is an Amazon service to run containers (\u0026ldquo;serverless\u0026rdquo;), being part of ECS (Elastic Container Service).\nProblem statement: Add a container healthcheck to the chartmuseum task definition associated with the container.\nThe official docs suggest using curl:\n[\u0026#34;CMD-SHELL\u0026#34;, \u0026#34;curl -f http://localhost/ || exit 1\u0026#34;] For Chartmuseum specifically we\u0026rsquo;re interested in its /health endpoint, as per this reference:\n[\u0026#34;CMD-SHELL\u0026#34;, \u0026#34;curl -f http://localhost/health || exit 1\u0026#34;] But we\u0026rsquo;re using port 8080:\n[\u0026#34;CMD-SHELL\u0026#34;, \u0026#34;curl -f http://localhost:8080/health || exit 1\u0026#34;] If you use this healthcheck for the official chartmuseum image (ghcr.io/helm/chartmuseum) it will fail, because the Alpine Linux environment it uses does not contain curl.\nA straightforward fix is to use wget instead:\n[\u0026#34;CMD-SHELL\u0026#34;, \u0026#34;wget -q --spider http://localhost:8080/health || exit 1\u0026#34;] --spider is needed because we do not want to download anything, -q is optional and short for \u0026ldquo;quiet\u0026rdquo;.\nThe /health endpoint merely returns a simple JSON:\n{\u0026#34;healthy\u0026#34;:true} References: https://stackoverflow.com/questions/47722898/how-can-i-make-a-docker-healthcheck-with-wget-instead-of-curl\n","permalink":"https://www.perrotta.dev/2024/05/adding-a-healthcheck-to-chartmuseum-in-aws-fargate/","summary":"\u003cp\u003eAssume that you have a \u003ca href=\"https://chartmuseum.com/\"\u003eChartmuseum\u003c/a\u003e container running\nin \u003ca href=\"https://aws.amazon.com/fargate/\"\u003eAWS Fargate\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eChartmuseum is a repository for helm charts. AWS Fargate is an Amazon service to\nrun containers (\u0026ldquo;serverless\u0026rdquo;), being part of ECS (Elastic Container Service).\u003c/p\u003e\n\u003cp\u003eProblem statement: Add a container \u003cem\u003ehealthcheck\u003c/em\u003e to the chartmuseum task\ndefinition associated with the container.\u003c/p\u003e","title":"Adding a healthcheck to chartmuseum in AWS Fargate"},{"content":"Pritunl is an Enterprise Distributed OpenVPN server.\nIn order to run its client on macOS, an .app is provided. However, using it to log in every day is quite tedious.\nLet\u0026rsquo;s automate it so that we can connect to the VPN with a single command.\nIn order to fetch the credentials in this example in a secure manner, we\u0026rsquo;ll be using 1password. pass would have worked just as fine.\nFirst, configure Pritunl via its app UI, creating a profile for your corp credentials.\nThen figure out what the profile ID you just created is:\nprofile_id=$(/Applications/Pritunl.app/Contents/Resources/pritunl-client list --json | jq -r \u0026#39;.[0].id\u0026#39;) Now, create an entry in 1Password for your Pritunl credentials associated with the profile above.\nLet\u0026rsquo;s use the op official CLI tool from 1Password to fetch the password and the OTP (one-time password) for the \u0026ldquo;Pritunl (VPN)\u0026rdquo; entry (change it accordingly).\nop_id=\u0026#34;$(op item get \u0026#39;Pritunl (VPN)\u0026#39; --format json | jq -r \u0026#39;.id\u0026#39;)\u0026#34; password=\u0026#34;$(op read \u0026#34;op://private/$op_id/password\u0026#34;)\u0026#34; otp=\u0026#34;$(op item get \u0026#34;$op_id\u0026#34; --totp)\u0026#34; Now we can use the pritunl-client to log in programmatically:\npritunl-client start \u0026#34;$profile_id\u0026#34; --password \u0026#34;$password$otp\u0026#34; Then verify it has indeed connected:\npritunl-client list The trick is that it accepts the concatenation of the password with the OTP as the password. There\u0026rsquo;s not a separate --otp flag.\nPutting everything together, we can create a function for our favorite shell:\n# Log into corp VPN pritunl_login() { local profile_id=$(/Applications/Pritunl.app/Contents/Resources/pritunl-client list --json | jq -r \u0026#39;.[0].id\u0026#39;) local op_id=\u0026#34;$(op item get \u0026#39;Pritunl (VPN)\u0026#39; --format json | jq -r \u0026#39;.id\u0026#39;)\u0026#34; local password=\u0026#34;$(op read \u0026#34;op://private/$op_id/password\u0026#34;)\u0026#34; local otp=\u0026#34;$(op item get \u0026#34;$op_id\u0026#34; --totp)\u0026#34; pritunl-client start \u0026#34;$profile_id\u0026#34; --password \u0026#34;$password$otp\u0026#34; pritunl-client list } ","permalink":"https://www.perrotta.dev/2024/05/pritunl-log-in-via-cli/","summary":"\u003cp\u003e\u003ca href=\"https://pritunl.com/\"\u003ePritunl\u003c/a\u003e is an Enterprise Distributed OpenVPN server.\u003c/p\u003e\n\u003cp\u003eIn order to run its client on macOS, an \u003ccode\u003e.app\u003c/code\u003e is provided. However, using it to\nlog in every day is quite tedious.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s automate it so that we can connect to the VPN with a single command.\u003c/p\u003e","title":"Pritunl log in via CLI"},{"content":"If you have a GitHub account configured with SSH, your public keys are available at https://github.com/$USERNAME.keys.\nFor example, mine: https://github.com/thiagowfx.keys\nAnd then let\u0026rsquo;s say you also use your full name on GitHub.\n% ssh whoami.filippo.io The authenticity of host \u0026#39;whoami.filippo.io (2a09:8280:1::a:5d6)\u0026#39; can\u0026#39;t be established. ED25519 key fingerprint is SHA256:qGAqPqtlvFBCt4LfMME3IgJqZWlcrlBMxNmGjhLVYzY. This key is not known by any other names. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added \u0026#39;whoami.filippo.io\u0026#39; (ED25519) to the list of known hosts. +---------------------------------------------------------------------+ | | | _o/ Hello Thiago Perrotta! | | | | | Did you know that ssh sends all your public keys to any server | | it tries to authenticate to? | | | | We matched them to the keys of your GitHub account, | | @thiagowfx, which are available via the GraphQL API | and at https://github.com/thiagowfx.keys | | | -- Filippo (https://filippo.io) | | | | | | P.S. The source of this server is at | | https://github.com/FiloSottile/whoami.filippo.io | | | +---------------------------------------------------------------------+ Shared connection to whoami.filippo.io closed. Then be careful when connecting to random public ssh servers when you have an intent to be anonymous.\nIt\u0026rsquo;s possible to \u0026ldquo;hide\u0026rdquo; yourself by either setting the IdentitiesOnly=yes option, or by removing all your local ssh keys altogether, even if only temporarily.\n","permalink":"https://www.perrotta.dev/2024/05/be-aware-that-your-public-ssh-keys-can-reveal-your-identity/","summary":"\u003cp\u003eIf you have a GitHub account configured with SSH, your public keys are available\nat \u003ccode\u003ehttps://github.com/$USERNAME.keys\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eFor example, mine: \u003ca href=\"https://github.com/thiagowfx.keys\"\u003ehttps://github.com/thiagowfx.keys\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAnd then let\u0026rsquo;s say you also use your full name on GitHub.\u003c/p\u003e","title":"Be aware that your public SSH keys can reveal your identity"},{"content":"I learned a neat shell trick this week. In both bash and zsh you can use the circumflex / caret (^) symbol to find \u0026amp; replace a word from the previous command.\nUsage: ^prev^next. It\u0026rsquo;s best illustrated with examples:\nterraform Instead of running:\nterraform init -var-file /path/to/foo.tfvars terraform plan -var-file /path/to/foo.tfvars terraform apply -var-file /path/to/foo.tfvars Run:\nterraform init -var-file /path/to/foo.tfvars ^init^plan ^plan^apply systemd Instead of running:\nsudo systemctl restart nginx sudo systemctl status nginx Run:\nsudo systemctl restart nginx ^restart^status one observation zsh will run the substitution right away, whereas bash will allow you to review and edit the replaced command before running it.\n","permalink":"https://www.perrotta.dev/2024/04/shell-text-substitution/","summary":"\u003cp\u003eI learned a neat shell trick this week. In both \u003ccode\u003ebash\u003c/code\u003e and \u003ccode\u003ezsh\u003c/code\u003e you can use the\ncircumflex / caret (\u003ccode\u003e^\u003c/code\u003e) symbol to find \u0026amp; replace a word from the previous\ncommand.\u003c/p\u003e\n\u003cp\u003eUsage: \u003ccode\u003e^prev^next\u003c/code\u003e. It\u0026rsquo;s best illustrated with examples:\u003c/p\u003e","title":"Shell text substitution"},{"content":"When working on Python projects, pyenv is a great python environment / version manager, especially on macOS wherein you cannot easily control the python system version.\nI\u0026rsquo;d recommend to install it with homebrew (brew install pyenv).\nThe upstream documentation is great. The commands you\u0026rsquo;ll typically use are:\npyenv versions: list all installed versions pyenv global \u0026lt;version\u0026gt;: set a specific python version for your whole system pyenv local: set a specific python version only for a specific project (directory) And then it\u0026rsquo;s handy to add the following blurb to your shell rc file to make pyenv work properly out-of-the-box:\n# pyenv: https://github.com/pyenv/pyenv if hash pyenv \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; then export PYENV_ROOT=\u0026#34;$HOME/.pyenv\u0026#34; path_munge \u0026#34;$PYENV_ROOT/bin\u0026#34; eval \u0026#34;$(pyenv init -)\u0026#34; fi Note that path_munge is a custom function, it merely appends the given argument to the $PATH.\n","permalink":"https://www.perrotta.dev/2024/03/pyenv/","summary":"\u003cp\u003eWhen working on Python projects, \u003ca href=\"https://github.com/pyenv/pyenv\"\u003e\u003ccode\u003epyenv\u003c/code\u003e\u003c/a\u003e is a\ngreat python environment / version manager, especially on macOS wherein you\ncannot easily control the python system version.\u003c/p\u003e","title":"pyenv"},{"content":"I used to have the following handy script to launch a new (fresh!) instance of Google Chrome when working on Chrome for Testing in the Browser Automation team at Google:\n#!/bin/bash # start chrome with ephemeral settings (every run of Chrome is empty) # usage: chrome-fresh out/Default/chrome TMPDIR=\u0026#34;$(mktemp -d)\u0026#34; trap \u0026#39;rm -rf \u0026#34;${TMPDIR}\u0026#34;\u0026#39; EXIT CHROME=\u0026#34;${1:-google-chrome}\u0026#34;; shift # https://github.com/GoogleChrome/chrome-launcher/blob/main/docs/chrome-flags-for-tools.md CHROME_FLAGS=\u0026#34;--use-mock-keychain\u0026#34; if [[ \u0026#34;$(uname -s)\u0026#34; == \u0026#34;Darwin\u0026#34; \u0026amp;\u0026amp; \u0026#34;$CHROME\u0026#34; == *.app ]]; then open -n \u0026#34;$CHROME\u0026#34; --args \u0026#34;$CHROME_FLAGS\u0026#34; --user-data-dir=\u0026#34;$TMPDIR\u0026#34; \u0026#34;$@\u0026#34; else # \u0026#34;Linux\u0026#34; \u0026#34;$CHROME\u0026#34; \u0026#34;$CHROME_FLAGS\u0026#34; --user-data-dir=\u0026#34;$TMPDIR\u0026#34; \u0026#34;$@\u0026#34; fi The script is self-documenting, it was properly tested on both Linux and macOS.\nThe typical use case would be to compile a new Google Chrome binary (/out/Default/chrome), and then use the script to launch it with a fresh user data directory, to ensure the previous launch settings do not interfere with the current one.\n","permalink":"https://www.perrotta.dev/2024/03/chrome-fresh-start-a-fresh-instance-of-google-chrome/","summary":"\u003cp\u003eI used to have the following handy script to launch a new (fresh!) instance of\nGoogle Chrome when working on \u003ca href=\"https://www.perrotta.dev/2024/01/google-chrome-for-testing-reliable-downloads-for-browser-automation/\"\u003eChrome for Testing\u003c/a\u003e in the Browser Automation team at\nGoogle:\u003c/p\u003e","title":"chrome-fresh: start a fresh instance of Google Chrome"},{"content":"I changed my macOS system language to German, with the intent of getting more exposure to it.\nOne negative side effect is that most binaries I execute with my shell (for example: git) are now outputting German text as well1. In hindsight, this should have been expected.\nA simple fix is to override the environment locale with English. I made the following addition to my dotfiles:\n# Force the system-wide language to English. # Both \u0026#34;en_US\u0026#34; and \u0026#34;en_CA\u0026#34; work here. # We could also optionally set LC_ALL but it is not necessary. # Verify current locale settings with `locale`. export LANG=\u0026#34;en_US\u0026#34; You could argue that it\u0026rsquo;s actually desirable to have terminal applications output German. However, nothing beats the standardization, consistency and familiarity of English for developer tooling. If I really need to have this extreme exposure one day, it\u0026rsquo;s always possible to just unset LANG.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/02/set-terminal-language-to-english/","summary":"\u003cp\u003eI changed my macOS system language to German, with the intent of \u003ca href=\"https://www.perrotta.dev/2022/04/translating-german-to-english/\"\u003egetting more\nexposure to it\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOne negative side effect is that most binaries I execute with my shell (for\nexample: \u003ccode\u003egit\u003c/code\u003e) are now outputting German text as well\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e. In hindsight, this\nshould have been expected.\u003c/p\u003e","title":"Set terminal language to English"},{"content":"Sometimes, when connecting to public Wi-Fi hotposts, especially in airports and coffee shops, the captive portal gateway required to get internet access will simply not show up.\nThe simplest way to force it to do so is to turn the Wi-Fi off and on again.\nShould it not work, the second way is to open your favorite browser and navigate to captive.apple.com (macOS, iOS).\n","permalink":"https://www.perrotta.dev/2024/02/force-captive-portal-to-open/","summary":"\u003cp\u003eSometimes, when connecting to public Wi-Fi hotposts, especially in airports and\ncoffee shops, the captive portal gateway required to get internet access will\nsimply not show up.\u003c/p\u003e","title":"Force captive portal to open"},{"content":"https://organicmaps.app/:\nOrganic Maps is a free Android \u0026amp; iOS offline maps app for travelers, tourists, hikers, drivers and cyclists based on OpenStreetMap data created by the community. It is a privacy-focused, open-source fork of [\u0026hellip;]\nOrganic Maps is one of the few applications nowadays that supports 100% of features without an active Internet connection. Install Organic Maps, download maps, throw away your SIM card, and go for a weeklong trip on a single battery charge without any byte sent to the network.\nPositive impressions:\nworks great on iOS (didn\u0026rsquo;t test on Android yet) a great companion for cycling within the city more fun and more lightweight than Google Maps low impact on device battery life extremely privacy friendly (no ads, no tracking, no push notifications, etc) extremely mobile data friendly (works fully offline) open-source, and without any IAP works out-of-the-box, no sign up necessary Negative impressions:\ndo not use it for public transit, it is terrible as it does not have real-time data integration ","permalink":"https://www.perrotta.dev/2024/02/organic-maps-off-line-maps/","summary":"\u003cp\u003e\u003ca href=\"https://organicmaps.app/\"\u003ehttps://organicmaps.app/\u003c/a\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eOrganic Maps is a free Android \u0026amp; iOS offline maps app for travelers,\ntourists, hikers, drivers and cyclists based on OpenStreetMap data created\nby the community. It is a privacy-focused, open-source fork of [\u0026hellip;]\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003eOrganic Maps is one of the few applications nowadays that supports 100% of\nfeatures without an active Internet connection. Install Organic Maps,\ndownload maps, throw away your SIM card, and go for a weeklong trip on a\nsingle battery charge without any byte sent to the network.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003ePositive impressions:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eworks great on iOS (didn\u0026rsquo;t test on Android yet)\u003c/li\u003e\n\u003cli\u003ea great companion for \u003cstrong\u003ecycling\u003c/strong\u003e within the city\u003c/li\u003e\n\u003cli\u003emore fun and more lightweight than Google Maps\u003c/li\u003e\n\u003cli\u003elow impact on device battery life\u003c/li\u003e\n\u003cli\u003eextremely privacy friendly (no ads, no tracking, no push notifications, etc)\u003c/li\u003e\n\u003cli\u003eextremely mobile data friendly (works fully offline)\u003c/li\u003e\n\u003cli\u003eopen-source, and without any IAP\u003c/li\u003e\n\u003cli\u003eworks out-of-the-box, no sign up necessary\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNegative impressions:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003edo not use it for public transit, it is terrible as it does not have\nreal-time data integration\u003c/li\u003e\n\u003c/ul\u003e","title":"Organic Maps: off-line maps"},{"content":"I got a new domain! This blog is now available on https://www.perrotta.dev/. There\u0026rsquo;s also a redirect to it from https://blog.perrotta.dev/.\nI don\u0026rsquo;t know which one I like best, so www is the canonical subdomain for now. Feel free to update the RSS in your feed reader, although https://thiagowfx.github.io/ should keep working for a little longer, until (if ever) I decide to migrate the static hosting off Github Pages.\nThis is a project I wanted to do since ages ago, and my goals go beyond merely making my blog available under it. Stay tuned for more updates.\n","permalink":"https://www.perrotta.dev/2024/02/new-domain/","summary":"\u003cp\u003eI got a new domain! This blog is now available on \u003ca href=\"https://www.perrotta.dev/\"\u003ehttps://www.perrotta.dev/\u003c/a\u003e.\nThere\u0026rsquo;s also a redirect to it from \u003ca href=\"https://blog.perrotta.dev/\"\u003ehttps://blog.perrotta.dev/\u003c/a\u003e.\u003c/p\u003e","title":"New domain"},{"content":"I got myself a brand new domain! As I play with it, expect documentation to be added.\nHow to query the WHOIS for the domain?\nFrom the command line: $ whois \u0026lt;domain\u0026gt; From the registrar WHOIS, e.g. https://porkbun.com/whois, https://www.gandi.net/en/domain/p/whois From the registry WHOIS, e.g. https://lookup.icann.org/, https://www.registry.google/whois-lookup/ It\u0026rsquo;s a good idea to set up WHOIS privacy, so that your domain registration details stay private. Some registrars such as Porkbun and NearlyFreeSpeech will gladly offer an option for that, either for free or at a low cost, respectively.\n","permalink":"https://www.perrotta.dev/2024/02/whois/","summary":"\u003cp\u003eI got myself a brand new domain! As I play with it, expect documentation to\nbe added.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eHow to query the WHOIS for the domain?\u003c/strong\u003e\u003c/p\u003e","title":"WHOIS"},{"content":"The more time you spent playing with Anki, the more opinionated you become.\nUpon reading Fluent Forever by Gabriel Wyner, I got an itch to create my own Anki note template for learning languages.\nThe template There\u0026rsquo;s no point explaining how to create a new template; the excellent Anki documentation already does so. Instead, I\u0026rsquo;ll just list and explain the template I created.\nThe template is called \u0026ldquo;Deutsch Language Card 🇩🇪\u0026rdquo;. It has four fields:\nFront Front Example Back Striked Front and Back come from the built-in template. There\u0026rsquo;s nothing special about them. I use \u0026ldquo;Front\u0026rdquo; for the canonical term in the foreign language I\u0026rsquo;m learning, and \u0026ldquo;Back\u0026rdquo; for the explanation in the base / native language I\u0026rsquo;m mostly familiar with1.\nHere\u0026rsquo;s an example:\nFront: das Buch Back: book 📚 Whenever possible I include one or more emojis 😃 in the \u0026ldquo;Back\u0026rdquo; field.\nThe canonicalization of the \u0026ldquo;Front\u0026rdquo; field is important, and one of the best (key, even!) features of Anki. It will smartly detect (and prevent!) duplicates from being created. It is case sensitive, therefore it\u0026rsquo;s important to create one convention and stick to it.\n\u0026ldquo;Front Example\u0026rdquo; is used to complement the \u0026ldquo;Front\u0026rdquo; field. It consists of one or both of the following:\nA phrase or sentence containing the Front term. A picture representing the Front term. To increase overall retention, it\u0026rsquo;s always best to add cues familiar to your context.\nAdd phrases that resonate with you or that you find in textbooks or blog posts that resonate with you. In my experience, adding random phrases is not effective.\nAdd images that represent well that you\u0026rsquo;re describing and that resonate with you. Photos that you take yourself are also fair game!\n\u0026ldquo;Striked\u0026rdquo; is to disambiguate synonyms or false cognates. For example:\nFront: der Sturm Back: storm ⛈️ Striked: das Gewitter, das Unwetter When I am reviewing the Back card, I want to cue myself not to think about the striked terms.\nThe source code The Front card {{Front}} {{tts de_DE:Front}} {{#Front Example}} \u0026lt;br\u0026gt; \u0026lt;i\u0026gt;{{Front Example}}\u0026lt;/i\u0026gt; {{tts de_DE:Front Example}} {{/Front Example}} The front card includes a text-to-speech sample that is generated on-the-fly. It works very well on macOS and iOS. In fact, that\u0026rsquo;s main reason why the template is called \u0026ldquo;Deutsch Language Card\u0026rdquo; instead of just \u0026ldquo;Language Card\u0026rdquo;. The text-to-speech engine is customized to have an accent in the given language. For (High) German, that is de_DE.\nThe Back card {{Back}} {{#Striked}} \u0026lt;br\u0026gt;\u0026lt;br\u0026gt; \u0026lt;s\u0026gt;{{Striked}}\u0026lt;/s\u0026gt; {{/Striked}} The striked terms are striked, as you would expect.\nInterestingly I prefer to use English most of the time, even though it is not my mother tongue.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/02/anki-custom-language-card/","summary":"\u003cp\u003eThe more time you spent playing with \u003ca href=\"https://apps.ankiweb.net/\"\u003eAnki\u003c/a\u003e, the\nmore opinionated you become.\u003c/p\u003e\n\u003cp\u003eUpon reading \u003ca href=\"https://fluent-forever.com/index.html\"\u003eFluent Forever\u003c/a\u003e by Gabriel\nWyner, I got an itch to create my own Anki note template for learning\nlanguages.\u003c/p\u003e","title":"★ Anki: custom language card"},{"content":"This post is a follow-up of Terraforming a Linode: hello world.\nIn a future post, we will continue from here by using Ansible to install and set up Miniflux in our new Linode.\nBefore we extensively use Ansible to configure our VPS instance, first let\u0026rsquo;s set up a basic integration between Terraform and Ansible.\nFirst of all, here\u0026rsquo;s an overview of where I stopped last time. There were a couple of lightweight modifications since then. I\u0026rsquo;ll explain some of them below.\n% cat variables.tf variable \u0026#34;github_username\u0026#34; { type = string default = \u0026#34;thiagowfx\u0026#34; } variable \u0026#34;linode_hostname\u0026#34; { type = string default = \u0026#34;coruscant\u0026#34; } variable \u0026#34;linode_region\u0026#34; { type = string default = \u0026#34;eu-central\u0026#34; } All variables were moved to a variables.tf file. This is to follow standard terraform conventions / recommendations for module structures. Furthermore, it becomes easier to manage variables when they are all stored in a single place.\nThe main module file now looks like this:\n% cat main.tf terraform { required_providers { http = { source = \u0026#34;hashicorp/http\u0026#34; } linode = { source = \u0026#34;linode/linode\u0026#34; } } } provider \u0026#34;linode\u0026#34; {} data \u0026#34;http\u0026#34; \u0026#34;github_keys\u0026#34; { url = \u0026#34;https://api.github.com/users/${var.github_username}/keys\u0026#34; } locals { keys = jsondecode(data.http.github_keys.response_body)[*].key } resource \u0026#34;linode_instance\u0026#34; \u0026#34;nanode\u0026#34; { type = \u0026#34;g6-nanode-1\u0026#34; image = \u0026#34;linode/alpine3.19\u0026#34; label = var.linode_hostname region = var.linode_region authorized_keys = local.keys backups_enabled = \u0026#34;false\u0026#34; booted = \u0026#34;true\u0026#34; watchdog_enabled = \u0026#34;true\u0026#34; } I removed the token from the linode provider. Now it is supplied via the LINODE_TOKEN environment variable. In order to automatically populate that variable, I use direnv. There\u0026rsquo;s an .envrc file that provides its value, like so:\n#!/bin/sh # terraform init export LINODE_TOKEN=\u0026#34;my-token-here\u0026#34; I also created a repository for this project: https://github.com/thiagowfx/knol. That\u0026rsquo;s enough for preliminaries, now let\u0026rsquo;s go back to Ansible.\nThe first component we\u0026rsquo;ll need is an Ansible inventory file, containing the IP address of the host we\u0026rsquo;ll manage. It could look like this:\n[all] 1.2.3.4 ansible_user=root \u0026hellip;wherein 1.2.3.4 is the IP address of our VPS.\nThat said, due to the fact the VPS instance is created dynamically, maintaining that IP address manually would be tedious. Therefore, let\u0026rsquo;s have Terraform manage it.\nWe can do so with a local_file. Heck, we could even use a template_file, however it would be overkill as there are only two simple lines in our inventory at this point. A local_file is created upon terraform apply and deleted upon terraform destroy. Therefore it doesn\u0026rsquo;t even need to be tracked by our VCS:\nresource \u0026#34;local_file\u0026#34; \u0026#34;ansible_inventory\u0026#34; { content = \u0026lt;\u0026lt;-EOF [all] ${linode_instance.nanode.ip_address} ansible_user=root EOF filename = \u0026#34;inventory.ini\u0026#34; file_permission = \u0026#34;0644\u0026#34; } Once we run terraform (plan + apply), an inventory.ini file should be created with the above contents.\nBecause the IP address is ephemeral and dynamic, we should have a straightforward way to see its value. A terraform output is perfect for that:\n% cat outputs.tf output \u0026#34;ip_address\u0026#34; { value = linode_instance.nanode.ip_address } Later on (after terraforming) we will be able to use terraform output to see the server IP address.\nWe have the inventory file. Now we need a playbook. A playbook contains a sequence of tasks to be applied to our server.\nLet\u0026rsquo;s start with a basic playbook that just installs and starts nginx:\n--- - hosts: all tasks: - name: Install the web server (nginx) community.general.apk: name: nginx state: present - name: Start the web server service: name: nginx state: started Save this to a playbook.yml file.\nAfter terraforming, we should now be able to run ansible:\n% ansible-playbook -i inventory.ini playbook.yml In order to make this setup more ergonomic, let\u0026rsquo;s create a Makefile:\nTERRAFORM := terraform all: terraform ansible ansible: ansible-playbook -i inventory.ini playbook.yml terraform: $(TERRAFORM) init $(TERRAFORM) plan $(TERRAFORM) apply clean: $(TERRAFORM) destroy .PHONY: all ansible terraform clean Then we can just run make terraform or make ansible for granular steps. Or just make to run everything in the right order.\nI extracted the terraform binary to its own variable because it facilitates the use of OpenTofu (a fork) in lieu of terraform.\nAnd that\u0026rsquo;s it for today! In a future post, we\u0026rsquo;ll look into extending our Ansible usage to fully bootstrap Miniflux on the server.\n","permalink":"https://www.perrotta.dev/2024/02/integrating-terraform-with-ansible/","summary":"\u003cp\u003eThis post is a follow-up of \u003ca href=\"https://www.perrotta.dev/2024/01/terraforming-a-linode-hello-world/\"\u003eTerraforming a Linode: hello world\u003c/a\u003e.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIn a future post, we will continue from here by using Ansible to install and\nset up Miniflux in our new Linode.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eBefore we extensively use Ansible to configure our VPS instance, first let\u0026rsquo;s\nset up a basic integration between Terraform and Ansible.\u003c/p\u003e","title":"★ Integrating terraform with ansible"},{"content":"Wordle made a splash during the pandemic. By now people became quite accustomed to it, and maybe even bored somewhat.\nJumblie is somewhat similar, and arguably more difficult challenging. It was created by Cassidy Williams.\n","permalink":"https://www.perrotta.dev/2024/02/jumblie/","summary":"\u003cp\u003e\u003ca href=\"https://www.nytimes.com/games/wordle/index.html\"\u003eWordle\u003c/a\u003e made a splash during\nthe pandemic. By now people became quite accustomed to it, and maybe even bored\nsomewhat.\u003c/p\u003e","title":"Jumblie"},{"content":"This blog is rendered by the means of a static site generator (SSG) called Hugo. Each blog post has a set of one or more tags associated to it. The more posts I create, the more consolidated the tags become.\nSometimes I need to rename tags after-the-fact to better reflect the underlying posts they represent.\nThis is how I typically do it. Start from the root of the git repository, then do:\n% for file in content/posts/**/*.md; do gsed -i -e \u0026#39;s/- german/- deutsch/g\u0026#39; \u0026#34;$file\u0026#34;; done The example above renames german -\u0026gt; deutsch.\nThis isn\u0026rsquo;t the most robust way to do so, but it\u0026rsquo;s the quickest one. For extra robustness, I\u0026rsquo;d do:\n% fd -t f -e md -e gsed -i -x \u0026#39;s/- german/- deutsch/g\u0026#39; \u0026hellip;however it\u0026rsquo;s always easier to remember the for loop syntax than the fd one.\nWhy fd instead of a for loop? fd(1) is more elegant than shell wildcards. Although, in practice, both ways are equivalent and should yield no difference.\nWhy gsed instead of sed? I am on macOS. The GNU version of sed does not create backup files, which is what I want in most cases. There\u0026rsquo;s no need for backups because everything is checked into git already; if I make a mistake, I can always git reset --hard or git checkout. The BSD version of sed will leave this mess behind:\n% fd -t f -e md -x /usr/bin/sed -i -e \u0026#39;s/- german/- deutsch/g\u0026#39; % git st On branch master Your branch is up to date with \u0026#39;origin/master\u0026#39;. Changes not staged for commit: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to update what will be committed) (use \u0026#34;git restore \u0026lt;file\u0026gt;...\u0026#34; to discard changes in working directory) modified: 2022-02-27-linux-us-international-keyboard-layout.md modified: 2022-04-03-translating-german-to-english.md modified: 2024-01-29-anki-find-all-notes-with-an-empty-field.md Untracked files: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to include in what will be committed) 2014-01-07-testando-uma-iso-no-linux-sem-o-virtualbox.md-e 2014-04-18-mini-recovery-tipico-via-usb.md-e 2014-05-01-instalando-o-gentoo-a-partir-do-arch.md-e 2014-09-28-my-first-ebuild.md-e 2015-01-07-the-eudyptula-challenge.md-e [...] There are even more of these *-e files, and they are super annoying. It\u0026rsquo;s easy to get rid of them:\n% rm **/*-e \u0026hellip;but why bother, if we can just stick to the more familiar GNU sed anyway?\nCaveats Finally, note the caveat: this find and replace is naive and could end up replacing false positives! Nonetheless, I\u0026rsquo;m still a big fan of this approach, because it\u0026rsquo;s the quickest one. As my blog is checked into git anyway, I can always easily review the changes before committing them:\n% git diff If there are too many diffs, then prefer an incremental approach:\n% git add -p Happy tag renaming! Well, this only happens every once in a while anyway.\n","permalink":"https://www.perrotta.dev/2024/01/hugo-rename-a-tag/","summary":"\u003cp\u003eThis blog is rendered by the means of a static site generator (SSG) called\n\u003ca href=\"https://gohugo.io/\"\u003eHugo\u003c/a\u003e. Each blog post has a set of one or more tags\nassociated to it. The more posts I create, the more consolidated the tags become.\u003c/p\u003e\n\u003cp\u003eSometimes I need to rename tags after-the-fact to better reflect the underlying\nposts they represent.\u003c/p\u003e","title":"Hugo: rename a tag"},{"content":"Gabriel Wyner recommends, in its Fluent Forever book, that each Anki note (card) has at least one image associated to it. This is intended to improve overall retention.\nBack when I started my German Anki deck for language learning, I did not add any images. Now I find myself slowly backfilling my already existing notes with images. However, the more images I add, the harder it becomes to find notes without images.\nUpon reading the Anki manual I figured out a way to find out which notes are still missing images:\nOpen Anki. Open the deck you want to modify – in my case, Languages::German. Click \u0026ldquo;Browse\u0026rdquo;. Type in deck:current \u0026quot;Front Example:\u0026quot;. \u0026ldquo;Front Example\u0026rdquo; is the name of the field of my note template wherein I add images; you should replace it with the corresponding one you use. This syntax isn\u0026rsquo;t intuitive at all. Initially I was trying something like -\u0026quot;Front Example:*\u0026quot;.\n","permalink":"https://www.perrotta.dev/2024/01/anki-find-all-notes-with-an-empty-field/","summary":"\u003cp\u003eGabriel Wyner recommends, in its \u003ca href=\"https://fluent-forever.com/index.html\"\u003eFluent\nForever\u003c/a\u003e book, that each\n\u003ca href=\"https://apps.ankiweb.net/\"\u003eAnki\u003c/a\u003e note (card) has at least one image\nassociated to it. This is intended to improve overall retention.\u003c/p\u003e","title":"Anki: find all notes with an empty field"},{"content":"Whenever I want to upgrade any one of my systems, I run sd-world.\nYou can find the current version of sd-world here in my dotfiles.\nHere\u0026rsquo;s a snapshot1:\n#!/usr/bin/env bash # perform a full system upgrade set -euo pipefail log() { local bold=$(tput bold) normal=$(tput sgr0) echo \u0026#34;${bold}$*${normal}\u0026#34; } run_if_exists() { if command -v \u0026#34;$1\u0026#34; \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; then if [[ $# -eq 1 ]]; then log \u0026#34;Running $1...\u0026#34; \u0026#34;$1\u0026#34; else shift log \u0026#34;Running $*...\u0026#34; \u0026#34;$@\u0026#34; fi elif [[ -d \u0026#34;$1\u0026#34; ]]; then shift log \u0026#34;Running $*...\u0026#34; \u0026#34;$@\u0026#34; fi } # usage: do_git \u0026lt;path/to/git/repo\u0026gt; do_git() { if [[ -d \u0026#34;$1\u0026#34; ]]; then run_if_exists \u0026#34;$1\u0026#34; git -C \u0026#34;$1\u0026#34; pull origin master fi } case \u0026#34;$(uname)\u0026#34; in # linux Linux) # alpine linux run_if_exists \u0026#34;apk\u0026#34; doas apk upgrade # arch linux run_if_exists \u0026#34;pacman\u0026#34; sudo pacman -Syu # debian linux # warning: macos has /usr/bin/apt which is a Java thing run_if_exists \u0026#34;apt-get\u0026#34; sudo apt-get upgrade -y run_if_exists \u0026#34;apt-get\u0026#34; sudo apt-get autoremove ;; # macOS Darwin) # homebrew run_if_exists \u0026#34;brew\u0026#34; brew upgrade run_if_exists \u0026#34;brew\u0026#34; brew cleanup # system update and app store # run_if_exists \u0026#34;softwareupdate\u0026#34; softwareupdate --install --all run_if_exists \u0026#34;mas\u0026#34; mas upgrade ;; # windows MINGW*) # third-party package manager run_if_exists \u0026#34;scoop\u0026#34; scoop update ;; esac # flatpaks run_if_exists \u0026#34;flatpak\u0026#34; flatpak update # nix run_if_exists \u0026#34;nix-channel\u0026#34; nix-channel --update run_if_exists \u0026#34;nix-env\u0026#34; nix-env -u # pihole # update pihole itself and gravity lists run_if_exists \u0026#34;pihole\u0026#34; pihole -up # dotfiles do_git \u0026#34;$HOME/.dotfiles\u0026#34; do_git \u0026#34;$HOME/.dotfiles_corp\u0026#34; There\u0026rsquo;s a lot to unpack here.\nWhy is it called sd-world? world is an inspiration taken from Gentoo Linux. To upgrade a typical gentoo system, you usually run:\nemerge --ask --quiet --update --changed-use --deep @world There\u0026rsquo;s something deeply inspiring about saying it out loud: \u0026ldquo;emerge the world\u0026rdquo;. As if the whole world is at your fingertips.\nsd stands for \u0026ldquo;script directory\u0026rdquo;, it\u0026rsquo;s an inspiration taken from Ian Henry.\nRationale: I tend to put scripts I run semi-frequently in a .bin directory that is in my system $PATH. However, there\u0026rsquo;s always a chance their name could clash with a built-in one (e.g. in /usr/bin/). In order to prevent (or mitigate) it from happening, a prefix is added. For a long time in my life I used the t- prefix, merely because of my first name initial. At some point I migrated to sd-. That\u0026rsquo;s all, nothing fancy about it.\nWhy bash? bash is the de-facto standard shell in most Linux distributions I care about. And it\u0026rsquo;s also easily available in macOS and BSDs. And it\u0026rsquo;s POSIX compliant.\nTherefore: availability, portability and compatibility.\nWhy /usr/bin/env bash instead of /bin/bash? Because the env shebang is more portable. This is more relevant when working with BSDs. On Linux /bin/bash should be mostly fine.\nWhy set -euo pipefail? A well-established good practice.\nWhy use a separare log function? Old habits die hard. Consistent formatting. Why run_if_exists? Since the script attempts to upgrade (potentially) many package managers, at the very least we try to skip the ones that aren\u0026rsquo;t installed. For example, there\u0026rsquo;s no need to attempt to run pacman in a macOS system.\nWhat else? The rest should be quite straightforward to understand. Some design decisions:\nsudo permissions are not asked upfront, because not every system uses sudo. Notably, Alpine Linux and OpenBSD use doas by default. Also, laziness is OK as the script is intended for interactive use. git is there merely for convenience. Updating my dotfiles could be done from a separate script, but that would be overkill for my simple use case. There\u0026rsquo;s no concurrency / parallelism, and that\u0026rsquo;s intentional. I prefer output readability and system stability in this case. The world is yours.\nPrefer to refer to the up-to-date version in my dotfiles repository though. I included a snapshot merely because there\u0026rsquo;s a non-zero chance the git version could be moved elsewhere someday.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/01/sd-world-perform-a-full-system-upgrade/","summary":"\u003cp\u003eWhenever I want to upgrade any one of my systems, I run \u003ccode\u003esd-world\u003c/code\u003e.\u003c/p\u003e","title":"sd-world: perform a full system upgrade"},{"content":"Let\u0026rsquo;s discuss the raison d\u0026rsquo;etre of Google Chrome for Testing, a project I was the Tech Lead of during my tenure on the Chrome Tooling / Browser Automation team at Google.\nOnce upon a time, a few (debugging) mistakes ago, web developers would run (web) integration tests with WebDriver Classic using Google Chrome (or Chromium)1. This was a chaotic era.\n\u0026ldquo;Why?\u0026rdquo;, you may rightfully ask.\nThe web browser and/or its components / extensions / etc could auto-update in-between successive test runs, yielding different test results, i.e. tests were not guaranteed to be hermetic / deterministic due to their (potentially) changing environment, yielding test flakiness Chrome adds an info bar whenever it is controlled in an automated fashion, which changes the CSS viewport, resulting in changes compared to a production environment. For example: an automated test that takes a screenshot would have a slightly smaller height whenever an infobar is present. There are no versioned Chrome builds for download. There\u0026rsquo;s no browser pinning. As a developer you always download the latest version. This makes it hard to reason about invariants, especially when new browser versions introduce breaking changes, even seemingly small ones. Corollary: The lack of versioned Chrome builds makes it hard to obtain a corresponding (matching) Chromedriver version for Chrome. The mismatch (delta) could provoke testing inconsistencies whenever browser APIs diverge2. In order to address these (and other) issues, Chrome for Testing (hereafter \u0026ldquo;CfT\u0026rdquo;) was born. To clarify, today where are three flavours of Chrom*:\nChromium: the open-source project, https://chromium.org/. The root of all derivatives (Microsoft Edge, Brave, etc). It is available in full source form, but there are no (official) pre-built binaries for it. Google Chrome: the proprietary, closed-source version of Chromium developed by Google. Think of it as Chromium on steroids. Google distributes pre-built Chrome binaries for every platform it supports. Google Chrome for Testing: think of it as \u0026ldquo;reproducible (or pinned, or frozen) Google Chrome\u0026rdquo;. It is basically a snapshot of Google Chrome in a fixed time in the past, plus a few bits of developer-oriented features mentioned in this article. There are other niceties that Chrome for Testing accomplishes as of today:\nThe CDP (Chrome DevTools Protocol) experiment (\u0026ldquo;Protocol Monitor\u0026rdquo;) is enabled by default, out-of-the-box. This kind of experiment, which enriches your debugging toolbox, is exactly the sensible state you want during the development cycle. Mechanisms such as self-XSS confirmation prompts are disabled by default, which is the desired behavior for automation. Consider an analogy with setting DEBIAN_FRONTEND=noninteractive when running apt in dockerfiles. You don\u0026rsquo;t want prompts (even benign ones) to suddenly get in the way of your tests and end up interrupting their execution flow. Completely agnostic to the concept of \u0026ldquo;Stable\u0026rdquo; / \u0026ldquo;Beta\u0026rdquo; / \u0026ldquo;Dev\u0026rdquo;. If you have pinned versions, you don\u0026rsquo;t need to care about any of that. CfT releases are made available alongside a subset of corresponding Google Chrome releases Something important to note:\nWarning: Chrome for Testing has been created purely for browser automation and testing purposes, and is not suitable for daily browsing.\nThe main reason for that is the fact that it does not auto-update. You could argue that it doesn\u0026rsquo;t matter: Chrome for most linux distributions also does not auto-update by itself. The updates are normally deferred to the distribution\u0026rsquo;s package manager (e.g. apt, dnf, pacman, etc). Why should it be different for Chrome for Testing?\nAn additional point to consider here is that Chrome for Testing could have new features in the future that would be optimized for developers, not for end users. You don\u0026rsquo;t want end users to shoot themselves on the foot, therefore it\u0026rsquo;s easier, better and safer to do a blanket anti-recommendation of CfT for non-developers3.\nBecause of that, CfT cannot be made the default system browser.\nThe easiest way to obtain CfT is via its public API, which is documented here: https://googlechromelabs.github.io/chrome-for-testing/, or through the official CLI utility that is part of Puppeteer.\nToday, for all the reasons above (and more to come!), CfT is the de-facto recommended solution for browser automation for all things web applications and web platform testing. If you\u0026rsquo;re currently using either Chromium or Google Chrome for these purposes, you should switch to it.\nBonus: How to run Chrome for Testing in CI? The chromium-bidi repository is an excellent (and simple-ish) example on how to do so4.\nGiven a .github/workflows/e2e.yml file:\nname: E2E tests jobs: e2e: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: actions/setup-node@v3 with: node-version: 16 - run: npm ci # This is the exciting part wherein we fetch CfT. # # Despite the \u0026#34;chromium\u0026#34; name, this is actually CfT. # # We set a explicit shell to force \u0026#34;set -eo pipefail\u0026#34; so that, # if the command fails, then the entire step fails. # We do not want \u0026#34;cut\u0026#34; to run if the download fails for some reason. # # The syntactic sugar of the parsing could be improved in a future # version of the CLI tool, but that\u0026#39;s how it should be done for now. # # We store the location of the CfT binary in an environment variable. - name: Install Google Chrome for Testing shell: bash run: | cft_binary=\u0026#34;$(npx @puppeteer/browsers install chromium@latest | cut -f 2- -d\u0026#39; \u0026#39;)\u0026#34; echo \u0026#34;cft_binary=$cft_binary\u0026#34; \u0026gt;\u0026gt; $GITHUB_ENV - uses: actions/setup-python@v4 with: python-version: \u0026#39;3.10\u0026#39; - run: pip install -r tests/requirements.txt # This is an example on how to run a test suite by explicitly pointing # out to CfT, using the environment variable set earlier. - name: Run E2E tests run: npm run e2e env: BROWSER_BIN: ${{ env.cft_binary }} You can find the complete version of this example in an older commit within that repository. The reason I link to an older commit is due to its direct usage of the @puppeteer/browsers CLI tool, which makes it easier to illustrate how to fetch CfT. Recent commits of the repository use a JS wrapper to do so, which is more flexible / robust for the purposes of that particular repository at the expense of decreased readability for a newcomer. Software Engineering is all about trade-offs after all.\nTo fully realize the benefits of reproducibility, you should not use latest. Instead, pin the browser to a specific version.\nIf using an environment variable (or a command-line flag) is not an option for some reason, then an alternative would be to create a symlink (ln -s) to $cft_binary from a place in the front of your $PATH. Or, alternatively, temporarily update your $PATH with the dirname of $cft_binary.\nAlso, if you cannot or do not want to install npm (npx) just for the sake of fetching CfT5, then just fetch it directly (use curl or wget) from its API endpoint, for example:\n% wget https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/121.0.6167.85/linux64/chrome-linux64.zip Although note that this is not a future-proof way of fetching CfT. It\u0026rsquo;s a simple shortcut. The better way is to query the JSON metadata file for a specific platform and browser version:\n% curl https://googlechromelabs.github.io/chrome-for-testing/latest-patch-versions-per-build-with-downloads.json | jq -r \u0026#39;.builds.\u0026#34;121.0.6167\u0026#34;.downloads.chrome[] | select(.platform == \u0026#34;linux64\u0026#34;).url\u0026#39; \u0026hellip;so that the download works even if the URL changes in the future for some reason.\nReferences Chrome for Testing Design Document: https://goo.gle/chrome-for-testing How Chrome DevTools helps to defend against self-XSS attacks For simplicity, referred to as just Chrome hereafter.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nYou can find lots of such reports here.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe same way you wouldn\u0026rsquo;t recommend Arch Linux for linux newbies.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDisclaimer: I used to work on that repository, thus my self-assessment is clearly biased :-)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI know, I know, JS bloat.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/01/google-chrome-for-testing-reliable-downloads-for-browser-automation/","summary":"\u003cp\u003eLet\u0026rsquo;s discuss the \u003cem\u003eraison d\u0026rsquo;etre\u003c/em\u003e of \u003ca href=\"https://developer.chrome.com/blog/chrome-for-testing\"\u003eGoogle Chrome for\nTesting\u003c/a\u003e, a project I was\nthe Tech Lead of during my tenure on the Chrome Tooling / Browser Automation team\nat Google.\u003c/p\u003e","title":"★ (Google) Chrome for Testing: reliable downloads for browser automation"},{"content":"I host my own Miniflux instance, which happens to be my favorite RSS reader. Currently it is hosted on Linode (Akamai Cloud) running Alpine Linux.\nMy current setup was performed manually. I was thinking that, for fun, it would be cool to fully automate it under the principles of IaC.\nThe current setup does not use any containers. I had proudly made it as KISS as possible at the time:\nLinode is a very beginner-friendly (and cheap) VPS Alpine Linux is a first-class citizen on Linode There\u0026rsquo;s an apk package for miniflux There\u0026rsquo;s an OpenRC1 script for miniflux (so that it can be controlled via service) For the first part of this automation we will look into provisioning a Linode with an Alpine Linux installation. In order to do so we will use HashiCorp Terraform.\nRequirements Provision a new Linode Deploy it in Europe Use the smallest shape (a so-called Nanode) Run Alpine Linux Set it up with my public ssh key, which is hosted on Github Terraform setup Install a provider for Linode: https://registry.terraform.io/providers/linode/linode/latest/docs Scaffold it like this, in a main.tf file:\nterraform { required_providers { linode = { source = \u0026#34;linode/linode\u0026#34; } } } Then run:\n% terraform init Generate a Linode API token Go to https://cloud.linode.com/profile/tokens, create a new token called terraform. with the \u0026ldquo;Linodes\u0026rdquo; scope set to \u0026ldquo;Read/Write\u0026rdquo;.\nAppend this API token to main.tf: provider \u0026#34;linode\u0026#34; { token = \u0026#34;\u0026lt;your token here\u0026gt;\u0026#34; } Add a linode_instance with the appropriate fields set according to the documentation: resource \u0026#34;linode_instance\u0026#34; \u0026#34;coruscant\u0026#34; { label = \u0026#34;coruscant\u0026#34; image = \u0026#34;linode/alpine3.19\u0026#34; region = \u0026#34;eu-central\u0026#34; type = \u0026#34;g6-nanode-1\u0026#34; authorized_keys = [\u0026#34;\u0026lt;your ssh public key here\u0026gt;\u0026#34;] backups_enabled = \u0026#34;false\u0026#34; watchdog_enabled = \u0026#34;true\u0026#34; booted = \u0026#34;true\u0026#34; } Then run:\n% terraform plan \u0026ldquo;Plan\u0026rdquo; is basically a dry-run. Terraform will output what it intends to do, but nothing will be done yet.\nAnalyze the output and double check that it looks correct. To actually perform the provisioning, run:\n% terraform apply Then confirm the prompt.\nWithin a few seconds (or maybe minutes), you should see your new Linode in the Linode Console.\nWe can test our deployment by ssh\u0026rsquo;ing to our new machine:\n% ssh root@\u0026lt;public IP address\u0026gt; -i ~/.ssh/my_ssh_key Welcome to Alpine! The Alpine Wiki contains a large amount of how-to guides and general information about administrating Alpine systems. See \u0026lt;https://wiki.alpinelinux.org/\u0026gt;. You can setup the system with the command: setup-alpine You may change this message by editing /etc/motd. Let\u0026rsquo;s take a pause to appreciate how lightweight it is:\nlocalhost:~# df -h Filesystem Size Used Available Use% Mounted on devtmpfs 10.0M 0 10.0M 0% /dev shm 487.8M 0 487.8M 0% /dev/shm /dev/sda 24.1G 238.1M 22.6G 1% / tmpfs 195.1M 268.0K 194.8M 0% /run Only 238 MiB!\nTo deprovision it, run:\n% terraform plan -destroy If everything looks correct, run:\n% terraform destroy Warning: It turns out the \u0026ldquo;Linodes\u0026rdquo; scope was not enough to do the deprovisioning. I needed to create a new scope, with more permissions, in order to do so.\nAs you can see, terraform makes it very trivial to deprovision systems.\nBonus points: run terraform fmt to format your file. Never go out of style.\nTip: At any point you can run terraform validate to verify your main.tf file is syntactically correct.\nTwo things could be improved in the previous setup:\nWe could use authorized_users to pass in our linode username. If we add an SSH key to our linode account, then that key would be automatically deployed to the system, thereby removing the need to specify authorized_keys. Alternatively, we could fetch our key from an URL endpoint with the use of the hashicorp/http provider, like so: terraform { required_providers { http = { source = \u0026#34;hashicorp/http\u0026#34; } } } data \u0026#34;http\u0026#34; \u0026#34;thiagowfx_ssh_keys\u0026#34; { url = \u0026#34;https://github.com/thiagowfx.keys\u0026#34; } resource \u0026#34;linode_instance\u0026#34; \u0026#34;coruscant\u0026#34; { # ... authorized_keys = compact([for line in split(\u0026#34;\\n\u0026#34;, data.http.thiagowfx_ssh_keys.response_body) : chomp(line)]) # ... } The \u0026ldquo;list comprehension\u0026rdquo; above does line splitting magic to convert them to a list of string, and the compact removes the empty new line at the end.\nWe could improve the example above even further.\nFor starters, let\u0026rsquo;s parameterize out the username to a variable:\nvariable \u0026#34;github_username\u0026#34; { type = string default = \u0026#34;thiagowfx\u0026#34; } data \u0026#34;http\u0026#34; \u0026#34;user_ssh_keys\u0026#34; { url = \u0026#34;https://github.com/${var.github_username}.keys\u0026#34; } resource \u0026#34;linode_instance\u0026#34; \u0026#34;coruscant\u0026#34; { # ... authorized_keys = compact([for line in split(\u0026#34;\\n\u0026#34;, data.http.user_ssh_keys.response_body) : chomp(line)]) # ... } We could then easily supply another username with -var:\n% terraform plan -var github_username=torvalds Note that the above example leverages string interpolation.\nWe could also extract the SSH keys list to its own \u0026ldquo;variable\u0026rdquo; (locals):\nlocals { ssh_keys = compact([for line in split(\u0026#34;\\n\u0026#34;, data.http.user_ssh_keys.response_body) : chomp(line)]) } resource \u0026#34;linode_instance\u0026#34; \u0026#34;coruscant\u0026#34; { # ... authorized_keys = local.ssh_keys # ... } A more robust (and stable) way to query the key though is through the Github API:\ndata \u0026#34;http\u0026#34; \u0026#34;github_keys\u0026#34; { url = \u0026#34;https://api.github.com/users/${var.github_username}/keys\u0026#34; } locals { ssh_keys = jsondecode(data.http.github_keys.response_body)[*].key } Note that a typical response body looks like the following:\n[ { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;id\u0026gt;\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;\u0026lt;ssh key\u0026gt;\u0026#34; } ] API endpoint documentation: https://docs.github.com/en/rest/users/keys?apiVersion=2022-11-28#list-public-keys-for-a-user\nIf we use output instead of locals, then we can debug (inspect) it with terraform output.\nAnd that\u0026rsquo;s it for today! In a future post, we will continue from here by using Ansible to install and set up Miniflux in our new Linode.\nAlpine Linux does not use systemd.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/01/terraforming-a-linode-hello-world/","summary":"\u003cp\u003eI host my own \u003ca href=\"https://miniflux.app/\"\u003eMiniflux\u003c/a\u003e instance, which happens to be\nmy favorite RSS reader. Currently it is hosted on Linode (Akamai Cloud)\nrunning \u003ca href=\"https://www.alpinelinux.org/\"\u003eAlpine Linux\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eMy current setup was performed manually. I was thinking that, for fun, it would\nbe cool to fully automate it under the principles of\n\u003ca href=\"https://en.wikipedia.org/wiki/Infrastructure_as_code\"\u003eIaC\u003c/a\u003e.\u003c/p\u003e","title":"★ Terraforming a Linode: hello world"},{"content":"Some websites attempt to prevent users from pasting text (i.e. Ctrl+V / Cmd+V) in web browsers.\nIt serves no purpose in most cases other than hindering accessibility and increasing annoyance for users.\nIt is relatively easy to bypass most trivial blocks with plain JavaScript. Open a DevTools console (Option + Cmd + I in Google Chrome on macOS), then paste in the following snippet:\nfunction () { const forceEnableCopyPaste = (e) =\u0026gt; { e.stopImmediatePropagation(); return true; }; [\u0026#39;paste\u0026#39;, \u0026#39;copy\u0026#39;].forEach(event =\u0026gt; { document.addEventListener(event, forceEnableCopyPaste, true); }); })(); Bonus points: it also works for copying text.\nI can see the point of trying to attempt to block copying (e.g. copyright, online exams), but there\u0026rsquo;s little reason to prevent pasting.\nThe only arguably valid use case I\u0026rsquo;ve seen to date for blocking pasting is in some sign-up forms wherein you need to type in some piece of user ID (e.g. your email, or your telephone number) twice. The second text field is sometimes blocked, as to encourage you to double check it is absolutely correct™, by the means of carefully typing it out.\nThe snippet above could also be easily converted to a bookmarklet.\nSearch for \u0026ldquo;bookmarklet builder\u0026rdquo; in your favorite search engine, go to a website such as https://caiorss.github.io/bookmarklet-maker/, paste the above snippet therein, then get a compressed version such as:\njavascript:(function()%7Bjavascript%3A%20(function%20()%20%7B%0A%20%20const%20forceEnableCopyPaste%20%3D%20(e)%20%3D%3E%20%7B%0A%20%20%20%20e.stopImmediatePropagation()%3B%0A%20%20%20%20return%20true%3B%0A%20%20%7D%3B%0A%0A%20%20%5B\u0026#39;paste\u0026#39;%2C%20\u0026#39;copy\u0026#39;%5D.forEach(event%20%3D%3E%20%7B%0A%20%20%20%20document.addEventListener(event%2C%20forceEnableCopyPaste%2C%20true)%3B%0A%20%20%7D)%3B%0A%7D)()%3B%7D)()%3B Now just create a web browser favorite with that resource. Clicking the bookmark will yield the same effect as pasting the snippet into devtools.\n","permalink":"https://www.perrotta.dev/2024/01/the-fundamental-right-to-paste/","summary":"\u003cp\u003eSome websites attempt to prevent users from pasting text (i.e. \u003ccode\u003eCtrl+V\u003c/code\u003e /\n\u003ccode\u003eCmd+V\u003c/code\u003e) in web browsers.\u003c/p\u003e\n\u003cp\u003eIt serves no purpose in most cases other than hindering accessibility and\nincreasing annoyance for users.\u003c/p\u003e","title":"The fundamental right to paste"},{"content":"Whenever disk space gets almost full, I like to use the following software to clean up (unnecessary) big files from my computers:\nWindows WinDirStat:\nWinDirStat is a disk usage statistics viewer and cleanup tool for various versions of Microsoft Windows.\nIt\u0026rsquo;s user-friendly and open source.\nLinux / macOS ncdu:\nNcdu is a disk usage analyzer with an ncurses interface. It is designed to find space hogs on a remote server where you don’t have an entire graphical setup available, but it is a useful tool even on regular desktop systems. Ncdu aims to be fast, simple and easy to use, and should be able to run in any minimal POSIX-like environment with ncurses installed.\nNaturally, the command line way™ prevails on Unix systems. It\u0026rsquo;s one installation command away from your favorite package manager. Usage:\n% ncdu / ","permalink":"https://www.perrotta.dev/2024/01/remove-large-files-from-your-computer/","summary":"\u003cp\u003eWhenever disk space gets almost full, I like to use the following software to\nclean up (unnecessary) big files from my computers:\u003c/p\u003e","title":"Remove large files from your computer"},{"content":"Graphviz\u0026hellip;\nis open source graph visualization software. Graph visualization is a way of representing structural information as diagrams of abstract graphs and networks.\nWe can also use it to craft family trees!\nI came up with the following template:\ndigraph G { {Greatgrandfather Greatgrandmother} -\u0026gt; Grandfather; {Grandfather Grandmother} -\u0026gt; Father; {Father Mother} -\u0026gt; Me; Greatgrandfather [shape = box;label = \u0026#34;Greatgrandfather Lastname\u0026#34;;]; Greatgrandmother [shape = box;label = \u0026#34;Greatgrandmother Lastname\u0026#34;;]; Grandfather [shape = box;label = \u0026#34;Grandfather Lastname\u0026#34;;]; Grandmother [shape = box;label = \u0026#34;Grandmother Lastname\u0026#34;;]; Father [shape = box;label = \u0026#34;Father Lastname\u0026#34;;]; Mother [shape = box;label = \u0026#34;Mother Lastname\u0026#34;;]; Me [shape = box;label = \u0026#34;Me Lastname\u0026#34;;]; Greatgrandfather [color = \u0026#34;lightblue\u0026#34;;style = filled;]; Greatgrandmother [color = \u0026#34;pink\u0026#34;;style = filled;]; Grandfather [color = \u0026#34;lightblue\u0026#34;;style = filled;]; Grandmother [color = \u0026#34;pink\u0026#34;;style = filled;]; Father [color = \u0026#34;lightblue\u0026#34;;style = filled;]; Mother [color = \u0026#34;pink\u0026#34;;style = filled;]; Me [style = filled;color = lightblue;]; } \u0026hellip;save this to a tree.dot file.\nA .png representation of the graph can then be generated with the following command:\ndot -Tpng tree.dot \u0026gt; tree.png Note that dot is part of the graphviz distribution.\nIn case it\u0026rsquo;s not installed on your system, it\u0026rsquo;s widely available, just do it. For example, on macOS:\nbrew install graphviz The final result:\n","permalink":"https://www.perrotta.dev/2024/01/create-a-family-tree-with-graphviz/","summary":"\u003cp\u003e\u003ca href=\"https://graphviz.org/\"\u003eGraphviz\u003c/a\u003e\u0026hellip;\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eis open source graph visualization software. Graph visualization is a way of\nrepresenting structural information as diagrams of abstract graphs and\nnetworks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWe can also use it to craft family trees!\u003c/p\u003e","title":"Create a family tree with graphviz"},{"content":"Sometimes I need to start a local HTTP server for a quick one-off task, often just to serve static content. It is not important which one it is, so long as I can do it quickly.\nOption #1: use python % python3 -m http.server Serving HTTP on :: port 8000 (http://[::]:8000/) ... This is often the most universal and convenient option, as python is widely available out-of-the-box.\nOption #2: use darkhttpd darkhttpd is available almost everywhere.\n% darkhttpd . darkhttpd/1.14, copyright (c) 2003-2022 Emil Mikulic. listening on: http://0.0.0.0:8080/ Their own README states:\nWhen you need a web server in a hurry.\nThis is the most convenient option when you are in control of a package manager, as it is one installation command away from your system. In particular, it\u0026rsquo;s available in both homebrew and nixpkgs.\nOption #3: use nodejs % npx http-server -p 8000 Starting up http-server, serving ./ http-server version: 14.1.1 http-server settings: CORS: disabled Cache: 3600 seconds Connection Timeout: 120 seconds Directory Listings: visible AutoIndex: visible Serve GZIP Files: false Serve Brotli Files: false Default File Extension: none Available on: http://127.0.0.1:8000 Hit CTRL-C to stop the server If you\u0026rsquo;re already within the node ecosystem, this is also just one installation away. I would typically not recommend this setup though if you don\u0026rsquo;t already have npm installed on your system.\nOption #4: use busybox This option seemed very attractive for use on Linux systems:\n% busybox httpd -f -p 8080 However in an up-to-date Alpine Linux system (3.20) it does not work:\n% busybox httpd -f -p 8080 httpd: applet not found Therefore I don\u0026rsquo;t consider it universal enough.\nReference: https://gist.github.com/willurd/5720255\n","permalink":"https://www.perrotta.dev/2024/01/start-an-http-server-asap/","summary":"\u003cp\u003eSometimes I need to start a local HTTP server for a quick one-off task, often\njust to serve static content. It is not important which one it is, so long as I\ncan do it quickly.\u003c/p\u003e","title":"Start an HTTP server ASAP"},{"content":"Some (annoying) websites and/or mobile apps will refuse to let you proceed past their registration / login screen unless you provide a last name. Isn\u0026rsquo;t my first name enough1?\nThere are a couple of ways to work around this:\nProvide a fake last name Provide a gibberish last name (aklsjslkja) Provide only the first or second letter of your real last name (e.g. \u0026ldquo;Thiago P\u0026rdquo;) Provide some non-latin-alphabetic character (e.g. \u0026ldquo;1\u0026rdquo;, \u0026ldquo;.\u0026rdquo;, \u0026ldquo;Э̇\u0026rdquo;) Recently it occurred to me there\u0026rsquo;s an even cleverer idea: provide an empty (whitespace) character.\nSome services have validation in place to prevent you from inserting a mere ASCII whitespace (\u0026rsquo; \u0026lsquo;).\nHowever, most will not bother to check \u0026ldquo;invisible\u0026rdquo; unicode whitespace:\n‎ There\u0026rsquo;s a single whitespace character above you can easily copy. vim displays it as \u0026lt;200e\u0026gt;.\nReference: https://emptycharacter.com/\nActually, why do you even need to know my first name in the first place? I am just some random database primary key ID.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/01/why-do-you-need-to-know-my-last-name/","summary":"\u003cp\u003eSome (annoying) websites and/or mobile apps will refuse to let you proceed past\ntheir registration / login screen unless you provide a last name. Isn\u0026rsquo;t my\nfirst name enough\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e?\u003c/p\u003e","title":"Why do you need to know my last name?"},{"content":" In Java, you can make a variable thread safe by just adding the synchronized keyword. Is there anything that can achieve the same results in Python?\nWithout having prior knowledge of any python libraries to do so, the primitive interface I would expect resembles the following:\nclass Foo(object): def __init__(self): self.lock = Lock() def perform_mutation(self, bytes): print(bytes) def write(self, bytes): self.lock.acquire() self.perform_mutation(bytes) self.lock.release() This isn\u0026rsquo;t robust: if an exception happens in perform_mutation the lock would never be released. A small improvement we can make is to wrap it with try/finally:\nclass Foo(object): def __init__(self): self.lock = Lock() def perform_mutation(self, bytes): print(bytes) def write(self, bytes): self.lock.acquire() try: self.perform_mutation(bytes) finally: self.lock.release() However it turns out there\u0026rsquo;s a more pythonic way to do so:\nfrom threading import Lock class Foo(object): def __init__(self): self.lock = Lock() def perform_mutation(self, bytes): print(bytes) def write(self, bytes): with self.lock: self.perform_mutation(bytes) How can we test this? First, let\u0026rsquo;s use a single thread.\nif __name__ == \u0026#34;__main__\u0026#34;: foo = Foo() foo.write(\u0026#34;hello from the main thread\u0026#34;) Now let\u0026rsquo;s use multiple threads:\nif __name__ == \u0026#34;__main__\u0026#34;: foo = Foo() threads = [] for i in range(10): thread = Thread(target=foo.write, args=(f\u0026#34;hello from thread {i}\u0026#34;,)) threads.append(thread) # Start all threads for thread in threads: thread.start() # Wait for all threads to finish for thread in threads: thread.join() Without the lock this is one of the results I get locally:\n% python3 lock.py hello from thread 0 hello from thread 1 hello from thread 2 hello from thread 3 hello from thread 4 hello from thread 6 hello from thread 8 hello from thread 7 hello from thread 5 hello from thread 9 With the lock I always get the following, as you would predict:\n% python3 lock.py hello from thread 0 hello from thread 1 hello from thread 2 hello from thread 3 hello from thread 4 hello from thread 5 hello from thread 6 hello from thread 7 hello from thread 8 hello from thread 9 We could go one level deeper in the abstraction by using a @synchronized decorator:\nclass Foo(object): def perform_mutation(self, bytes): print(bytes) @synchronized def write(self, bytes): self.perform_mutation(bytes) How do we implement it?\ndef synchronized(member): @wraps(member) def wrapper(*args, **kwargs): lock = vars(member).get(\u0026#34;_synchronized_lock\u0026#34;, None) if lock is None: lock = vars(member).setdefault(\u0026#34;_synchronized_lock\u0026#34;, Lock()) with lock: return member(*args, **kwargs) return wrapper One last concept to learn: RLock a.k.a. reentrant lock.\nConsider the following program:\nfrom threading import Lock, Thread class Foo: def __init__(self): self.lock = Lock() def changeA(self, bytes): with self.lock: print(bytes) def changeB(self, bytes): with self.lock: print(bytes) def changeAandB(self, bytes): with self.lock: print(bytes) self.changeA(bytes) # a usual lock would block here self.changeB(bytes) Invoked as follows:\nfoo = Foo() threads = [] for i in range(5): thread = Thread(target=foo.changeA, args=(f\u0026#34;hello from thread {i} A\u0026#34;,)) threads.append(thread) thread = Thread(target=foo.changeB, args=(f\u0026#34;hello from thread {i} B\u0026#34;,)) threads.append(thread) thread = Thread(target=foo.changeAandB, args=(f\u0026#34;hello from thread {i} AB\u0026#34;,)) threads.append(thread) # Start all threads for thread in threads: thread.start() # Wait for all threads to finish for thread in threads: thread.join() It will not work as expected. As soon as the first changeAandB gets called, its inner self.changeA call will block. This is because the lock can only be acquired once.\nIn this specific example, the straightforward way to fix the issue is to use an RLock: self.lock = RLock(). The reentrant lock can be locked multiple times.\nReferences https://theorangeduck.com/page/synchronized-python https://stackoverflow.com/questions/29158282/how-to-create-a-synchronized-function-across-all-instances https://stackoverflow.com/questions/53026622/python-equivalent-of-java-synchronized https://stackoverflow.com/questions/16567958/when-and-how-to-use-pythons-rlock ","permalink":"https://www.perrotta.dev/2024/01/synchronized-in-python/","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u003ca href=\"https://stackoverflow.com/questions/53026622/python-equivalent-of-java-synchronized\"\u003eIn Java, you can make a variable thread safe by just adding the \u003ccode\u003esynchronized\u003c/code\u003e\nkeyword. Is there anything that can achieve the same results in\nPython?\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e","title":"★ Synchronized in Python"},{"content":"In a typical dynamic programming (DP) problem, you\u0026rsquo;ll usually instantiate a variable to hold previously computed data (cache).\nFor example, let\u0026rsquo;s consider a naive implementation of the factorial function:\ndef factorial(n: int) -\u0026gt; int: if n == 0: return 1 return n * factorial(n - 1) Now let\u0026rsquo;s add a cache to improve it, upon a use case wherein it would be called multiple times in a row:\ncache: dict[int, int] = {} def factorial(n: int) -\u0026gt; int: if n == 0: return 1 if n in cache: return cache[n] cache[n] = n * factorial(n - 1) return cache[n] This is straightforward, the only caveat to watch out for is the scope of the cache. In general you wouldn\u0026rsquo;t want to store it globally.\nOne elegant way to address this is with lru_cache:\nfrom functools import lru_cache @lru_cache(maxsize=None) def factorial(n: int) -\u0026gt; int: if n == 0: return 1 return n * factorial(n - 1) The snippet above creates and maintains a cache under the hood. The main caveat of that snippet is that it\u0026rsquo;s not easy to remember:\nis it max_size or maxsize? is it maxsize=0 or maxsize=None? This week I found out that there\u0026rsquo;s an even more ergonomic decorator, which it is super easy to remember!\nfrom functools import cache @cache def factorial(n: int) -\u0026gt; int: if n == 0: return 1 return n * factorial(n - 1) @cache is equivalent to lru_cache(maxsize=None).\nWith this trick, you won\u0026rsquo;t ever need to manually memoize any function in python anymore!\nThis works for any number of arguments so long as they can be used as dictionary keys, i.e. the arguments must be hashable. Practically speaking, this means lists are not cacheable, but tuples are.\nHappy dynamic programming!\nReference: https://docs.python.org/3/library/functools.html\n","permalink":"https://www.perrotta.dev/2024/01/python-all-hail-to-cache-memoization/","summary":"\u003cp\u003eIn a typical dynamic programming (DP) problem, you\u0026rsquo;ll usually instantiate a\nvariable to hold previously computed data (cache).\u003c/p\u003e","title":"Python: all hail to cache memoization"},{"content":"It\u0026rsquo;s 2024, the year of the linux desktop, and the best™ way to debug computer programs is still the good ol\u0026rsquo; print statement.\nSince Python 3.6 it is possible to use f-strings.\nOne of my favorite ways to use them for debugging is with the equal sign (=):\nTo display both the expression text and its value after evaluation, (useful in debugging), an equal sign '=' may be added after the expression.\nHere is one example:\ndef is_full_word_match(token, words): print(f\u0026#39; is_full_word_match: {token=} {words=}\u0026#39;) return token in words If you call it like so:\nis_full_word_match(\u0026#34;hello\u0026#34;, \u0026#34;hello world\u0026#34;) Then it will print the following:\nis_full_word_match: token=\u0026#39;hello\u0026#39; words=\u0026#39;hello world\u0026#39; This is a more ergonomic (and quicker) way to write than the classic:\nprint(\u0026#39; is_full_word_match: token=\u0026#39; + token + \u0026#39; words=\u0026#39; + words) Or even:\nprint(\u0026#39; is_full_word_match: token={} words={}\u0026#39;.format(token, words)) ","permalink":"https://www.perrotta.dev/2024/01/python-debugging-tip-with-print-and-f-strings/","summary":"\u003cp\u003eIt\u0026rsquo;s 2024, \u003ca href=\"https://yotld.com/\"\u003ethe year of the linux desktop\u003c/a\u003e, and the best™\nway to debug computer programs is still the good ol\u0026rsquo; \u003ccode\u003eprint\u003c/code\u003e statement.\u003c/p\u003e","title":"Python: debugging tip with print and f-strings"},{"content":"For some odd reason my Calibre backup to cloud storage had a bunch of empty directories. I\u0026rsquo;ve been meaning to remove them, but it\u0026rsquo;s cumbersome to do so from the web client.\nInstead, let\u0026rsquo;s do it from a local client.\nUpon installing the cloud storage software, a local directory is exposed under /Users/$USER/Library/CloudStorage (macOS).\nMy first instinct is to use find(1):\n$ find -empty -type d -delete However that does not work on macOS:\nfind: illegal option -- e usage: find [-H | -L | -P] [-EXdsx] [-f path] path ... [expression] find [-H | -L | -P] [-EXdsx] -f path [path ...] [expression] My second go to choice is fd(1):\n$ fd -t e -x rmdir \u0026hellip;this lists all empty directories and invokes rmdir on each of them.\n","permalink":"https://www.perrotta.dev/2023/12/macos-remove-all-empty-directories/","summary":"\u003cp\u003eFor some odd reason my \u003ca href=\"https://calibre-ebook.com\"\u003eCalibre\u003c/a\u003e backup to cloud\nstorage had a bunch of empty directories. I\u0026rsquo;ve been meaning to remove them, but\nit\u0026rsquo;s cumbersome to do so from the web client.\u003c/p\u003e","title":"macOS: remove all empty directories"},{"content":"Gerrit 3.9 has been released recently. This is a dear release to me because I was responsible for some of its changes1:\n$ PAGER=\u0026#34;cat\u0026#34; git shortlog --author=\u0026#34;Thiago Perrotta\u0026#34; v3.8.0..v3.9.0 Thiago Perrotta (3): Add \u0026#39;description\u0026#39;, \u0026#39;d\u0026#39; aliases for the \u0026#39;message\u0026#39; search operator Add \u0026#39;m\u0026#39; alias for the \u0026#39;message\u0026#39; search operator UX: \u0026#34;Your Turn\u0026#34; -\u0026gt; \u0026#34;Your turn\u0026#34; This is not the first Google open source project I contributed to, however I wanted to note these contributions here nonetheless.\nOne of my favorite aspects of Google culture is the ability to contribute to any project in the company at any time in any capacity (the so called \u0026ldquo;20% contributions\u0026rdquo;).\nThe output was edited to remove duplicate entries and merge commits because the Gerrit project does not maintain a clean history of their commits i.e. they do not adopt a rebase workflow.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2023/12/gerrit-v3.9-is-out/","summary":"\u003cp\u003eGerrit \u003ca href=\"https://www.gerritcodereview.com/3.9.html\"\u003e3.9\u003c/a\u003e has been released recently. This is a dear release to me because I was responsible for some of its changes\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e:\u003c/p\u003e","title":"Gerrit v3.9 is out"},{"content":"Whenever I need to write out a simple document wherein content is more important than form, LaTeX is my preferred choice. It beats Google Docs, Microsoft Word, LibreOffice, or any other text processor for that matter.\nOverleaf is a solid cloud editor choice these days but I tend to prefer to have full control over my programming environment, thus for a local solution TeXShop is my favorite on macOS:\n$ brew install texshop The following template worked well for me to answer interview questions:\n% The next lines tell TeXShop to typeset with xelatex, and to open and save the source with Unicode encoding. %!TEX TS-program = xelatex %!TEX encoding = UTF-8 Unicode \\documentclass[12pt, oneside]{article} \\usepackage{geometry} \\geometry{letterpaper} \\usepackage{amssymb} \\usepackage{fontspec,xltxtra,xunicode} \\defaultfontfeatures{Mapping=tex-text} \\setromanfont[Mapping=tex-text]{Hoefler Text} \\setsansfont[Scale=MatchLowercase,Mapping=tex-text]{Gill Sans} \\setmonofont[Scale=MatchLowercase]{Andale Mono} \\title{My document title} % \\author{Thiago Perrotta} % uncomment if applicable \\date{\\today} \\begin{document} \\maketitle \\tableofcontents \\pagebreak \\section{Preliminaries} \\paragraph{Foo bar?} Lorem ipsum. \\paragraph{What is the answer for life, universe and everything?} 42. ","permalink":"https://www.perrotta.dev/2023/12/latex-template-for-simple-interviews/","summary":"\u003cp\u003eWhenever I need to write out a simple document wherein content is more important than form, \u003cstrong\u003eLaTeX\u003c/strong\u003e is my preferred choice. It beats Google Docs, Microsoft Word, LibreOffice, or any other text processor for that matter.\u003c/p\u003e","title":"LaTeX template for simple interviews"},{"content":"uBlock-Origin-dev-filter:\nFilters to block and remove copycat-websites from DuckDuckGo, Google and other search engines. Used to be specific to dev websites like StackOverflow or GitHub, but it currently supports others like Wikipedia.\n1-click to subscribe to a blocking list via uBlock origin, customizable on a per search engine basis (e.g. Google, DuckDuckGo, etc).\nCredits: https://www.lkhrs.com/blog/2022/04/block-domains-from-search/\n","permalink":"https://www.perrotta.dev/2023/12/enhance-adblock-lists-with-ublock-origin-dev-filter/","summary":"\u003cp\u003e\u003ca href=\"https://github.com/quenhus/uBlock-Origin-dev-filter\"\u003euBlock-Origin-dev-filter\u003c/a\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eFilters to block and remove copycat-websites from DuckDuckGo, Google and\nother search engines. Used to be specific to dev websites like StackOverflow\nor GitHub, but it currently supports others like Wikipedia.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Enhance adblock lists with uBlock origin dev filter"},{"content":"Recent macbooks have a fingerprint reader, which is typically used to unlock the computer and log in.\nIt is also possible to use it for sudo authentication via PAM.\nThis was previously covered here.\nNow, with macOS Sonoma, it\u0026rsquo;s also possible to make this setting survive OS upgrades.\n% sudo cp /etc/pam.d/sudo_local{.template,} % sudo $EDITOR /etc/pam.d/sudo_local Then uncomment (or add, if not existing) the following line:\nauth sufficient pam_tid.so You can test it out by opening a new terminal and executing sudo echo.\nCredits: https://sixcolors.com/post/2023/08/in-macos-sonoma-touch-id-for-sudo-can-survive-updates/\n","permalink":"https://www.perrotta.dev/2023/12/macos-sudo-with-touch-id-survive-upgrades/","summary":"\u003cp\u003eRecent macbooks have a fingerprint reader, which is typically used to unlock the computer and log in.\u003c/p\u003e\n\u003cp\u003eIt is also possible to use it for \u003ccode\u003esudo\u003c/code\u003e authentication via \u003ca href=\"https://en.wikipedia.org/wiki/Pluggable_authentication_module\"\u003ePAM\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThis was previously covered \u003ca href=\"https://www.perrotta.dev/2022/03/macos-sudo-with-touch-id/\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eNow, with macOS Sonoma, it\u0026rsquo;s also possible to make this setting survive OS upgrades.\u003c/p\u003e","title":"macOS: sudo with touch ID: survive upgrades"},{"content":"Strongly recommended to all software developers who use macOS: Maccy.\nClipboard manager for macOS which does one job - keep your copy history at hand. Period.\nLightweight. Open source. No fluff.\nShortcut: Cmd + Shift + C to open a clipboard menu with all your recently copied items, plus a search bar for quick grepping.\nHands down this is the best piece of software I added to my workflow in 2023, competing with Obsidian and Things in terms of productivity.\n","permalink":"https://www.perrotta.dev/2023/12/maccy-macos-clipboard-manager/","summary":"\u003cp\u003e\u003cstrong\u003eStrongly recommended\u003c/strong\u003e to all software developers who use macOS: \u003ca href=\"https://maccy.app/\"\u003eMaccy\u003c/a\u003e.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eClipboard manager for macOS which does one job - keep your copy history at hand. Period.\u003c/p\u003e\n\u003cp\u003eLightweight. Open source. No fluff.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Maccy macOS clipboard manager"},{"content":"2022 was an amazing year for AIs.\nChatGPT ChatGPT can effectively replace Stack Overflow to a large extent, if you learn how to ask questions to it. Example queries that work amazingly, returning exactly what you would expect, with detailed explanation and/or context:\nPKGBUILD how to specify git version python how to sort list reverse systemd create unit file that forks xorg start i3wm with startx git update all submodules to latest tip typescript make simple post request chromium difference between args and declare_args sekiro versus elden ring generate random number between 10 and 20 I cannot run the code. Please generate a random number for me between 10 and 20 though add pasta before or after boiling water? what is the difference between auszahlen and Bargeld abheben? write a trip packing checklist for me Someone even wrote a Chrome extension to query ChatGPT alongside a Google search: https://github.com/wong2/chat-gpt-google-extension (chatgpt4google.com). I couldn\u0026rsquo;t get it to work for some reason though.\nStable Diffusion Given a query (text), it generates an image for you. It pairs up quite nicely with Slides (when looking for inspiration) and can effectively replace Google Images to some extent.\n","permalink":"https://www.perrotta.dev/2022/12/ais-galore/","summary":"\u003cp\u003e2022 was an amazing year for AIs.\u003c/p\u003e","title":"AIs galore"},{"content":"I haven\u0026rsquo;t written a blog post for a couple of months now, which is a good indicator I should probably document my workflow before I forget how to do it\u0026hellip;\nFirst, git clone --recurse https://github.com/thiagowfx/thiagowfx.github.io/. I like to store it in ~/Projects/thiagowfx.github.io.\nTo create a new post, hugo new content/posts/2022-10-09-title-comes-here.md.\nUse either vim or textmate to draft the post. Choose one or more tags, trying to reuse existing ones whenever possible. When using vim, use Q to reformat paragraphs.\nTo preview the post locally, run make run and then open http://localhost:1313/.\nIf everything looks good, git commit and git push. GitHub CI will then publish the post to GitHub Pages in a couple of seconds.\nTo blog on the go, use https://github.dev/. I documented this setup earlier, here.\n","permalink":"https://www.perrotta.dev/2022/10/do-i-still-remember-how-to-blog/","summary":"\u003cp\u003eI haven\u0026rsquo;t written a blog post for a couple of months now, which is a good indicator I should probably document my workflow before I forget how to do it\u0026hellip;\u003c/p\u003e","title":"Do I still remember how to blog?"},{"content":"moreutils has previously been covered elsewhere, multiple times. It\u0026rsquo;s a collection of small unix tools that follow the unix philosophy1 very strongly.\nHere are some of my favorites with example usages. Obviously this post isn\u0026rsquo;t a manual which would have been a disservice to the community; refer to the upstream man pages for detailed instructions.\nsponge sponge(1) - soak up standard input and write to a file\nHere\u0026rsquo;s a typical workflow sponge(1) is great at:\n# Given a file $ cat myfile a b c # Imagine that for whatever reason we want to replace \u0026#39;a\u0026#39; with \u0026#39;b\u0026#39; # Naively, we could try this: $ cat myfile | tr \u0026#39;a\u0026#39; \u0026#39;b\u0026#39; \u0026gt; myfile $ cat myfile # However the file becomes empty! # It got clobbered when we tried to simultenaously read from and write to it # sponge comes to the rescue! $ cat myfile | tr \u0026#39;a\u0026#39; \u0026#39;b\u0026#39; | sponge myfile $ cat myfile b b c It is great to use sponge in lieu of \u0026gt; (shell output redirection) in shell pipelines when trying to both read from and write to the same file.\nvidir vidir(1) - edit directories and filenames\nvidir(1) is great to bulk rename files/directories within a given directory, one level at a time. For example, if I open vidir at the top-level directory of this blog repository, it opens up vim (although it doesn\u0026rsquo;t need to be vim, your $EDITOR is honoured) with the following content:\n1\t./.git 2\t./.github 3\t./.gitignore 4\t./.gitmodules 5\t./.hugo_build.lock 6\t./LICENSE 7\t./Makefile 8\t./README.md 9\t./archetypes 10\t./config.yml 11\t./content 12\t./layouts 13\t./public 14\t./resources 15\t./static 16\t./themes If I make, say, the following modifications (lines 7 and 8):\n1\t./.git 2\t./.github 3\t./.gitignore 4\t./.gitmodules 5\t./.hugo_build.lock 6\t./LICENSE 7\t./GNUMakefile 8\t./README.rst 9\t./archetypes 10\t./config.yml 11\t./content 12\t./layouts 13\t./public 14\t./resources 15\t./static 16\t./themes And then save and quit vim (:wq), then the effect would have been the same as:\n$ mv Makefile GNUMakefile $ mv README.md README.rst If I changed my mind and decided not to save the modifications, I could just do :cq.\nIt\u0026rsquo;s possible to leverage vim features such as . (repeat command) and :%s/ (find and replace) to perform those mass file renames quickly and effectively. vidir is a breeze to use!\nifne ifne(1) - Run command if the standard input is not empty\nifne(1) is effective when used with find or fd to keep shell pipes \u0026ldquo;happy\u0026rdquo;. Here\u0026rsquo;s one simple example:\n$ find . -name \u0026#39;*.cpp\u0026#39; | xargs clang-format This should work as expected, but it\u0026rsquo;s cleaner to do:\n$ find . -name \u0026#39;*.cpp\u0026#39; | ifne xargs clang-format The added ifne ensures the xargs command is only executed if and only if find yields at least one result in its output.\nThis wasn\u0026rsquo;t a very practical example though: a more realistic way to use ifne is with prototypical on-the-fly manipulation of shell pipes wherein initially you just do whatever, but then whenever you notice some command in the middle of the pipe has failed because its input was empty (=the previous pipe command output was empty) you just prepend ifne to it:\n$ this | is | a | complicated | pipe # assume \u0026#34;complicated\u0026#34; fails because it has no input # so we iterate and do: $ this | is | a | ifne complicated | pipe combine combine(1) - combine sets of lines from two files using boolean operations\ncombine(1) is pretty much comm(1), but much more user-friendly. Given two files file1 and file2 it makes it easy to query which lines are {unique, common} to {each, both} files, using boolean operations (or, and, not, xor). Here\u0026rsquo;s one example to find the common lines in both files, compare combine and comm:\n$ combine file1 and file2 $ comm -12 file1 file2 # flags are harder to remember The Unix philosophy emphasizes building simple, short, clear, modular, and extensible code that can be easily maintained and repurposed by developers other than its creators. The Unix philosophy favors composability as opposed to monolithic design.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/05/tools-you-should-know-about-moreutils/","summary":"\u003cp\u003e\u003ca href=\"https://joeyh.name/code/moreutils/\"\u003e\u003ccode\u003emoreutils\u003c/code\u003e\u003c/a\u003e has previously been covered \u003ca href=\"https://news.ycombinator.com/item?id=31043655\"\u003eelsewhere\u003c/a\u003e, multiple times. It\u0026rsquo;s a collection of small unix tools that follow the \u003ca href=\"https://en.wikipedia.org/wiki/Unix_philosophy\"\u003eunix philosophy\u003c/a\u003e\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e very strongly.\u003c/p\u003e\n\u003cp\u003eHere are some of my favorites with example usages. Obviously this post isn\u0026rsquo;t a manual which would have been a disservice to the community; refer to the upstream man pages for detailed instructions.\u003c/p\u003e","title":"★ Tools you should know about: moreutils"},{"content":"It occurred to me the Large Prints section of our local public library is a decent way to sample popular books.\nWikipedia:\nLarge-print (also large-type or large-font) refers to the formatting of a book or other text document in which the typeface (or font) are considerably larger than usual to accommodate people who have low vision. Frequently the medium is also increased in size to accommodate the larger text. Special-needs libraries and many public libraries will stock large-print versions of books, along with versions written in Braille.\nInstead of browsing the entire library catalogue to try to find an interesting book to read1, it’s often more effective to browse the Large Print ones.\na.k.a. serendipity.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/04/large-prints/","summary":"\u003cp\u003eIt occurred to me the \u003cstrong\u003eLarge Prints\u003c/strong\u003e section of our local public library is a decent way to sample popular books.\u003c/p\u003e","title":"Large prints"},{"content":"I was looking for a way to track packages and parcels (mail) for the most popular post and courier services e.g. DHL, UPS, Fedex, Canada Post, USPS, Correios, etc.\nThere were only two requirements:\none place to rule them all: whether an app, chatbot, self-hosted software, or website, all services should be available from a single UI endpoint, for ease of management automatic / periodic updates: whether by polling, subscription or webhook, the service should autonomously retrieve parcel statuses; accessing each individual provider website should be a no-go Ultimately I found two solutions that pleased me, both of which free:\nShopify\u0026rsquo;s Shop app: Pro: Great and KISS design, Con: Tracking; Fortune 500 company gathering analytics and data from my purchases. Although I do have a great amount of respect for Shopify generally, the less amount of tracking by Big Tech the better.\nTelegram\u0026rsquo;s @Trackbot:\nTrackBot is a Telegram bot for tracking all your shipments. Free, forever. Automatic courier recognition. TrackBot automatically detects the courier of the shipment by using machine learning techniques, with an accuracy higher than 97%.\nAfter using both of them for a while, my preferred solution nowadays is the Telegram bot. Its basic operations are (i) List all shipments and (ii) Add a new shipment. It is smart enough to detect the carrier by itself in most cases from the tracking code alone, however whenever it doesn\u0026rsquo;t one can simply specify it manually. Whenever new updates to your existing shipments are detected, it sends you a message (notification) on Telegram.\n","permalink":"https://www.perrotta.dev/2022/04/tracking-packages-automatically-in-a-single-place/","summary":"\u003cp\u003eI was looking for a way to \u003cstrong\u003etrack packages and parcels\u003c/strong\u003e (mail) for the most popular post and courier services e.g. DHL, UPS, Fedex, Canada Post, USPS, Correios, etc.\u003c/p\u003e","title":"Tracking packages automatically in a single place"},{"content":"Issue: For whatever reason, the Home and End keys on my Keychron K2 do not work as intended on macOS.\nExpectations vs Reality For example, when using a text editor such as TextMate or a web browser like Chrome, I\u0026rsquo;d expect:\nHome to position the text cursor in the beginning of the line (à la C-a in emacs) End to position the text cursor in the end of the line (à la C-e in emacs) The only way to provoke these effects out-of-the-box is by pressing, respectively, the Cmd + Left and Cmd + Right shortcuts, as you would normally do in a Macbook laptop native keyboard.\nThis is very annoying because it only happens in macOS1: the Home and End keys work just fine in both Linux and Windows. A reddit user reported the same issue in /r/keychron, but the existing thread has no proposed solutions.\nEnter Karabiner Elements I\u0026rsquo;ve always heard good things about Karabiner Elements as a praised one-size-fits-all application for keyboards and macros in macOS, thus decided to give it a try. Bonus points: it is open source, released into the public domain.\nUpon installing it with Homebrew Cask (brew install karabiner-elements), I executed it. Then I needed to give a bunch of permissions to the application via macOS Settings \u0026gt; Security \u0026amp; Privacy \u0026gt; Privacy \u0026gt; Input Monitoring. The following apps were whitelisted accordingly:\nkarabiner_grabber karabiner_observer Karabiner-EventViewer.app: this one is optional, but useful for debugging The app is straightforward to use. It allows you to do all sorts of reactions to key codes input events.\nI had a simple idea: I wanted to map Home to Cmd + Left, and End to Cmd + Right.\nUnfortunately these are considered \u0026ldquo;Complex modifications\u0026rdquo; because they map one origin key to two destination keys. \u0026ldquo;Simple modifications\u0026rdquo; are one-to-one key mappings. Why is it unfortunate? Because it doesn\u0026rsquo;t seem to be possible to do such mappings via the app UI. Apparently one needs to express those mappings in a .json file instead.\nAh, communities Sure, no problem, I was about to do it but then I realized there\u0026rsquo;s an official website for community-maintained mappings. The website is well organized and curated. Why create something fully from scratch when I could just reuse an existing one?\nI found a \u0026ldquo;Keychron K2\u0026rdquo; category which made me instantly happy but it turned out not to be useful, as there were only two defined mappings therein:\nChange Keychron K2 keyboard layout to more closely resemble an Apple keyboard Remap some Keychrom K2(US) keys to make it less painful to switch from Macbook(RU) keyboard None of these mattered to me. Then I searched for home to cmd which led me to this entry, which had exactly the mappings I wanted:\nHome and End\nHome to Command Left End to Command Right Its resulting JSON looks roughly like this (irrelevant bits stripped for the sake of brevity):\n{ \u0026#34;title\u0026#34;: \u0026#34;Home and End\u0026#34;, \u0026#34;rules\u0026#34;: [ { \u0026#34;description\u0026#34;: \u0026#34;Home to Command Left\u0026#34;, \u0026#34;manipulators\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;basic\u0026#34;, \u0026#34;from\u0026#34;: { \u0026#34;key_code\u0026#34;: \u0026#34;home\u0026#34; }, \u0026#34;to\u0026#34;: [ { \u0026#34;key_code\u0026#34;: \u0026#34;left_arrow\u0026#34;, \u0026#34;modifiers\u0026#34;: \u0026#34;command\u0026#34; } ] } ] }, { \u0026#34;description\u0026#34;: \u0026#34;End to Command Right\u0026#34;, \u0026#34;manipulators\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;basic\u0026#34;, \u0026#34;from\u0026#34;: { \u0026#34;key_code\u0026#34;: \u0026#34;end\u0026#34; }, \u0026#34;to\u0026#34;: [ { \u0026#34;key_code\u0026#34;: \u0026#34;right_arrow\u0026#34;, \u0026#34;modifiers\u0026#34;: \u0026#34;command\u0026#34; } ] } ] } ] } There\u0026rsquo;s conveniently an Import button in the website though, which automatically opens the mappings in Karabiner Elements, so I didn\u0026rsquo;t even need to copy and paste the JSON.\nVerdict End Result: It worked flawlessly! The only caveat is that from now on I need to keep the Karabiner Elements application running as a daemon, but it is well justified. Plus, if I ever need2 to map additional keys in the future, now I already have a workflow in place to do so.\nKarabiner is like having QMK purely at the software layer, which works for any keyboard whatsoever.\nAs of this writing: macOS Monterey: 12.3.1.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://en.wikipedia.org/wiki/You_aren%27t_gonna_need_it\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/04/keychron-k2-on-macos-fix-home-and-end-keys/","summary":"\u003cp\u003e\u003cstrong\u003eIssue\u003c/strong\u003e: For whatever reason, the \u003ccode\u003eHome\u003c/code\u003e and \u003ccode\u003eEnd\u003c/code\u003e keys on my \u003ca href=\"https://www.perrotta.dev/2022/01/keychron-k2-review/\"\u003eKeychron K2\u003c/a\u003e do not work as intended on macOS.\u003c/p\u003e","title":"★ Keychron K2 on macOS: fix Home and End keys"},{"content":"I\u0026rsquo;ve been trying to learn German on my own, without taking formal classes. I should write a post about this sometime.\nFor now, in this post I will briefly list the resources I use to translate German to English and/or to look up the definition of German words in German.\nGoogle Translate Everyone knows Google Translate, right? Direct link for DE-\u0026gt;EN translations: https://translate.google.com/?sl=de\u0026amp;tl=en/\nGoogle Translate is my to go swiss-army knife one-size-fits-all application whenever I need to translate something without thinking too much. It is decent for words and for phrases, sentences and expressions, giving you: (i) translation, (ii) synonyms / alternate translations, (iii) dictionary definition and (iv) pronunciation.\nA quicker shortcut to use on the go is Google itself: A query like translate strasse from german works as you would expect, and it even displays an Open in Google Translate button for further tweaking.\ndict.cc Dict.cc is great to look up detailed definitions of German words and idioms. It\u0026rsquo;s a superb dictionary (Wörterbuch). Example query: strasse. It has apps for Android and iOS, both of which support offline lookups.\nDeepL DeepL Translate is the new kid in town (released on 2017), \u0026ldquo;The world\u0026rsquo;s most accurate translator\u0026rdquo; as they describe themselves. It\u0026rsquo;s free, but with some limitations. Apparently they use some heavy machine learning machinery different than Google\u0026rsquo;s that may yield better (\u0026ldquo;more natural\u0026rdquo;) results in some situations. I like to keep it around as an alternative to Google Translate when its translations aren\u0026rsquo;t satisfactory, but I don\u0026rsquo;t use it much.\nGoogle Dictionary When using Google Chrome the Google Dictionary extension is handy to quickly look up word definitions without leaving the current page. A double click in a word triggers a pop-up balloon with a concise definition.\nAs a bonus it doubles down as an English dictionary.\nApple Dictionary On Apple operating systems, including iOS and macOS, one can out-of-the-box force touch a word which yields a behavior very similar to Google Dictionary (\u0026ldquo;Look Up\u0026rdquo;).\n","permalink":"https://www.perrotta.dev/2022/04/translating-german-to-english/","summary":"\u003cp\u003eI\u0026rsquo;ve been trying to learn German on my own, without taking formal classes. I should write a post about this sometime.\u003c/p\u003e\n\u003cp\u003eFor now, in this post I will briefly list the resources I use to translate German to English and/or to look up the definition of German words in German.\u003c/p\u003e","title":"Translating German to English"},{"content":"Terminal.app is a pretty decent terminal emulator for macOS, with sensible defaults. That said, I have my own gripes about it, but the list is surprisingly small.\n1. No OSC-52 support https://github.com/roy2220/osc52pty:\nOSC 52 is one of Xterm Control Sequences, which is designated for clipboard setting. Once a terminal supporting OSC 52 catches a text in the form of OSC 52 from the output, instead of printing the text onto the screen, it decodes the text first and then sends the content to the system clipboard.\nAlthough Terminal.app does NOT support OSC 52, here [osc52pty] is the workaround for it.\nI dislike this workaround because it requires an external binary. Even though it is a single binary because it is a Golang executable, I still dislike the external dependency.\n2. No true color (256 colors) What is true color? See stack overflow for context.\nRun the following to print a color band, a smooth (gradient) output indicates true color support:\nawk \u0026#39;BEGIN{ s=\u0026#34;/\\\\/\\\\/\\\\/\\\\/\\\\\u0026#34;; s=s s s s s s s s; for (colnum = 0; colnum\u0026lt;77; colnum++) { r = 255-(colnum*255/76); g = (colnum*510/76); b = (colnum*255/76); if (g\u0026gt;255) g = 510-g; printf \u0026#34;\\033[48;2;%d;%d;%dm\u0026#34;, r,g,b; printf \u0026#34;\\033[38;2;%d;%d;%dm\u0026#34;, 255-r,255-g,255-b; printf \u0026#34;%s\\033[0m\u0026#34;, substr(s,colnum+1,1); } printf \u0026#34;\\n\u0026#34;; }\u0026#39; Terminal.app will not print a gradient.\n3. No GPU acceleration https://unix.stackexchange.com/q/658709:\nQ: What are the advantages of hardware-accelerated terminal emulators?\nA: They can potentially be faster at outputting and refreshing vast amounts of information. It could also allow for smooth(er) scrolling. Human beings however are quite slow at reading this information, [\u0026hellip;] the average person is unlikely to be able to comprehend it anyways. CPU usage could be lower but it needs to be tested.\nTerminal.app isn\u0026rsquo;t GPU accelerated.\nRecommendations Both alacritty and kitty are decent replacements (or complements) for Terminal.app that work out-of-the-box, with sensible defaults including all the aforementioned points.\n","permalink":"https://www.perrotta.dev/2022/03/macos-terminal-app-gripes/","summary":"\u003cp\u003e\u003ccode\u003eTerminal.app\u003c/code\u003e is a pretty decent terminal emulator for macOS, with sensible\ndefaults. That said, I have my own gripes about it, but the list is\nsurprisingly small.\u003c/p\u003e","title":"macOS terminal app gripes"},{"content":"Just tested straight to spam today:\nLove emails but hate people? Don’t want someone 🤡 at your party 🥳 but have to invite them 🤢 cause your mom 💁‍♀️ made you? Trust Straight 2 Spam to send your v important email 📧 straight to their spam 🗑\nClick the button below👇 to copy a nasty ❌ ooey ❌ gooey ❌ spam-keyword filled invisible message 🔤 for your email that you totally sent on time ⏰ but the 🐦 dodo-brain 🧠 won\u0026rsquo;t see it because they didn’t check their spam folder 📂 (Just make sure you\u0026rsquo;re not in the recipient\u0026rsquo;s address book 📇, or all bets are off 🙅‍♀️)\nIt works exactly as advertised:\nSend an email to someone whose address book contains your email address, and it will not go to spam. Send an email to someone whose address book does not contain your email address, and it goes straight to the spam folder. The email body is indeed invisible, at least in the Gmail web UI. Even Ctrl+A won\u0026rsquo;t reveal it. If you click \u0026ldquo;Show original\u0026rdquo; to inspect the full message body and headers though, you\u0026rsquo;ll see some junk like the following:\nHello#1 $$$ 100% Act now Action Additional income Affordable All natural/new Amazed Apply now Avoid Be amazed/your own Bitcoin boss Beneficiary Billing Billion Bonus Boss Buy Call!!!!!! free/now Cancel Crypto Cash Casino There\u0026rsquo;s actually more, but I don\u0026rsquo;t want to make this post too spammy for search engines.\nThe aforementioned text is wrapped in this HTML:\n\u0026lt;div dir=3D\u0026#34;ltr\u0026#34;\u0026gt;Hello\u0026lt;span style=3D\u0026#34;color:rgb(255,255,255);font-family:\u0026amp;quot;Comic Sans MS\u0026amp;quot;;font-size:1px\u0026#34;\u0026gt; Which explains why it is \u0026lsquo;invisible\u0026rsquo; (note the white color).\n","permalink":"https://www.perrotta.dev/2022/03/send-emails-straight-to-spam/","summary":"Just tested straight to spam today:\nLove emails but hate people? Don’t want someone 🤡 at your party 🥳 but have to invite them 🤢 cause your mom 💁‍♀️ made you? Trust Straight 2 Spam to send your v important email 📧 straight to their spam 🗑\nClick the button below👇 to copy a nasty ❌ ooey ❌ gooey ❌ spam-keyword filled invisible message 🔤 for your email that you totally sent on time ⏰ but the 🐦 dodo-brain 🧠 won\u0026rsquo;t see it because they didn’t check their spam folder 📂 (Just make sure you\u0026rsquo;re not in the recipient\u0026rsquo;s address book 📇, or all bets are off 🙅‍♀️)","title":"Send emails straight to spam"},{"content":"EFF\u0026rsquo;s1 HTTPS Everywhere is a browser extension available for all major browsers that automatically upgrades HTTP to HTTPS on supported websites.\nOther than adblocking, it\u0026rsquo;s one of the first extensions I add to a fresh browser installation.\nToday I learned it is apparently not needed anymore. Both Google Chrome and Firefox have settings these days to perform exactly the same functionality of the extension.\nIn Google Chrome do: chrome://settings -\u0026gt; Security and Privacy -\u0026gt; Advanced -\u0026gt; Toggle \u0026lsquo;Always use secure connections\u0026rsquo; on.\nIn Firefox the option is located on Settings -\u0026gt; Privacy \u0026amp; Security -\u0026gt; HTTPs only mode.\nEFF is a big proponent and advocate for a secure web, being one of the core responsible actors for certbot and Let\u0026rsquo;s Encrypt.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/03/https-everywhere-so-long-and-thanks-for-all-the-fish/","summary":"\u003cp\u003eEFF\u0026rsquo;s\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e \u003ca href=\"https://chrome.google.com/webstore/detail/https-everywhere/gcbommkclmclpchllfjekcdonpmejbdp\"\u003eHTTPS Everywhere\u003c/a\u003e is a browser extension available for all major browsers that automatically upgrades HTTP to HTTPS on supported websites.\u003c/p\u003e","title":"HTTPS Everywhere: So long and thanks for all the fish"},{"content":"Recent macbooks have a fingerprint reader, which is typically used to unlock the computer and log in.\nIt is also possible to use it for sudo authentication via PAM:\n% $EDITOR /etc/pam.d/sudo # sudo: auth account password session auth sufficient pam_tid.so # \u0026lt;== add this line auth sufficient pam_smartcard.so auth required pam_opendirectory.so account required pam_permit.so password required pam_deny.so session required pam_permit.so Once the file is saved with the added line, a command with sudo will spawn the touch ID prompt. I confirmed it works on both Terminal.app and Kitty.\nThis solution does not work within tmux (confirmed), and apparently within iTerm2 as well (not confirmed). A separate PAM module is needed to do so (pam_reattach.so). I\u0026rsquo;d rather keep my core dependencies surface small though and not include a third party, so for now I am satisfied with the native touch ID module.\nReferences https://sixcolors.com/post/2020/11/quick-tip-enable-touch-id-for-sudo/ https://apple.stackexchange.com/a/306324 ","permalink":"https://www.perrotta.dev/2022/03/macos-sudo-with-touch-id/","summary":"\u003cp\u003eRecent macbooks have a fingerprint reader, which is typically used to unlock the computer and log in.\u003c/p\u003e\n\u003cp\u003eIt is also possible to use it for \u003ccode\u003esudo\u003c/code\u003e authentication via \u003ca href=\"https://en.wikipedia.org/wiki/Pluggable_authentication_module\"\u003ePAM\u003c/a\u003e:\u003c/p\u003e","title":"macOS: sudo with touch ID"},{"content":"Computers are fast.\nLet\u0026rsquo;s find out how well you know computers! All of these programs have a variable NUMBER in them. Your mission: guess how big NUMBER needs to get before the program takes 1 second to run.\nYou don\u0026rsquo;t need to guess exactly: they\u0026rsquo;re all between 1 and a billion. Just try to guess the right order of magnitude!\nThis is basically an interactive version of Latency Numbers Every Programmer Should Know, originally coined by Jeff Dean.\n","permalink":"https://www.perrotta.dev/2022/03/computers-are-fast/","summary":"\u003cp\u003e\u003ca href=\"https://computers-are-fast.github.io\"\u003eComputers are fast\u003c/a\u003e.\u003c/p\u003e","title":"Computers are fast"},{"content":"Whenever I need to fully reinstall a Windows Desktop system, there are certain applications that need to be bootstrapped: image viewer, image editor, office suite, PDF viewer, video player, web browsers, etc.\nIn my opinion, Ninite is the best way to do so.\nAll you need to do is to select a few checkboxes. The Ninite installer will then automatically download and install all selected applications and software, one-by-one, with sensible defaults1 and a decent progress report:\nApp1\tOK App2\tInstalling App3\tWaiting to install App4\tDownloading App5\tWaiting to download I dunno why they do it one-by-one, but it\u0026rsquo;s in principle reasonable, probably intended to avoid potential conflicts of multiple installers trying to fiddle with each other at the same time.\nThe Ninite installer also has an interesting reuse2 property: You could save it to run it again in the future: it will end up updating the existing applications – and maybe reinstalling them, in case some of them were uninstalled in the meantime.\nIf you bookmark the URL generated by the webapp, which looks like https://ninite.com/7zip-chrome-irfanview-steam/ 3, the same set of applications could be bootstrapped once again in the future, which is useful to do batch installations in multiple computers, or to reinstall everything after a factory reset.\nThis is the URL I used to install sensible applications for my parents:\nhttps://ninite.com/7zip-chrome-classicstart-dropbox-firefox-gimp-googledrivefordesktop-inkscape-irfanview-klitecodecs-libreoffice-qbittorrent-steam-sumatrapdf-teamviewer15-thunderbird-vlc/\nFor example, by saying \u0026lsquo;No\u0026rsquo; to junk like browser toolbars, add-ons and \u0026ldquo;extras\u0026rdquo;.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI wanted to say \u0026lsquo;reproducibility\u0026rsquo;, but it\u0026rsquo;s not quite what it means.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nYou can add more pieces of software as needed.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/03/ninite-bootstrap-a-windows-installation/","summary":"\u003cp\u003eWhenever I need to fully reinstall a Windows Desktop system, there are certain applications that need to be bootstrapped: image viewer, image editor, office suite, PDF viewer, video player, web browsers, etc.\u003c/p\u003e\n\u003cp\u003eIn my opinion, \u003ca href=\"https://ninite.com\"\u003eNinite\u003c/a\u003e is the best way to do so.\u003c/p\u003e","title":"Ninite: Bootstrap a Windows installation"},{"content":"Miniflux 2.0.36 has been released this week. This is a dear release to me because I was responsible for many of its changes:\n$ PAGER=\u0026#34;cat\u0026#34; git shortlog --author=\u0026#34;Thiago Perrotta\u0026#34; 2.0.35..2.0.36 Thiago Perrotta (8): Add pagination on top of all entries. Closes #1305. Add links to scraper/rewrite/filtering docs when editing feeds Add several icons to menus according to their roles Add new keyboard shortcut: \u0026#39;M\u0026#39; - toggle read/unread, go to prev item refactor handleEntryStatus / goToNextListItem / goToPrevListItem Add (+) action next to Feeds to quickly add new feeds Add \u0026#39;+\u0026#39; shortcut for new subscription page Gray out pagination buttons when they are not applicable This is the first open source project I self-host in a serious manner that I am contributing back to. It is very exciting and fulfilling, and it sparks a lot of joy in my heart.\nI have a few other ideas for improving the miniflux user experience and functionality, while still keeping it simple and elegant. As everything else in life, time is the only constraint\u0026hellip;\n","permalink":"https://www.perrotta.dev/2022/03/miniflux-v2.0.36-is-out/","summary":"\u003cp\u003eMiniflux \u003ca href=\"https://miniflux.app/releases/2.0.36.html\"\u003e2.0.36\u003c/a\u003e has been released this week. This is a dear release to me because I was responsible for many of its changes:\u003c/p\u003e","title":"Miniflux v2.0.36 is out"},{"content":"I try to avoid websites with paywalls. If I really like the website and it deserves my attention, I will throw in a monthly subscription for it. High-quality content deserves to be supported. The fragmentation isn\u0026rsquo;t always great and it\u0026rsquo;s often hard to keep track of multiple distinct news sources and portals / subscriptions, but that\u0026rsquo;s a topic for another day.\nSometimes people will link to news articles or websites with paywalls from various sources (blogs, social media, other news articles, etc). I\u0026rsquo;d rather know in advance that those articles are paywalled, but that\u0026rsquo;s not always possible. After clicking them, curiosity already killed the cat.\nThere are several ways to access those as one-offs. I will add a disclaimer that I do not publicly endorse any of those methods, they are just mentioned for educational purposes.\nThe most typical way is to open an incognito tab or window in your browser with the desired URL. This works because many paywalls are often implemented with browser cookies.\nThe second most typical way is to use a VPN to appear that you\u0026rsquo;re accessing the URL from another IP address. This works for websites that add rolling article limits per IP address.\nOccasionally some large news websites will implement paywalls poorly:\nThe idea is pretty simple, news sites want Google to index their content so it shows up in search results. So they don\u0026rsquo;t show a paywall to the Google crawler. We benefit from this because the Google crawler will cache a copy of the site every time it crawls it.\nAll we do is show you that cached, unpaywalled version of the page.\n12ft automatically uses this mechanism to display cached versions of news articles. If you\u0026rsquo;re in \u0026lt;url\u0026gt;, just prepend 12ft.io to it: https://12ft.io/\u0026lt;url\u0026gt;.\nAlternatively, Outline used to be another website/service to do so, but apparently it is unavailable since last week. Outline displays a pretty printed version of text from an article, looking a lot like a markdown-rendered version of a web page.\nSomeone on Hacker News suggested txtify.it as a replacement to it. Indeed, Txtify is very similar to Outline, however it displays plain text instead (i.e. no formatting at all).\nApparently some people even go further by installing browser extensions to do so.\nUltimately, whenever possible, prefer to access news sources from news portals that aren\u0026rsquo;t paywalled and/or that you are a subscriber of.\n","permalink":"https://www.perrotta.dev/2022/03/bypass-news-article-paywalls/","summary":"\u003cp\u003eI try to avoid websites with paywalls. If I really like the website and it\ndeserves my attention, I will throw in a monthly subscription for it.\nHigh-quality content deserves to be supported. The fragmentation isn\u0026rsquo;t always\ngreat and it\u0026rsquo;s often hard to keep track of multiple distinct news sources and\nportals / subscriptions, but that\u0026rsquo;s a topic for another day.\u003c/p\u003e\n\u003cp\u003eSometimes people will link to news articles or websites with paywalls from\nvarious sources (blogs, social media, other news articles, etc). I\u0026rsquo;d rather\nknow in advance that those articles are paywalled, but that\u0026rsquo;s not always\npossible. After clicking them, curiosity already killed the cat.\u003c/p\u003e\n\u003cp\u003eThere are several ways to access those as one-offs. I will add a disclaimer\nthat I do not publicly endorse any of those methods, they are just mentioned\nfor educational purposes.\u003c/p\u003e","title":"Bypass news article paywalls"},{"content":"I use QWERTY keyboards with a US layout. Sometimes I need to type accents or cedillas, and I keep forgetting how to do so, this post summarizes how to do it.\nIntro There are basically two layouts:\nUS (\u0026lsquo;vanilla\u0026rsquo;): type accents like '^`~ and they will be emitted immediately US International (INTL): accents are the so called \u0026lsquo;dead keys\u0026rsquo;: A dead key is a special kind of a modifier key on a mechanical typewriter, or computer keyboard, that is typically used to attach a specific diacritic to a base letter.\nWe can switch between keyboard layouts with setxkbmap. It\u0026rsquo;s also possible to use localectl in systemd-based distros, but its syntax is harder to remember so I won\u0026rsquo;t even include it here.\nSet US \u0026lsquo;vanilla\u0026rsquo; keyboard layout $ setxkbmap us This is what a standard QWERTY keyboard should use to type in English.\nSet US International (INTL) keyboard layout $ setxkbmap -layout us -variant intl This is what a standard QWERTY keyboard1 should use to type, for example, in Portuguese or in German.\nPortuguese - á é í ó ú : \u0026#39; + \u0026lt;vowel\u0026gt; - â ê î ô û : ^ + \u0026lt;vowel\u0026gt; - ã õ : ~ + \u0026lt;vowel\u0026gt; - à : ` + \u0026lt;vowel\u0026gt; - ç (cedilla) : Alt Gr + , (Option + c on macOS) German - ß (ss) : Alt Gr + s (Option + s on macOS) - ä ö ü : \u0026#34; + \u0026lt;vowel\u0026gt; Alt Gr is typically the Right Alt key.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/02/linux-us-international-keyboard-layout/","summary":"\u003cp\u003eI use QWERTY keyboards with a US layout. Sometimes I need to type accents or\ncedillas, and I keep forgetting how to do so, this post summarizes how to do it.\u003c/p\u003e","title":"Linux: US International keyboard layout"},{"content":"I keep forgetting these, so I wrote a small summary for my own reference.\nHSTS Wikipedia — HSTS:\nHTTP Strict Transport Security (HSTS) is a policy mechanism that helps to protect websites against man-in-the-middle attacks such as protocol downgrade attacks and cookie hijacking. It allows web servers to declare that web browsers (or other complying user agents) should automatically interact with it using only HTTPS connections.\nIn layman\u0026rsquo;s terms: Force HTTPS on a given domain.\nHSTS Preload List:\nThis form is used to submit domains for inclusion in Chrome\u0026rsquo;s HTTP Strict Transport Security (HSTS) preload list. This is a list of sites that are hardcoded into Chrome as being HTTPS only.\nMost major browsers (Chrome, Firefox, Opera, Safari, IE 11 and Edge) also have HSTS preload lists based on the Chrome list. (See the HSTS compatibility matrix.)\nIf you add your website to that list, major browsers will honor it and only use HTTPS for your domain.\nSome TLDs enforce HTTPS through HSTS, some popular ones are .app and .dev. Here is a more comprehensive list.\nHSTS is fire-and-forget, you\u0026rsquo;ll usually only need to worry about it once, when configuring a SSL certificate (HTTPS) for your domain or subdomains.\nCSP Wikipedia — CSP:\nContent Security Policy (CSP) is a computer security standard introduced to prevent cross-site scripting (XSS), clickjacking and other code injection attacks resulting from execution of malicious content in the trusted web page context.\nMDN — CSP:\nContent Security Policy (CSP) is an added layer of security that helps to detect and mitigate certain types of attacks, including Cross-Site Scripting (XSS) and data injection attacks. These attacks are used for everything from data theft, to site defacement, to malware distribution.\nCSP can be configured in at least two distinct ways:\nWeb server: return the Content-Security-Policy HTTP header: Content-Security-Policy: default-src \u0026#39;self\u0026#39;; img-src https://*; child-src \u0026#39;none\u0026#39;; HTML \u0026lt;meta\u0026gt; tag: \u0026lt;meta http-equiv=\u0026#34;Content-Security-Policy\u0026#34; content=\u0026#34;default-src \u0026#39;self\u0026#39;; img-src https://*; child-src \u0026#39;none\u0026#39;;\u0026#34;\u0026gt; CSP is something to worry about at the application level. For example, miniflux to fetch resources (fonts) from another domain (Google Fonts).\nCORS Wikipedia — CORS:\nCross-origin resource sharing (CORS) is a mechanism that allows restricted resources on a web page to be requested from another domain outside the domain from which the first resource was served.\nCORS can be configured via web server: return the Access-Control-Allow-Origin HTTP header:\nAccess-Control-Allow-Origin: * Access-Control-Allow-Origin: http://example.com:8080 CORS is something to worry about at the application level. For example, https://keep.google.com/ ⟷ https://google.com/ cookies.\nCSRF Wikipedia — CSRF:\nCross-site request forgery, also known as one-click attack or session riding and abbreviated as CSRF (sometimes pronounced sea-surf) or XSRF, is a type of malicious exploit of a website where unauthorized commands are submitted from a user that the web application trusts.\nCSRF is something to be aware of and to watch out for. OWASP has some additional resources on it.\n","permalink":"https://www.perrotta.dev/2022/02/http-a-few-acronyms/","summary":"\u003cp\u003eI keep forgetting these, so I wrote a small summary for my own reference.\u003c/p\u003e","title":"HTTP: a few acronyms"},{"content":"This post contains a small handful of distinct services to query your machine external IP address.\nGoogle URL: https://www.google.com/search?q=what+is+my+ip\nAs of this writing, this doesn\u0026rsquo;t work on duckduckgo: https://duckduckgo.com/?q=what+is+my+ip. I suppose this is related to their philosophy of not tracking their users.\nThis is the easiest method when you have a web browser as you do not need to memorize any URL.\nI can haz ip URL: https://icanhazip.com/\n$ curl icanhazip.com NNN.NNN.NNN.NNN I love the simplicity of I can haz ip. It just returns your IP address in plain text, nothing else. It also works from the web browser. You can find details about it here. TL;DR: It was an open source pet project of a single person (Major Hayden), then it was eventually bought by Cloudflare as it immensely grew.\nIt\u0026rsquo;s also possible to query your IPv6 address in case you have one:\n$ curl -6 icanhazip.com IPInfo URL: https://ipinfo.io/\nIPInfo returns structured data beyond just your IP address. There are several similar services that do this, for example, What is my IP? and https://ifconfig.co/, however IPInfo is the cleanest one I have seen.\nping.eu URL: https://ping.eu/\nI\u0026rsquo;ll also give an honourable mention to ping.eu as it contains a small handful of utilities to check for things like Traceroute, DNS, whois, port check, etc.\nifconfig.io Update (2024-07-11): Add ifconfig.io.\nURL: https://ifconfig.io/\n$ curl ifconfig.io ","permalink":"https://www.perrotta.dev/2022/02/what-is-my-ip/","summary":"\u003cp\u003eThis post contains a small handful of distinct services to query your machine\nexternal IP address.\u003c/p\u003e","title":"What is my IP?"},{"content":"When I created this blog, I pondered a lot about which typography to use. I kept experimenting with several fonts available in Google Fonts, and settled on a few favorites for websites:\nHeader fonts (sans-serif): Inter, Fira Sans, Lato\nBody fonts (serif): Crimson Pro, Vollkorn, Alegreya\nCode fonts (mono): Fira Code, PT Mono, IBM Plex Mono\nUltimately though, none of them mattered. I was motivated and influenced by Kev Quirk\u0026rsquo;s Trying To Go Green With Local Fonts and Steve\u0026rsquo;s This website is killing the planet, which basically boils down to the same spirit of https://motherfuckingwebsite.com/: The web is too bloated nowadays, most websites have a ton of unnecessary CSS and JavaScript junk to fetch over and over again.\nThis is not a big deal if you have access to fast internet and powerful computers, but that\u0026rsquo;s not the case for many people in the planet.\nWith the intent of not unnecessarily fetching fonts from the web, that\u0026rsquo;s why my current font stack just uses the existing fonts in your system, with a few opinionated bits in case you have some of my favorite fonts already installed:\nbody { font-family: Crimson Pro, Vollkorn, Alegreya, Iowan Old Style, Apple Garamond, Baskerville, Times New Roman, Noto Serif, Droid Serif, Times, Source Serif Pro, serif, Apple Color Emoji, Segoe UI Emoji, Segoe UI Symbol, Noto Color Emoji; } h1, h2, h3, h4, h5, h6, footer, nav, .toc, .post-meta { font-family: Inter, Fira Sans, Lato, system-ui, -apple-system, BlinkMacSystemFont, Avenir Next, Avenir, Segoe UI, Helvetica Neue, Helvetica, Ubuntu, Roboto, Noto, Cantarell, Arial, sans-serif; } code, pre { font-family: Fira Code, PT Mono, IBM Plex Mono, Menlo, Consolas, Monaco, Liberation Mono, Ubuntu Mono, Lucida Console, monospace; } The system font stack reference comes from https://systemfontstack.com/ and CSS Tricks.\n","permalink":"https://www.perrotta.dev/2022/02/website-fonts-just-use-the-system-font-stack/","summary":"\u003cp\u003eWhen I created this blog, I pondered a lot about which typography to use. I\nkept experimenting with several fonts available in \u003ca href=\"https://fonts.google.com/\"\u003eGoogle\nFonts\u003c/a\u003e, and settled on a few favorites for websites:\u003c/p\u003e","title":"Website fonts: just use the system font stack"},{"content":"Suppose you want to take a screenshot from a web page, maybe to add to a pull request of a project you\u0026rsquo;re working on.\n(i) The straightforward way to do so is to use your OS tool:\nWindows: Snipping tool or hit the PrintScreen key Linux: scrot or something from your Desktop Environment (DE) such as xfce4-screenshooter. For a full-featured one, I\u0026rsquo;d recommend flameshot (first) or shutter (second). For Wayland people seem to like grim. macOS: Hit Cmd + Shift + 4 or one of its variations. Even Chromebooks have a way to do so these days.\nHowever, maybe you didn\u0026rsquo;t know you can also use Google Chrome to take screenshots! Here\u0026rsquo;s how:\nGo to the page you want to screenshot. Open DevTools (Ctrl + Shift + J on Linux). Hit Ctrl + Shift + P à la VSCode to pop up a command bar, type \u0026lsquo;screenshot\u0026rsquo;. Choose one option. I like the \u0026lsquo;Capture area screenshot\u0026rsquo; one which allows me to drag a square for the area I want to capture. Hit Enter. You will then be prompted where you want to save your screenshot. Profit!\n","permalink":"https://www.perrotta.dev/2022/02/screenshot-a-web-page-from-within-chrome-devtools/","summary":"\u003cp\u003eSuppose you want to take a screenshot from a web page, maybe to add to a \u003ca href=\"https://github.com/miniflux/v2/pull/1341\"\u003epull\nrequest\u003c/a\u003e of a project you\u0026rsquo;re working\non.\u003c/p\u003e\n\u003cp\u003e(i) The straightforward way to do so is to use your OS tool:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eWindows\u003c/strong\u003e: \u003ca href=\"https://support.microsoft.com/en-us/windows/use-snipping-tool-to-capture-screenshots-00246869-1843-655f-f220-97299b865f6b\"\u003eSnipping\ntool\u003c/a\u003e\nor hit the \u003cem\u003ePrintScreen\u003c/em\u003e key\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLinux\u003c/strong\u003e: \u003ca href=\"\"\u003e\u003ccode\u003escrot\u003c/code\u003e\u003c/a\u003e or something from your Desktop Environment (DE) such as\n\u003ccode\u003exfce4-screenshooter\u003c/code\u003e. For a full-featured one, I\u0026rsquo;d recommend\n\u003ca href=\"https://flameshot.org/\"\u003e\u003ccode\u003eflameshot\u003c/code\u003e\u003c/a\u003e (first) or\n\u003ca href=\"https://shutter-project.org/\"\u003e\u003ccode\u003eshutter\u003c/code\u003e\u003c/a\u003e (second). For Wayland people seem\nto like \u003ca href=\"https://wayland.emersion.fr/grim/\"\u003e\u003ccode\u003egrim\u003c/code\u003e\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003emacOS\u003c/strong\u003e: Hit \u003ccode\u003eCmd + Shift + 4\u003c/code\u003e or one of its\n\u003ca href=\"https://support.apple.com/en-ca/HT201361\"\u003evariations\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEven \u003ca href=\"https://support.google.com/chromebook/answer/10474268?hl=en\"\u003eChromebooks\u003c/a\u003e\nhave a way to do so these days.\u003c/p\u003e\n\u003cp\u003eHowever, maybe you didn\u0026rsquo;t know you can also use \u003ca href=\"https://www.google.com/intl/en_ca/chrome/\"\u003eGoogle Chrome\u003c/a\u003e to take screenshots! Here\u0026rsquo;s how:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGo to the page you want to screenshot.\u003c/li\u003e\n\u003cli\u003eOpen DevTools (\u003ccode\u003eCtrl + Shift + J\u003c/code\u003e on Linux).\u003c/li\u003e\n\u003cli\u003eHit \u003ccode\u003eCtrl + Shift + P\u003c/code\u003e à la VSCode to pop up a command bar, type \u0026lsquo;screenshot\u0026rsquo;.\u003c/li\u003e\n\u003cli\u003eChoose one option. I like the \u0026lsquo;Capture area screenshot\u0026rsquo; one which allows me to drag a square for the area I want to capture.\u003c/li\u003e\n\u003cli\u003eHit \u003ccode\u003eEnter\u003c/code\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eYou will then be prompted where you want to save your screenshot. Profit!\u003c/p\u003e","title":"Screenshot a web page from within chrome devtools"},{"content":"I\u0026rsquo;ve been giving Wayland a try. My window manager of choice in X11/Xorg is i3, so the natural choice in Wayland is sway.\nIntro sway works well with the i3 config out-of-the-box. A few adjustments were necessary for full compatibility. To maximize code reuse, I went with the following structure:\n$ tree ~/.config/{i3,sway} /home/typhoon/.config/i3 ├── conf.d │ └── i3.conf └── config /home/typhoon/.config/sway ├── conf.d │ └── sway.conf └── config -\u0026gt; ../../../i3/.config/i3/config The master config is ~/.config/i3/config. It is pretty standard, generated by i3-config-wizard with a few tweaks on top for my own workflow. It works for both i3 and sway. The config contains this snippet: # Load user configs if existing. Order is important. include conf.d/*.conf The snippet allows drop-in customizations to live in conf.d. The include directive is a relatively new addition to the i3config syntax and it\u0026rsquo;s the main reason this setup is elegant and minimalist.\ni3-only config options live in ~/.config/i3/conf.d/i3.conf. To give you an idea of what it looks like and which options aren\u0026rsquo;t compatible with sway, here\u0026rsquo;s a snapshot of my config in early 2022: # i3(1) only config file # Commands herein are not compatible or interoperable with sway(1) # Reference: https://i3wm.org/docs/userguide.html # Autostart XDG applications (.desktop files). # https://wiki.archlinux.org/title/XDG_Autostart # # Troubleshooting: # dex -ade i3 exec dex --autostart --environment i3 # lock screen, Ctrl+Alt+l (systemd) exec --no-startup-id xss-lock -l -- i3lock -c 222222 bindsym Ctrl+Mod1+l exec loginctl lock-session # XF86AudioPlayPause is not recognized by sway, add it only to i3 # https://github.com/swaywm/sway/issues/4783 bindsym XF86AudioPlayPause exec playerctl play-pause # show window title icon for_window [all] title_window_icon on set $bgcolor #526532 set_from_resource $black i3.color0 set_from_resource $red i3.color1 set_from_resource $green i3.color2 set_from_resource $white i3.color7 set_from_resource $gray i3.color8 # Theme colors client.focused $bgcolor $bgcolor $white $green client.focused_inactive $gray $gray $black $gray client.unfocused $black $black $gray $black client.urgent $red $red $white $red # Start i3bar to display a workspace and status bar bar { status_command i3status position top workspace_min_width 25 colors { background $black statusline $white focused_workspace $bgcolor $bgcolor $white $black active_workspace $gray $gray $black $gray inactive_workspace $black $black $gray $gray urgent_workspace $red $red $white $green } } # restart i3 inplace (preserves layout/session, can be used to upgrade i3) bindsym $mod+Shift+r restart # vim: ft=i3config It\u0026rsquo;s possible some of these configs will become compatible with sway over time, but at the time of this writing they are not.\nsway-only config options live in ~/.config/sway/conf.d/sway.conf. To give you an idea of what it looks like and which options aren\u0026rsquo;t compatible with i3, here\u0026rsquo;s a snapshot of my config in early 2022: # sway(1) only config file # Commands herein are not compatible or interoperable with i3(1) # References: # sway(5) # https://github.com/swaywm/sway/wiki # https://github.com/swaywm/sway/wiki/Useful-add-ons-for-sway # HiDPI output \u0026#34;*\u0026#34; scale 1.5 # Wallpaper output \u0026#34;*\u0026#34; bg ~/.wallpaper fill # Gaps a la i3-gaps gaps inner 10 # XF86AudioPlayPause is not recognized by sway: xmodmap -pke | grep XF86AudioPlay # https://github.com/swaywm/sway/issues/4783 bindcode 172 exec playerctl play-pause # Start i3bar to display a workspace and status bar bar { status_command i3status position top workspace_min_width 25 } # restart i3 inplace (preserves layout/session, can be used to upgrade i3) bindsym $mod+Shift+r exec sway reload # vim: ft=i3config Most of those are wayland-specific options.\nQuirks gaps is available in i3 as well but only if you use i3-gaps, which generally I refuse to in order to stay closer to vanilla/upstream i3.\nThe play-pause multimedia key is a bug I found on sway. It\u0026rsquo;s quite annoying, the workaround as you can see above is to use bindcode instead of bindsym. For more details see the bug.\nIn general sway works very well out-of-the-box so long as you install XWayland (xorg-xwayland on Arch Linux). XWayland transparently proxies X11 apps to a X11 server that runs inside wayland.\nIt\u0026rsquo;s possible to detect those apps by running xprop and trying to click a window: If you cannot do it, then the window is not a X11 app. Alternatively xeyes is another way to detect them.\nTo achieve a 100% Xorg/X11-free experience with pure wayland, just add xwayland disable to the sway config. I wouldn\u0026rsquo;t recommend that though, most Linux GUI apps aren\u0026rsquo;t Wayland ready and will probably never be. To put it another way, X11/Xorg will take a long time (if ever) to disappear the same way that IPv4 will take a long time (if ever) to let IPv6 completely replace it. That\u0026rsquo;s life.\nX11 apps look a bit blurry in a 4K monitor with scaled DPI (\u0026gt;96) when they run inside Wayland with XWayland. I am not particularly bothered by that, but it\u0026rsquo;s noticeable.\nThere\u0026rsquo;s no need to replace all of your small i3 Xorg utilities with wayland ones. For example, rofi (application launcher) works just fine (no need for wofi). The stock i3 bar (sway bar?) works just fine, there\u0026rsquo;s no need for polybar or waybar.\nSome utilities need to be replaced though. For example, dunst (notification daemon) does not seem to work with sway out-of-the-box, mako seems to be a recommended replacement. i3lock (lock screen) also does not work, sway comes with its own screen lock directives. Screenshotters (e.g. scrot) will also need to be replaced.\nThe system tray does not seem to work fine out-of-the-box. I haven\u0026rsquo;t investigated much to figure out what\u0026rsquo;s wrong with it.\nI was looking for a display manager that works well with both X11 and Xorg and ended up trying greetd, emptty and ly, in that order. ly is in my opinion the best one in terms of balancing simplicity and usefulness.\nsway / XWayland doesn\u0026rsquo;t source ~/.Xresources. This is an issue if you rely on customizations therein. It does source ~/.Xdefaults though! Leveraging this, I did the following changes:\n(i) ~/.Xresources sources ~/.Xdefaults: $ cat ~/.Xresources ! These settings apply to X11 only. ! Use ~/.Xdefaults for settings that apply to both X11 and Wayland (xorg-xwayland). #include \u0026#34;.Xdefaults\u0026#34; ! Source: ! xrdb -merge ~/.Xresources ! ! Dump all properties: ! xrdb -q ! ! Check if DPI is set: ! xrdb -q | grep -i dpi ! HiDPI ! Common values: ! 96 (x1.0, baseline) ! 144 (x1.5) ! 192 (x2.0, HiDPI) *.dpi: 144 (ii) ~/.Xdefaults holds my customizations that originally lived in ~/.Xresources: $ cat ~/.Xdefaults ! These settings apply to both X11 and Wayland (xorg-xwayland). ! Use ~/.Xresources for X11-only settings. Xft.antialias: true Xft.hinting: true ... In principle I could just have symlinked them:\n$ ln -s ~/.Xresources ~/.Xdefaults The reason why I didn\u0026rsquo;t do it is to avoid double scaling (DPI). You see, my sway config already sets DPI / scaling to 1.5x. If we do that in ~/.Xdefaults as well then Xorg applications would have been scaled twice.\nClosing remarks In general Wayland / sway works reasonably well out-of-the-box in 2022, but tiny adjustments are still necessary, and it isn\u0026rsquo;t as polished as it could have been. Furthermore, my workflow is very simple. Try sharing your screen in a video call in Wayland and you\u0026rsquo;ll run into other quirks. I have mixed feelings and wouldn\u0026rsquo;t necessarily recommend it. I wouldn\u0026rsquo;t give an anti recommendation either. It\u0026rsquo;s complicated\u0026hellip;even though Wayland is supposed to overcome some X11 / Xorg limitations, as a client and without knowing its internals I fail to see its advantages.\n","permalink":"https://www.perrotta.dev/2022/02/wayland-from-i3-to-sway/","summary":"\u003cp\u003eI\u0026rsquo;ve been giving Wayland a try. My window manager of choice in X11/Xorg is \u003ca href=\"https://i3wm.org/\"\u003e\u003ccode\u003ei3\u003c/code\u003e\u003c/a\u003e, so the natural choice in Wayland is \u003ca href=\"https://swaywm.org/\"\u003e\u003ccode\u003esway\u003c/code\u003e\u003c/a\u003e.\u003c/p\u003e","title":"★ Wayland: from i3 to sway"},{"content":"This blog is managed and generated with Hugo, which is a SSG (static site generator), which basically means I write all my blog posts in static markdown files, off-line, and then do git commit + git push to publish them.\nThe advantage of this workflow is that it\u0026rsquo;s really minimalist, efficient and portable: I can use whatever text editor I want to1, from pretty much any operating system I want to, even from my phone if I am really inclined2.\nThat said, sometimes I am on the go with a Chromebook and don\u0026rsquo;t have easy access to a machine to ssh to. Sure, I could just write a post in a note-taking app like Google Keep, Standard Notes or Simple Note and publish it later. But what if I wanted to publish it right away?\nIt would be really nice if I could just pop up an editor in a web browser just like the cool kids do with WordPress, Medium and Substack\u0026hellip;\nMike Stone describes one way to do so, where he edits it directly from GitHub. That works fine, but then you need to write your post all at once, there\u0026rsquo;s no \u0026ldquo;save and continue later\u0026rdquo;.\nI think a better approach is to use GitHub codespaces: I go to https://github.dev/thiagowfx/thiagowfx.github.io where there\u0026rsquo;s a Visual Studio Code instance running on the web, make my edits or compose a new post therein, and then git push. Even if I don\u0026rsquo;t want to git push right away, I could just come back later and continue it from where I stopped. It\u0026rsquo;s brilliant! It even has terminal access if needed (e.g. to play with hugo on the go).\nI mostly use vim to compose these blog posts, and occasionally Visual Studio Code (vscode).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThere is a ton of markdown note-taking apps these days. Bear ($$) seems to be a popular one for iOS, but even the stock Notes app is decent for drafting blog posts.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/02/hugo-compose-or-edit-blog-posts-from-the-web/","summary":"\u003cp\u003eThis blog is managed and generated with \u003ca href=\"https://gohugo.io\"\u003eHugo\u003c/a\u003e, which is\na \u003ca href=\"https://jamstack.org/generators/\"\u003eSSG\u003c/a\u003e (static site generator), which\nbasically means I write all my blog posts in static markdown files, off-line,\nand then do \u003ccode\u003egit commit\u003c/code\u003e + \u003ccode\u003egit push\u003c/code\u003e to publish them.\u003c/p\u003e\n\u003cp\u003eThe advantage of this workflow is that it\u0026rsquo;s really minimalist, efficient and\nportable: I can use whatever text editor I want to\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e, from pretty much any\noperating system I want to, even from my phone if I am really inclined\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e\n\u003cp\u003eThat said, sometimes I am on the go with a Chromebook and don\u0026rsquo;t have easy\naccess to a machine to \u003ccode\u003essh\u003c/code\u003e to. Sure, I could just write a post in\na note-taking app like \u003ca href=\"https://keep.google.com/\"\u003eGoogle Keep\u003c/a\u003e, \u003ca href=\"https://standardnotes.com\"\u003eStandard\nNotes\u003c/a\u003e or \u003ca href=\"https://simplenote.com\"\u003eSimple Note\u003c/a\u003e and\npublish it later. But what if I wanted to publish it right away?\u003c/p\u003e\n\u003cp\u003eIt would be really nice if I could just pop up an editor in a web browser just like the cool kids do with WordPress, Medium and Substack\u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://mikestone.me/creating-on-github/\"\u003eMike Stone\u003c/a\u003e describes one way to do so, where he edits it directly from GitHub. That works fine, but then you need to write your post all at once, there\u0026rsquo;s no \u0026ldquo;save and continue later\u0026rdquo;.\u003c/p\u003e\n\u003cp\u003eI think a better approach is to use \u003ca href=\"https://www.perrotta.dev/2022/01/ephemeral-linux-shell-access-in-the-cloud/\"\u003eGitHub codespaces\u003c/a\u003e: I go to \u003ca href=\"https://github.dev/thiagowfx/thiagowfx.github.io\"\u003ehttps://github.dev/thiagowfx/thiagowfx.github.io\u003c/a\u003e where there\u0026rsquo;s a Visual Studio Code instance running on the web, make my edits or compose a new post therein, and then \u003ccode\u003egit push\u003c/code\u003e. Even if I don\u0026rsquo;t want to \u003ccode\u003egit push\u003c/code\u003e right away, I could just come back later and continue it from where I stopped. It\u0026rsquo;s brilliant! It even has terminal access if needed (e.g. to play with \u003ccode\u003ehugo\u003c/code\u003e on the go).\u003c/p\u003e","title":"Hugo: compose or edit blog posts from the web"},{"content":"As much as I love my Miniflux setup, I\u0026rsquo;ve also come to appreciate some indie blog aggregators. They are especially handy when I feel like breaking out of my own bubble and/or getting the occasional serendipity dose to discover new blogs to follow.\nHere\u0026rsquo;s a non-exhaustive list in alphabetical order:\n(i) Aggregation of blogs, acts as poor man\u0026rsquo;s RSS feed readers that someone else manages:\nBlog Surf, by DKB: See about. Diff Blog: See FAQ. (ii) Article submission websites, where their users can submit articles which are then upvoted (or not) by other users (wisdom of the crowds):\nHacker News. You probably know this one already. The /best page should yield a higher SNR1. Alternatively, to consume the best of hacker news directly in your RSS feed, you can use something like HNRSS to filter out all submissions below a certain threshold. Lobsters. See about. Lobsters is basically a more niche version of Hacker News. One distinctive feature is that new users can only join if they are invited by another user2. And for completeness I\u0026rsquo;ll also mention Reddit but everyone already knows it at this point.\nSignal-to-noise ratio.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPlease invite me oh, dear reader. I do not currently have an account there.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/02/praise-for-blog-aggregators/","summary":"\u003cp\u003eAs much as I love my \u003ca href=\"https://miniflux.app/\"\u003eMiniflux\u003c/a\u003e setup, I\u0026rsquo;ve also come to\nappreciate some indie blog aggregators. They are especially handy when I feel\nlike breaking out of my own bubble and/or getting the occasional serendipity dose\nto discover new blogs to follow.\u003c/p\u003e","title":"Praise for blog aggregators"},{"content":"I am currently evaluating Nix as a replacement for Homebrew CLI apps in macOS1. Others have previously written about this.\nMy goal is to keep a sane learning curve and learn things on-the-fly, only as needed. Nix is a massive ecosystem and has so many batteries included and components (NixOS, NixPkgs, NixOps, Nix programming language, nix-shell, nix-env, nix-darwin, home-manager, \u0026hellip;). The good news is that those components are for the most part modular, there\u0026rsquo;s no need to adopt them all in order to reap the benefits that Nix provides.\nFor now, I am only adopting nix-env and nix-shell, with no *.nix config files. This post covers nix-env.\nFor simplicity, think of nix-env as a package manager, akin to apk, pacman, brew, apt, pkg, etc.\nInstall a package $ nix-env -i moreutils installing \u0026#39;moreutils-0.67\u0026#39; building \u0026#39;/nix/store/jsp0l5ny3kx8p9lx9w9r0x159i9jjnn6-user-environment.drv\u0026#39;... I see some guides using nix-env -iA but -i seems to suffice. We could optionally specify the nixpkgs. prefix:\n$ nix-env -i nixpkgs.moreutils error: selector \u0026#39;nixpkgs.moreutils\u0026#39; matches no derivations Oh no! Maybe that\u0026rsquo;s what the -A is for?\n$ nix-env -iA nixpkgs.moreutils replacing old \u0026#39;moreutils-0.67\u0026#39; installing \u0026#39;moreutils-0.67\u0026#39; Indeed! Apparently that -A thing stands for attribute. The only thing I know is that there are both nixpkgs.* and nixos.*. But I don\u0026rsquo;t care about NixOS at this point. I\u0026rsquo;ll just ignore -A from now on, for the time being.\nList installed packages $ nix-env -q moreutils-0.67 Easy! This actually gets displayed in my less pager.\nUpgrade installed packages $ nix-env -u Easy! At this point, I am not super confident whether that works as intended though. We will find out in a few days when there\u0026rsquo;s some update to one of my installed CLI applications. I\u0026rsquo;ve heard there\u0026rsquo;s something called nix channel to control that. Leaving it for another day though.\nUpdate(2022-02-18): I learned that nix-env -u is akin to apt upgrade or apk upgrade. It upgrades installed packages to newer versions but only if it is aware there are newer versions. To actually refresh the repositories à la apt update or apk update, use nix-channel --update.\nNote: On macOS this needs to be sudo -i nix-channel --update. See issue.\nUninstall a package $ nix-env --uninstall moreutils uninstalling \u0026#39;moreutils-0.67\u0026#39; building \u0026#39;/nix/store/5k8rsf4cxg4iz7cqnqirpww6r97bwnqr-user-environment.drv\u0026#39;... Easy!\nSearch for packages $ nix-env -qaP \u0026#39;.*moreutils.*\u0026#39; The .* seems to be needed. It works if I omit them, but only if I write the exact package name (apparently called \u0026lsquo;derivation\u0026rsquo; in Nix):\n$ nix-env -qaP moreutils nixpkgs.moreutils moreutils-0.67 If I write the wrong package name, the following happens:\n$ nix-env -qaP moreutil error: selector \u0026#39;moreutil\u0026#39; matches no derivations, maybe you meant: moreutils It was helpful in this case, but I wouldn\u0026rsquo;t always count on that. It is a bit annoying that there\u0026rsquo;s no nix search moreutils command, but it seems that nix-env is very heavily tailored to use short flags, just like pacman in Arch Linux. I got used to pacman, hopefully I can get used to the nix-env short flags at some point.\nActually I tried it out and there is a nix search command!\n$ nix search moreutils error: experimental Nix feature \u0026#39;nix-command\u0026#39; is disabled; use \u0026#39;--extra-experimental-features nix-command\u0026#39; to override This isn\u0026rsquo;t very promising though. How come searching is experimental?! Anyway, I can live with the nix-env form for now.\nThese are the 5 basic package management operations that I needed to bootstrap my dev environment. Without putting much effort on it, my initial list of package looks like this:\n$ nix-env -q atool-0.39.0 bash-interactive-5.1-p12 coreutils-9.0 exa-0.10.1 fpp-0.9.2 fzf-0.29.0 git-2.34.1 htop-3.1.2 hugo-0.92.0 jq-1.6 less-600 moreutils-0.67 ncdu-1.16 perl5.34.0-ack-3.5.0 ranger-1.9.3 stow-2.3.1 tmux-3.2a tree-1.8.0 vim-8.2.4186 watch-procps-3.3.16 wget-1.21.2 zoxide-0.8.0 Those were very intuitive to find, with the exception of ack and bash-interactive:\nbash is a bit odd because Nix splits it into two packages: a non-interactive version and an interactive version. I have no idea why. My ~/.bashrc wrecked havoc with the non-interactive version. ack is very oddly named. Really. Also: nix-env -i ack doesn\u0026rsquo;t work, but nix-env -iA nixpkgs.ack does. I suspect it will be hard to ignore -A in the future. Strictly speaking there\u0026rsquo;s nothing special about macOS in this context. The same setup can also be used in Linux distributions, for example, Debian or Ubuntu. In fact, this is what I did at $DAYJOB, because relying solely on Debian for package management is a very big limitation. I find that Nix complements the Debian repositories very well, the same way that it does for macOS.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/02/nix-env-in-a-nutshell-for-basic-usage-in-macos/","summary":"\u003cp\u003eI am currently evaluating \u003ca href=\"https://nixos.org/download.html\"\u003eNix\u003c/a\u003e as a\nreplacement for \u003ca href=\"https://brew.sh\"\u003eHomebrew\u003c/a\u003e CLI apps in macOS\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e.\n\u003ca href=\"https://wickedchicken.github.io/post/macos-nix-setup/\"\u003eOthers\u003c/a\u003e\n\u003ca href=\"https://ianthehenry.com/posts/how-to-learn-nix/switching-from-homebrew-to-nix/\"\u003ehave\u003c/a\u003e\n\u003ca href=\"https://ghedam.at/15490/so-tell-me-about-nix\"\u003epreviously\u003c/a\u003e written about this.\u003c/p\u003e\n\u003cp\u003eMy goal is to keep a sane learning curve and learn things on-the-fly, only as\nneeded. Nix is a massive ecosystem and has so many batteries included and\ncomponents (NixOS, NixPkgs, NixOps, Nix programming language, nix-shell,\nnix-env, nix-darwin, home-manager, \u0026hellip;). The good news is that those components\nare for the most part modular, there\u0026rsquo;s no need to adopt them all in order to\nreap the benefits that Nix provides.\u003c/p\u003e\n\u003cp\u003eFor now, I am only adopting \u003ccode\u003enix-env\u003c/code\u003e and \u003ccode\u003enix-shell\u003c/code\u003e, with no \u003ccode\u003e*.nix\u003c/code\u003e config\nfiles. This post covers \u003ccode\u003enix-env\u003c/code\u003e.\u003c/p\u003e","title":"★ nix-env in a nutshell for basic usage in macOS"},{"content":"One of the most classic sysadmin/DevOps tasks is to use secure shell to connect to remote machines.\nTo persist those connections, a terminal multiplexer is often used, tmux and screen being the two most popular ones.\nIn this post I will cover a few different client-side and server-side ways to have ssh automatically spawn tmux upon connection.\nOption #1: Use command-line ssh flags (client-side, recommended) Start tmux, forcing unicode, attaching to and/or creating a session named main:\n$ ssh user@host -t -- tmux -u new -A -s main -u is not strictly necessary, however I experienced occasional weirdness when connecting to some machines and omitting it. Some unicode characters wouldn\u0026rsquo;t be properly rendered, like the horizontal and vertical lines used to render tmux pane splits. Even though most machines should work just fine these days by supporting UTF-8 out-of-the-box, it\u0026rsquo;s safer to always include -u just in case.\nTip: If it\u0026rsquo;s annoying to remember to type the full command above, consider adding an alias in your shell config. Alternatively, use a ssh client that remembers your flags preferences such as the chrome secure shell extension.\nOption #2: Use ~/.ssh/config (client-side) This option is very similar to the previous one, but the flags live in the ssh config rather then being specified at the command line:\n$ cat ~/.ssh/config Host * RequestTTY yes RemoteCommand tmux -u new -A -s main You don\u0026rsquo;t need to match all hosts (Host *), if you\u0026rsquo;d rather match one or more specific hosts, refer to the ssh config syntax ssh_config(5) to add them. A simple example would be Host mymachine.example.org.\nCaveat: I\u0026rsquo;ve found this method interferes with git + ssh authentication. More specifically:\n$ git remote -v origin\tgit@github.com:thiagowfx/.dotfiles.git (fetch) origin\tgit@github.com:thiagowfx/.dotfiles.git (push) $ git push Cannot execute command-line and remote command. fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists. Therefore I discourage it, unless you only use it with specific hosts i.e. don\u0026rsquo;t use it with Host *.\nOption #3: Use ~/.bash_profile or similar (server-side, recommended) This method leverages your login shell startup config file (~/.bash_profile, ~/.zprofile, etc) to automatically spawn tmux.\n# This file is invoked as part of my ~/.bash_profile. $ cat ~/.profile.d/tmux_auto_ssh.sh. # Automatically spawn tmux within ssh sessions for interactive terminals. # https://stackoverflow.com/a/43819740/1745064 # # The session is called `main`. # Create a session with PREFIX :new, rename with PREFIX $, toggle with PREFIX s. # # Escape hatch: # ssh \u0026lt;host\u0026gt; -t -- NOTMUX=1 \u0026lt;shell\u0026gt; if [ -z \u0026#34;$NOTMUX\u0026#34; ] \u0026amp;\u0026amp; [ -z \u0026#34;$TMUX\u0026#34; ] \u0026amp;\u0026amp; [ -n \u0026#34;$SSH_TTY\u0026#34; ] \u0026amp;\u0026amp; [[ $- =~ i ]]; then tmux -u new -A -s main exit fi The if basically checks:\nwhether we\u0026rsquo;re not already inside a tmux session (we shouldn\u0026rsquo;t be), so that we don\u0026rsquo;t nest tmux whether we\u0026rsquo;re accessing the shell via ssh (we should be) whether we\u0026rsquo;re accessing an interactive shell (we should be), so that it doesn\u0026rsquo;t interfere with oneshot ssh commands There\u0026rsquo;s also a escape hatch. If you want to get an interactive shell but bypass tmux for some reason1, just set NOTMUX=1:\n$ ssh user@host -t -- NOTMUX=1 bash Final remarks My favorite methods are #1 and #3, and whether I use one or the other depends whether I want to unconditionally spawn tmux server-side, or selectively spawn tmux client-side.\nWhen using chrome secure shell (hterm) I find #1 convenient because hterm remembers your ssh host settings. That said, in scenarios where I fully control a host and it\u0026rsquo;s not solely used for production, #3 is my favorite as it works unconditionally regardless of the client terminal emulator I am using.\nFor example, maybe if tmux broke due to a recent upgrade, or if the ~/.tmux.conf is invalid.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/02/ssh-plus-tmux-automatically/","summary":"\u003cp\u003eOne of the most classic sysadmin/DevOps tasks is to use secure shell to connect to remote machines.\u003c/p\u003e\n\u003cp\u003eTo persist those connections, a terminal multiplexer is often used, \u003ccode\u003etmux\u003c/code\u003e and \u003ccode\u003escreen\u003c/code\u003e being the two most popular ones.\u003c/p\u003e\n\u003cp\u003eIn this post I will cover a few different client-side and server-side ways to have \u003ccode\u003essh\u003c/code\u003e automatically spawn \u003ccode\u003etmux\u003c/code\u003e upon connection.\u003c/p\u003e","title":"★ SSH plus tmux automatically"},{"content":"Sometimes I fire up a python interpreter in my terminal for quick prototyping, but often forget what the standard library method signatures are.\nFor example, how should I invoke subprocess.call?\nThe most straightforward action at this point is to simply google it, no shame. The first result helpfully redirects me to the official python documentation, as one would expect.\nFrom the documentation, I\u0026rsquo;d run something like this:\nsubprocess.call([\u0026#34;ls\u0026#34;, \u0026#34;-al\u0026#34;], cwd=\u0026#39;/tmp\u0026#39;) What if I wanted to figure out the correct way to do so from the command line though?\nbpython Enter bpython:\nbpython is a fancy interface to the Python interpreter for Linux, BSD, OS X and Windows (with some work). bpython is released under the MIT License. It has the following (special) features:\nIt should be available in your favorite linux distribution. Once it\u0026rsquo;s installed, a typical session would look like this:\n% bpython bpython version 0.22.1 on top of Python 3.10.2 /usr/bin/python \u0026gt;\u0026gt;\u0026gt; import subprocess \u0026gt;\u0026gt;\u0026gt; subprocess.call([\u0026#34;ls\u0026#34;, \u0026#34;-la\u0026#34;], cwd=\u0026#39;/tmp\u0026#39;) ┌──────────────────────────────────────────────────────────────────────────────────────┐ │ subprocess.call: (*popenargs, timeout=None, **kwargs) │ │ call │ │ Run command with arguments. Wait for command to complete or │ │ timeout, then return the returncode attribute. │ │ │ │ The arguments are the same as for the Popen constructor. Example: │ │ │ │ retcode = call([\u0026#34;ls\u0026#34;, \u0026#34;-l\u0026#34;]) │ └──────────────────────────────────────────────────────────────────────────────────────┘ To see all Popen arguments:\n\u0026gt;\u0026gt;\u0026gt; subprocess.Popen( ┌──────────────────────────────────────────────────────────────────────────────────────┐ │ subprocess.Popen: (args, bufsize=-1, executable=None, stdin=None, stdout=None, │ │ stderr=None, preexec_fn=None, close_fds=True, shell=False, cwd=None, env=None, │ │ universal_newlines=None, startupinfo=None, creationflags=0, restore_signals=True, │ │ start_new_session=False, pass_fds=(), *, user=None, group=None, extra_groups=None, │ │ encoding=None, errors=None, text=None, umask=-1, pipesize=-1) │ │ Popen │ │ Execute a child program in a new process. │ │ │ │ For a complete description of the arguments see the Python documentation. │ │ │ │ Arguments: │ │ args: A string, or a sequence of program arguments. │ # output truncated for brevity; bpython displays it all As you can see, it wouldn\u0026rsquo;t be difficult to have a rough idea of which arguments are available and what they do.\nI could keep going:\n\u0026gt;\u0026gt;\u0026gt; p = subprocess.run([\u0026#34;ls\u0026#34;, \u0026#34;-la\u0026#34;], cwd=\u0026#39;/tmp\u0026#39;) \u0026gt;\u0026gt;\u0026gt; p. ┌──────────────────────────────────────────────────────────────────────────────────────┐ │ args check_returncode returncode stderr │ │ stdout │ └──────────────────────────────────────────────────────────────────────────────────────┘ \u0026gt;\u0026gt;\u0026gt; p.args. ┌──────────────────────────────────────────────────────────────────────────────────────┐ │ append clear copy count extend │ │ index insert pop remove reverse │ │ sort │ └──────────────────────────────────────────────────────────────────────────────────────┘ Out-of-the-box it also displays autosuggestions based on the history of my previous commands1. It also supports python 3. For the full list of features, refer to https://bpython-interpreter.org/.\nipython Alternatively ipython2 is comparable to bpython, however I find it a bit less user-friendly out-of-the-box:\n% ipython iPython 3.10.2 (main, Jan 15 2022, 19:56:27) [GCC 11.1.0] Type \u0026#39;copyright\u0026#39;, \u0026#39;credits\u0026#39; or \u0026#39;license\u0026#39; for more information IPython 8.0.1 -- An enhanced Interactive Python. Type \u0026#39;?\u0026#39; for help. In [1]: import subprocess In [2]: subprocess. builtins contextlib io select threading call() DEVNULL list2cmdline() selectors time CalledProcessError errno os signal TimeoutExpired check_call() fcntl PIPE STDOUT types check_output() getoutput() Popen SubprocessError warnings In [2]: subprocess.call( abs() False ModuleNotFoundError SystemError all() FileExistsError NameError SystemExit any() FileNotFoundError next() TabError ArithmeticError filter() None timeout= ascii() float NotADirectoryError TimeoutError The tab completion after call( doesn\u0026rsquo;t display the documentation for it. However, appending a ? works:\n% ipython Python 3.10.2 (main, Jan 15 2022, 19:56:27) [GCC 11.1.0] Type \u0026#39;copyright\u0026#39;, \u0026#39;credits\u0026#39; or \u0026#39;license\u0026#39; for more information IPython 8.0.1 -- An enhanced Interactive Python. Type \u0026#39;?\u0026#39; for help. In [1]: import subprocess In [2]: subprocess.call? Signature: subprocess.call(*popenargs, timeout=None, **kwargs) Docstring: Run command with arguments. Wait for command to complete or timeout, then return the returncode attribute. The arguments are the same as for the Popen constructor. Example: retcode = call([\u0026#34;ls\u0026#34;, \u0026#34;-l\u0026#34;]) File: /usr/lib/python3.10/subprocess.py Type: function Furthermore, subprocess.Popen? opens a pager with the documentation for the method.\nConclusion Both bpython and ipython are excellent tools to enhance the user experience within the python interpreter, being great for quick prototyping, experimentation or exploration. bpython seems a bit more user-friendly and intuitive upon first usage, ipython takes a bit getting used to.\nfish shell and zsh-autosuggestions users should know what I\u0026rsquo;m talking about.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nipython has been around for longer and these days there\u0026rsquo;s the whole Jupyter Notebook ecosystem around it.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/02/python-interactive-completion/","summary":"\u003cp\u003eSometimes I fire up a \u003ccode\u003epython\u003c/code\u003e interpreter in my terminal for quick\nprototyping, but often forget what the standard library method signatures are.\u003c/p\u003e\n\u003cp\u003eFor example, how should I invoke \u003ccode\u003esubprocess.call\u003c/code\u003e?\u003c/p\u003e","title":"Python: interactive completion"},{"content":"As soon as we finish installing Nix on Darwin, we\u0026rsquo;re greeted with a call to action:\nAlright! We\u0026#39;re done! Try it! Open a new terminal, and type: $ nix-shell -p nix-info --run \u0026#34;nix-info -m\u0026#34; Thank you for using this installer. If you have any feedback or need help, don\u0026#39;t hesitate: You can open an issue at https://github.com/nixos/nix/issues Hello world (bloated) All right then, let\u0026rsquo;s do it!\n$ nix-shell -p nix-info --run \u0026#34;nix-info -m\u0026#34; - system: `\u0026#34;aarch64-darwin\u0026#34;` - host os: `Darwin 21.3.0, macOS 12.2` - multi-user?: `yes` - sandbox: `no` - version: `nix-env (Nix) 2.6.0` - channels(root): `\u0026#34;nixpkgs\u0026#34;` - nixpkgs: `/nix/var/nix/profiles/per-user/root/channels/nixpkgs` Cool, it works. Let\u0026rsquo;s break it down a bit.\nHello world (classic) Nix shell creates an ephemeral shell environment with the customizations you want. The most basic customization is to make a given set of packages available. There\u0026rsquo;s a hello package:\n$ nix-shell -p hello $ hello Hello, world! In case you\u0026rsquo;re curious, this is a GNU binary:\n$ hello --version hello (GNU Hello) 2.10 Copyright (C) 2014 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later \u0026lt;http://gnu.org/licenses/gpl.html\u0026gt; This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. I have no idea why they are in version 2.10 and what their changelog is. It\u0026rsquo;s such a simple binary\u0026hellip;\nIf you exit the shell, hello seemingly vanishes:\n$ exit exit $ hello zsh: command not found: hello An easy way to think of nix-shell is like an ephemeral sandbox where all your desired packages are made available when you enter it. It\u0026rsquo;s possible to provide more than one package, naturally. It\u0026rsquo;s also possible to provide a shell.nix file with the package declarations, so that when you can nix-shell without any arguments.\n$ cat shell.nix { pkgs ? import \u0026lt;nixpkgs\u0026gt; {} }: pkgs.mkShell { # nativeBuildInputs is usually what you want -- tools you need to run nativeBuildInputs = [ pkgs.buildPackages.hello ]; } $ nix-shell $ hello Hello, world! Hello world (oneshot) $ nix-shell -p hello --run hello Hello, world! This oneshot style doesn\u0026rsquo;t enter the shell, it just runs the given --run command and then exits.\nThis post just scratched the surface of what nix-shell can do. See the references below for more in-depth guides about it.\nReferences Tools You Should Know About: nix-shell An introduction to nix-shell NixOS manual: nix-shell ","permalink":"https://www.perrotta.dev/2022/02/nix-shell-in-a-nutshell/","summary":"\u003cp\u003eAs soon as we finish installing \u003ca href=\"https://nixos.org/download.html\"\u003e\u003ccode\u003eNix\u003c/code\u003e\u003c/a\u003e on\nDarwin, we\u0026rsquo;re greeted with a call to action:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eAlright! We\u0026#39;re done!\nTry it! Open a new terminal, and type:\n\n  $ nix-shell -p nix-info --run \u0026#34;nix-info -m\u0026#34;\n\nThank you for using this installer. If you have any feedback or need\nhelp, don\u0026#39;t hesitate:\n\nYou can open an issue at https://github.com/nixos/nix/issues\n\u003c/code\u003e\u003c/pre\u003e","title":"nix-shell in a nutshell"},{"content":"Not everything is available via RSS. However, there are some decent workarounds in a few situations.\nNewsletters Some blogs and authors refuse to provide RSS feeds to their websites. Instead, they will only provide newsletters. This is very hostile to the open web, and the main reason why it\u0026rsquo;s done is so that these authors can own a direct channel to reach out to their audience directly, which is better for (their) business, making it easier for them to push sponsored and promoted content and measure engagement metrics and analytics.\nKill the Newsletter is a service that proxies those newsletters, publishing them as RSS feeds.\nYou can either self-host it or use its official hosted version at https://kill-the-newsletter.com/.\nTwitter Use Nitter:\nNitter is a free and open source alternative Twitter front-end focused on privacy. The source is available on GitHub at https://github.com/zedeus/nitter\nFurthermore, it has built-in RSS support!\nFor example, you can see @taylorswift13\u0026rsquo;s profile on Nitter at https://nitter.net/taylorswift13 and follow her via RSS with https://nitter.net/taylorswift13/rss — by merely appending /rss to it.\nYou can either self-host it or use one of its public instances. At the time of this writing the official instance is https://nitter.net.\nReddit Reddit famously includes RSS support for every subreddit, for example1: https://www.reddit.com/r/archlinux/.rss. It has a lot of noise though as it includes all recent posts including the ones with a few number of votes.\nTo experience a higher quality, filtered version of the latest given subreddit posts with more than a certain threshold (of your choosing) of upvotes, check out the reddit-top-rss project:\nReddit Top RSS is a set of scripts for Reddit\u0026rsquo;s API that generates RSS feeds for specified subreddits with score thresholds. To preview your outputted feed items there is a front end that utilizes the Bootstrap v4 framework.\nYou\u0026rsquo;re supposed to self-host it, but there\u0026rsquo;s a demo version available at https://reddit-top-rss.herokuapp.com/.\nAppendix For more RSS bridges and resources, see:\nhttps://github.com/RSS-Bridge/rss-bridge https://github.com/AboutRSS/ALL-about-RSS Disclaimer: I do not endorse these lists of resources. Use them at your own risk.\nThe last slash isn\u0026rsquo;t strictly necessary: https://www.reddit.com/r/archlinux.rss is also valid.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/02/rss-bridging-the-gap/","summary":"\u003cp\u003eNot everything is available via RSS. However, there are some decent workarounds in a few situations.\u003c/p\u003e","title":"RSS: bridging the gap"},{"content":"From the homepage:\nGemini is a new internet protocol which:\nIs heavier than gopher Is lighter than the web Will not replace either Strives for maximum power to weight ratio Takes user privacy very seriously That\u0026rsquo;s too abstract though. I prefer the way Kev Quirk puts it:\nTo put that into human-digestible form; Gemini is basically a very light, text-only alternative to HTML.\nGemini aims to replace \u0026ldquo;lightweight HTML\u0026rdquo;, but it already starts with a big barrier for entry and adoption: It\u0026rsquo;s not obvious what it is by just reading its project homepage alone. This in my opinion comes off as elitist.\nFurthermore, you need a custom piece of software in order to consume the so-called gemini capsules (a fancy name for what\u0026rsquo;s the equivalent of a plain-text SSG website).\nI tried out amfora which is a popular CLI one1. Amfora is pretty decent and lightweight. The experience is very similar to a hybrid of using a CLI RSS reader like newsboat to keep track of your favorite capsules, and a CLI Web browser like elinks or w3m to navigate them.\nAnd that\u0026rsquo;s part of the adoption problem: Why would you subject yourself to purposely using a text-only browser in the 2020s? It is a painful experience, and there\u0026rsquo;s not any extra value compared to just using a minimalist RSS reader like miniflux to keep track of your favorite blogs / news portals via RSS.\nNowadays there are plenty of SSGs, for every programming language you can think of, even in plain shell scripting (POSIX sh). There\u0026rsquo;s little reason to learn a new niche protocol given that it\u0026rsquo;s relatively easy to publish simple blogs.\nConclusion: As Kev puts it:\nI’m not sure if you heard, but The Web Is F*cked and techies everywhere are touting the Gemini protocol as its saviour. I disagree. A lot.\nI will end this article with a praise for Gemini, courtesy of Drew DeVault. Drew argues that:\nMy disdain for web browsers is well documented. Web browsers are extraordinarily complex, and any attempt to build a new one would be a Sisyphean task. Successfully completing that implementation, if even possible, would necessarily produce a Lovecraftian mess: unmaintainable, full of security vulnerabilities, with gigabytes in RAM use and hours in compile times. And given that all of the contemporary web browsers that implement a sufficiently useful subset of web standards are ass and getting assier, what should we do?\nFine, but the beloved plain duo of HTML + CSS still works just fine. There\u0026rsquo;s no need to create a new, difficult-to-use protocol to force people to keep things simple. Unless you just wanna have fun and treat it like a toy or learning project; then go for it. Nothing wrong with that.\nMy interest for Gemini ends as soon as this post is published. Q.E.D.\nPackaged for every relevant platform out there nowadays.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/02/thoughts-on-gemini/","summary":"\u003cp\u003eFrom the \u003ca href=\"https://gemini.circumlunar.space\"\u003ehomepage\u003c/a\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eGemini is a new internet protocol which:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIs heavier than gopher\u003c/li\u003e\n\u003cli\u003eIs lighter than the web\u003c/li\u003e\n\u003cli\u003eWill not replace either\u003c/li\u003e\n\u003cli\u003eStrives for maximum power to weight ratio\u003c/li\u003e\n\u003cli\u003eTakes user privacy very seriously\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThat\u0026rsquo;s too abstract though. I prefer the way \u003ca href=\"https://kevq.uk/gemini-isnt-the-solution-to-the-broken-web/\"\u003eKev Quirk\u003c/a\u003e puts it:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eTo put that into human-digestible form; Gemini is basically a very light, text-only alternative to HTML.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Thoughts on Gemini"},{"content":"doas is a lightweight and safer replacement for sudo. In most occasions you invoke it exactly like sudo:\n$ sudo apt install \u0026lt;foo\u0026gt; $ doas apt install \u0026lt;foo\u0026gt; doas has gained popularity recently. Besides being the default in OpenBSD, Alpine Linux 3.15 (released last year) has also switched to it:\ndoas is the default temporary privilege escalation tool. You are advised to migrate from sudo to doas as 3.15 will be the last release to support sudo throughout its full lifecycle, in 3.16 sudo will be moved from main to community.\nIt\u0026rsquo;s not very difficult to get used to it, however you may still find yourself writing sudo occasionally. This post highlights a few ways to bridge that gap.\nUse a shell alias In your ~/.bashrc or ~/.zshrc or in your favorite shell, do:\nalias sudo=doas Caveat: Besides being an user-dependent workaround1, doas isn\u0026rsquo;t really a full drop-in replacement to sudo. This workaround will work in most day-to-day situations but it will obviously not support most sudo specific flags.\nUse a shim/wrapper (recommended) Alpine Linux provides a doas-sudo-shim package:\n$ doas apk add doas-sudo-shim This is a shim for the sudo command that utilizes doas. It supports only a subset of the sudo options (both short and long variants) that have an equivalent in doas, plus option -i (--login).\nThis is a slightly better solution, as this thin wrapper is aware of some sudo flags, translating them to the equivalent doas ones; furthermore, it works out-of-the-box and it\u0026rsquo;s system-wide. As an added bonus, it\u0026rsquo;s implemented entirely in shell script, being as much portable as possible.\nFinal remarks Last but not least, you could choose to install sudo and configure it, keeping both doas and sudo, but what\u0026rsquo;s the point? If your system favours doas, stick to doas. There\u0026rsquo;s no need to unnecessarily increase complexity by keeping around two programs that serve exactly the same purpose.\nIf you don\u0026rsquo;t like or want doas for some reason, you could look into the other way around: find a doas shim that bridges to sudo, or define an alias: $ alias doas=sudo.\nThe best long-term solution though would be to just use doas without any alias or shim, but our muscle memory may have trouble adapting to that, especially when sudo is still the de facto standard in most Linux distributions out there these days.\nTo make it system-wide, change the relevant file in /etc: for example, /etc/bashrc for bash. I would advise against it though.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/02/doas-bridging-the-sudo-gap/","summary":"\u003cp\u003e\u003ca href=\"https://man.openbsd.org/doas\"\u003e\u003ccode\u003edoas\u003c/code\u003e\u003c/a\u003e is a lightweight and safer replacement for \u003ccode\u003esudo\u003c/code\u003e. In most occasions you invoke it exactly like \u003ccode\u003esudo\u003c/code\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e$ sudo apt install \u0026lt;foo\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e$ doas apt install \u0026lt;foo\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003ccode\u003edoas\u003c/code\u003e has gained popularity recently. Besides being the default in OpenBSD, Alpine Linux 3.15 (released last year) has also \u003ca href=\"https://wiki.alpinelinux.org/wiki/Release_Notes_for_Alpine_3.15.0#Move_from_sudo_to_doas\"\u003eswitched to it\u003c/a\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003ccode\u003edoas\u003c/code\u003e is the default temporary privilege escalation tool. You are advised to migrate from \u003ccode\u003esudo\u003c/code\u003e to \u003ccode\u003edoas\u003c/code\u003e as 3.15 will be the last release to support \u003ccode\u003esudo\u003c/code\u003e throughout its full lifecycle, in 3.16 \u003ccode\u003esudo\u003c/code\u003e will be moved from main to community.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIt\u0026rsquo;s not very difficult to get used to it, however you may still find yourself writing \u003ccode\u003esudo\u003c/code\u003e occasionally. This post highlights a few ways to bridge that gap.\u003c/p\u003e","title":"Doas: bridging the sudo gap"},{"content":"After years of using bash as my default interactive shell at $DAYJOB, I decided to switch to zsh. I didn\u0026rsquo;t want to start from scratch and lose all my history though:\n$ wc -l ~/.bash_history | cut -f1 -d\u0026#39; \u0026#39; 64002 Thus my goal was to first migrate all my history from bash to zsh.\nThe bash-to-zsh-hist.py python script in this gist did most of the job:\n#!/usr/bin/env python # -*- coding: utf-8 -*- # # This is how I used it: # $ cat ~/.bash_history | python bash-to-zsh-hist.py \u0026gt;\u0026gt; ~/.zsh_history import sys import time def main(): timestamp = None for line in sys.stdin.readlines(): line = line.rstrip(\u0026#39;\\n\u0026#39;) if line.startswith(\u0026#39;#\u0026#39;) and timestamp is None: t = line[1:] if t.isdigit(): timestamp = t continue else: sys.stdout.write(\u0026#39;: %s:0;%s\\n\u0026#39; % (timestamp or time.time(), line)) timestamp = None if __name__ == \u0026#39;__main__\u0026#39;: main() To use it:\n$ wget https://gist.githubusercontent.com/muendelezaji/c14722ab66b505a49861b8a74e52b274/raw/49f0fb7f661bdf794742257f58950d209dd6cb62/bash-to-zsh-hist.py $ chmod +x ./bash-to-zsh-hist.py $ cat .bash_history | ./bash-to-zsh-hist.py \u0026gt;\u0026gt; ~/.zsh_history However, that didn\u0026rsquo;t fully work. Upon running zsh, there was an error:\n$ zsh zsh: corrupt history file /usr/local/google/home/tperrotta/.zsh_history A quick google search led me to a blog post. I adapted the command suggest therein1:\n$ strings -eS .zsh_history | sponge .zsh_history And that fixed the issue!\nsponge comes from the moreutils package.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/02/transfer-bash-history-to-zsh/","summary":"\u003cp\u003eAfter years of using \u003ccode\u003ebash\u003c/code\u003e as my default interactive shell at $DAYJOB,\nI decided to switch to \u003ccode\u003ezsh\u003c/code\u003e. I didn\u0026rsquo;t want to start from scratch and lose all\nmy history though:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e$ wc -l ~/.bash_history | cut -f1 -d\u003cspan style=\"color:#e6db74\"\u003e\u0026#39; \u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e64002\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThus my goal was to first migrate all my history from \u003ccode\u003ebash\u003c/code\u003e to \u003ccode\u003ezsh\u003c/code\u003e.\u003c/p\u003e","title":"Transfer bash history to zsh"},{"content":"My search engine of choice is Google, nonetheless I still enjoy DuckDuckGo occasionally.\nThe main reason to stick with Google is its superior quality, by the means of better search results. 20 years later, it\u0026rsquo;s still arguably the best search engine. Of course, part of the reason for that is contingent upon how much data it collects, but that\u0026rsquo;s a topic for another day\u0026hellip;\nThere are at least three reasons to use DuckDuckGo as an alternative:\nit\u0026rsquo;s privacy-focused, there\u0026rsquo;s no tracking and no bubbling1 its search results stem from sources other than Google; these days, mostly from Bing it has !bangs, lots of them The quest(ion) then becomes: How can I use mostly Google, but still have quick access to DuckDuckGo bangs?\nThere are several ways to do so.\nDuckDuckGoog DuckDuckGoog is a search engine which does exactly that:\nSearches Google and !bangs DuckDuckGo. Tell your browser!\nIf I search for i3, it will open https://www.google.com/search?q=i3 and probably think I am interested in Intel i3 CPUs. If I search for !aw i3, using the ArchWiki bang, it opens https://wiki.archlinux.org/title/I3 and goes straight to the i3 window manager page in the ArchWiki, exactly what I wanted. If I search for !ddg i3, it opens https://duckduckgo.com/?q=i3, on DuckDuckGo. Caveat: You cannot add custom search engines to Safari, therefore this method only works in other browsers (Firefox, Chrome, etc).\nSelf-Hosted DuckDuckGoog claims to collect no data:\nIt\u0026rsquo;s quite simple. DuckDuckGoog doesn\u0026rsquo;t track any queries submitted whatsoever, It simply redirects you to DuckDuckGo or Google depending on whether your search contains a !bang or not.\nIf you still don\u0026rsquo;t trust it for some reason, you could also self-host it in your own server, as it\u0026rsquo;s open source.\nOne advantage of doing so is using (and owning) your own infrastructure, which is probably more reliable in terms of bandwidth and latency than a random guy\u0026rsquo;s server in the wild.\n!Bang Quick Search !Bang Quick Search is a Chrome extension:\nThis extension adds DuckDuckGo !bang search to chrome. You can use it from the URL bar as long as your default search engine is either google or bing (for now). You can also use it directly on google\u0026rsquo;s and bing\u0026rsquo;s websites.\nSo long as your search engine is set to either Google or Bing, it will intercept !bangs from your query to redirect them to DuckDuckGo.\nTip: Use !ddg to search on DuckDuckGo.\nCaveat: As a Chrome extension, it obviously only works in Chrome (or any of its derivatives like Edge or Vivaldi).\nDuckDuckGo Another simple way is to just use DuckDuckGo directly. Whenever you want to go to Google, just add !g to your query.\nFinal words I used all three methods in the past. My favorite one these days is the Chrome Extension because Chrome is my current browser.\nAs a fallback I find that using DuckDuckGo directly is acceptable as well, however it quickly becomes quite annoying to constantly add !g to every query. Defaults matter.\nRelated Switching to DuckDuckGo Biased search results based on your past searches.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/02/google-and-duckduckgo/","summary":"\u003cp\u003eMy search engine of choice is \u003ca href=\"https://google.com/\"\u003eGoogle\u003c/a\u003e, nonetheless I still enjoy \u003ca href=\"https://duckduckgo.com/\"\u003eDuckDuckGo\u003c/a\u003e occasionally.\u003c/p\u003e\n\u003cp\u003eThe main reason to stick with Google is its superior quality, by the means of better search results. 20 years later, it\u0026rsquo;s still arguably the best search engine. Of course, part of the reason for that is contingent upon how much data it collects, but that\u0026rsquo;s a topic for another day\u0026hellip;\u003c/p\u003e\n\u003cp\u003eThere are at least three reasons to use DuckDuckGo as an \u003cem\u003ealternative\u003c/em\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eit\u0026rsquo;s privacy-focused, there\u0026rsquo;s no tracking and no bubbling\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e\u003c/li\u003e\n\u003cli\u003eits search results stem from sources other than Google; these days, mostly from Bing\u003c/li\u003e\n\u003cli\u003eit has \u003ca href=\"https://duckduckgo.com/bang\"\u003e\u003cstrong\u003e!bangs\u003c/strong\u003e\u003c/a\u003e, lots of them\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe quest(ion) then becomes: How can I use mostly Google, but still have quick access to DuckDuckGo bangs?\u003c/p\u003e","title":"Google and Duckduckgo"},{"content":"In this post we will cover a few linux swap recipes.\nEmpty swap space Completely empty (flush) swap space:\n% swapoff --all \u0026amp;\u0026amp; swapon --all Decrease swappiness Emptying is too extreme. Why did you get so much swap in the first place? A small tweak is to decrease the sensibility of the system to swap:\n$ cat /etc/sysctl.d/90-custom.conf vm.swappiness=20 vm.vfs_cache_pressure=50 The default swappiness of the Linux kernel these days is 60%, which IMHO is quite aggressive for desktop usage. By decreasing it to 20%, our system will only start to swap once we use more than 80% of total RAM. In other words, only when there is 20% or less of free / available RAM.\nvfs_cache_pressure:\nThis percentage value controls the tendency of the kernel to reclaim the memory which is used for caching of directory and inode objects.\nAt the default value of vfs_cache_pressure=100 the kernel will attempt to reclaim dentries and inodes at a \u0026ldquo;fair\u0026rdquo; rate with respect to pagecache and swapcache reclaim. Decreasing vfs_cache_pressure causes the kernel to prefer to retain dentry and inode caches. When vfs_cache_pressure=0, the kernel will never reclaim dentries and inodes due to memory pressure and this can easily lead to out-of-memory conditions. Increasing vfs_cache_pressure beyond 100 causes the kernel to prefer to reclaim dentries and inodes.\nHowever, /etc/sysctl.d settings will only be applied after a reboot. To apply them immediately, use the sysctl(8) command:\n% sudo sysctl -p /etc/sysctl.d/90-custom.conf vm.swappiness = 20 vm.vfs_cache_pressure = 50 Use a swapfile If you find yourself with a fully partitioned disk without any dedicated swap partition, there\u0026rsquo;s a trick to adding swap anyway: Use a swap file! Everything is a file anyway!\n# https://wiki.archlinux.org/title/Swap#Swap_file # Create the swap file: 8GiB in this case, to match our total RAM % dd if=/dev/zero of=/swapfile bs=1M count=8000 status=progress # Set restricting permissions % chmod 600 /swapfile # Format the ~~partition~~ file % mkswap /swapfile # Activate the swap file % swapon /swapfile You can check it\u0026rsquo;s working correctly by inspecting /proc/swaps:\n% cat /proc/swaps Filename\tType\tSize\tUsed\tPriority /swapfile file\t8388604\t0\t-2 Then finally add it to your /etc/fstab so that it is automatically mounted in subsequent boots:\n# swap file /swapfile none swap defaults 0 0 Add ZRAM swap Explaining zram is out of scope if this post, but check out the ArchWiki or Wikipedia.\nThe recipe I use in Arch Linux is the zramswap package:\nInstall the package. Set desired zram swap percentage, I picked 20%: % cat /etc/zramswap.conf ZRAM_SIZE_PERCENT=20 Enable/Start the service: % systemctl enable --now zramswap % systemctl status zramswap ● zramswap.service - Zram-based swap (compressed RAM block devices) Loaded: loaded (/usr/lib/systemd/system/zramswap.service; enabled; vendor preset: disabled) Active: active (exited) since Tue 2022-02-01 16:13:37 EST; 7h ago Main PID: 582 (code=exited, status=0/SUCCESS) CPU: 27ms Feb 01 16:13:37 localhost.localdomain systemd[1]: Starting Zram-based swap (compressed RAM block devices)... Feb 01 16:13:37 localhost.localdomain zramctrl[627]: Setting up swapspace version 1, size = 1.5 GiB (1654009856 bytes) Feb 01 16:13:37 localhost.localdomain zramctrl[627]: LABEL=zram0, UUID=a39e0131-f102-4503-a1e7-a3e0ca330126 Feb 01 16:13:37 localhost.localdomain systemd[1]: Finished Zram-based swap (compressed RAM block devices). You can inspect /proc/swaps again to check it\u0026rsquo;s working properly1:\n% cat /proc/swaps Filename\tType\tSize\tUsed\tPriority /swapfile file\t8388604\t0\t-2 /dev/zram0 partition\t1615244\t0\t100 zswap should have more priority than the swap file.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/02/linux-swap-shenanigans/","summary":"\u003cp\u003eIn this post we will cover a few linux swap recipes.\u003c/p\u003e","title":"Linux swap shenanigans"},{"content":"In this post we will learn how to share environment variables (e.g. $GDK_SCALE) between a system user session and X11/Xorg.\nThe typical ~/.xinitrc and/or ~/.xprofile setup in 2020s involves some environment variable exports such as the following:\n# fix java application decorations, for tiling window managers export _JAVA_AWT_WM_NONREPARENTING=1 # make Chrome pick up proxy settings stored in gconf export DESKTOP_SESSION=gnome # HiDPI settings for GTK3+ export GDK_DPI_SCALE=0.5 export GDK_SCALE=2 # HiDPI settings for QT export QT_FONT_DPI=192 This particular set of customizations stems from my dotfiles but there isn\u0026rsquo;t anything special about it. I\u0026rsquo;ll include an explanation anyway for completeness:\nThe java setting is meant for launching certain java-based applications from within a tiling window manager.\nAll the other settings are meant for 4K HiDPI displays. The baseline DPI is 96, which is too small for 4K monitors, the fonts and icons all look tiny. In order to make them scale it\u0026rsquo;s necessary to use a higher DPI. Typical setups use either 144 (x1.5) or 192 (x2.0), the bigger the DPI the bigger fonts and icons will appear in the screen.\nThose exports work well for graphical applications launched from your favorite window manager after it has already started, however if you decide to launch an application from systemd, those settings will not be picked up by it.\nFor example, if you decide to manage redshift1 (more specifically, redshift-gtk which has a system tray app) from a systemd user session2, its fonts will look small.\nThere are several ways to address this issue.\nOne of them is to edit the service file directly:\n$ systemctl --user edit redshift-gtk And then add:\n[Unit] Environment=GDK_SCALE=2 GDK_DPI_SCALE=0.5 Which results in:\n$ cat ~/.config/systemd/user/redshift-gtk.service.d/override.conf [Unit] Environment=GDK_SCALE=2 GDK_DPI_SCALE=0.5 Which you can make effective by:\n$ systemctl --user daemon-reload $ systemctl --user restart redshift-gtk I am not a fan of this approach though, because this step would need to be repeated to all service files you want to manage this way. There\u0026rsquo;s a better, DRY way to do so.\nsystemd supports environment files (environment.d(5)). User-defined ones live in ~/.config/environment.d/*.conf by default.\nThis means we could produce the following file:\n$ cat ~/.config/environment.d/user.conf # systemd environment.d(5) EnvironmentFile # https://www.freedesktop.org/software/systemd/man/environment.d.html # # Do not use export here. # # Alternatively # systemctl --user import-environment [var1] [var2] [...] # # Troubleshooting # systemctl --user show-environment # fix java application decorations, for tiling window managers _JAVA_AWT_WM_NONREPARENTING=1 # make Chrome pick up proxy settings stored in gconf DESKTOP_SESSION=gnome # HiDPI settings for GTK3+ GDK_DPI_SCALE=0.5 GDK_SCALE=2 # HiDPI settings for QT QT_FONT_DPI=192 Which is applied to all systemd user service files automatically, no need to set Environment= manually everywhere.\nHowever, now we need to maintain two different files: the systemd .conf one and the xorg ~/.xinitrc one.\nOne elegant way to reduce maintenance burden is, in my opinion, the follownig:\n$ cat ~/.xinitrc ... # Parse user session environment variables. # This file is shared with the systemd user instance. # Export all variables: https://stackoverflow.com/a/30969768/1745064 set -a [ -r ~/.config/environment.d/user.conf ] \u0026amp;\u0026amp; . ~/.config/environment.d/user.conf set +a It does what you expect: the underlying shell sources the *.conf file as if you were exporting each variable therein.\nOne caveat of this setup is that you cannot define the variables dynamically; for example, with subshells, with external programs, or with simple mathematical operations derived from other variables3.\nUltimately though you end up with only one file to manage, which is the systemd one. KISS™.\nXKCD Courtesy of Randall Munroe\nRedshift adjusts the color temperature of your screen according to your surroundings. This may help your eyes hurt less if you are working in front of the screen at night. Redshift is similar to f.lux.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nsystemctl --user start redshift.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFor example, QT_FONT_DPI=$(($GDK_SCALE * 96)) or similar.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/01/systemd-share-environment-variables-with-xorg/","summary":"\u003cp\u003eIn this post we will learn how to share environment variables (e.g.\n\u003ccode\u003e$GDK_SCALE\u003c/code\u003e) between a system user session and X11/Xorg.\u003c/p\u003e","title":"★ systemd: share environment variables with xorg"},{"content":"This post describes some tooling usages to watch for file changes and run or reload a command whenever they happen.\nContext I am contributing to miniflux, a minimalist and opinionated RSS reader. Miniflux\u0026rsquo;s stack is as minimalist as the app itself: It\u0026rsquo;s a Golang application that connects to a local PostgreSQL database. It has a well-documented and comprehensive Makefile.\nIn order to achieve an edit-and-preview workflow for quick prototyping and local iteration, all that it\u0026rsquo;s needed is to execute make run whenever any1 file in the repository is changed.\nMy goal was to achieve that workflow with the least amount of friction, and with an application that is widely available in most package managers / linux distributions out there.\nOption #1: entr (recommended) entr(1):\nRun arbitrary commands when files change\nThe following invocation does the job:\n$ fd | entr -r -- make run However, we could do better. From the upstream docs:\n» ag and ack offer many advantages over utilities such as find(1) or ls(1) in that they recognize files by their contents and are smart enough to skip directories such as .git\nI am happy with fd for this use case though. To limit entr to .go files only, we could do:\n$ fd -e go | entr -r -- make run It took me less than 5 minutes to install and figure out how to use entr.\nThis blog post covers it in more detail.\nOption #2: watchman watchman from Facebook Open Source:\nWatchman exists to watch files and record when they change. It can also trigger actions (such as rebuilding assets) when matching files change.\nWatchman\u0026rsquo;s workflow doesn\u0026rsquo;t seem to be very suited for this job though. It\u0026rsquo;s much more centered on subscribing to inotify events:\ncd \u0026lt;repository root\u0026gt; watchman watch . \u0026hellip;and then adding predefined actions to recompile parts of the application as they change. The official docs give an example with CSS minification:\n# set up a trigger named \u0026#39;buildme\u0026#39; # will run \u0026#39;minify-css\u0026#39; whenever a CSS file is changed watchman -- trigger . buildme \u0026#39;*.css\u0026#39; -- minify-css In this regard it seems to be more modular, and I could easily see a scenario where I would kick off several specialized triggers in a webdev project: for example, one for CSS minification, one for JS minification, another one for TypeScript compilation, etc.\nThat said, for the simple use case of triggering (and reloading) make run, it seems overkill. I also found its official docs too verbose and lacking sample usages for simple Makefile-based projects like miniflux.\nOne caveat of watchman is that it\u0026rsquo;s less widely available than entr. Another caveat is that recently official distributions of watchman seem to be binary only, even though watchman itself is open source.\nIt took me several minutes to figure out what\u0026rsquo;s the gist of watchman, only to realize it is more bloated than warranted.\nConclusion For simple projects, entr is the way to go, hands down. For complex webdev projects, I would look into watchman more deeply.\nTo be truly strict, only changes to .go files matter.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/01/watch-files-and-react-to-changes-during-development/","summary":"\u003cp\u003eThis post describes some tooling usages to watch for file changes and run or reload a command whenever they happen.\u003c/p\u003e","title":"★ Watch files and react to changes during development"},{"content":"A few recipes for remounting linux devices / disks. It mostly boils down to running mount -o remount as root.\nRemount as read-write If /dev/sdb1 is mounted on /mnt/data as read-only (ro), it could be remounted as rw:\n% mount -o remount,rw /mnt/data or\n% mount -o remount,rw /dev/sdb1 Increase RAM disk size /dev/shm (shared memory) is typically allocated half of the available amount of RAM in the system. For example, in my 8GB Arch Linux system:\n$ df -h | grep /dev/shm tmpfs 3.9G 127M 3.8G 4% /dev/shm To increase the amount of space allocated to it:\n% mount -o remount,size=8G /dev/shm The result:\n$ df -h | grep /dev/shm tmpfs 8.0G 72M 8.0G 1% /dev/shm ","permalink":"https://www.perrotta.dev/2022/01/linux-remount-device-with-different-options/","summary":"\u003cp\u003eA few recipes for remounting linux devices / disks. It mostly boils down to running \u003ccode\u003emount -o remount\u003c/code\u003e as root.\u003c/p\u003e","title":"Linux: Remount device with different options"},{"content":"Last year I found out about Advent of Code.\nYou said what? Advent of Code by Eric Wastl happens every year since 2015, every December from the 1st to the 25th. Each day there\u0026rsquo;s a new programming challenge1 split into two parts. The first part tends to be easier than the second one. The second part usually builds upon the first one, being a follow-up task that requires more steps and/or with a higher degree of complexity. You can\u0026rsquo;t always reuse the bits from the first part to solve the second one though.\nFor those familiar with programming contests like ACM ICPC or OBI, or online judges like SPOJ or UVa, advent of code feels like home. The main difference is that there is no time pressure and no need to write spaghetti and unreadable code; in fact, writing readable and elegant solutions is encouraged (citation needed\u0026hellip;).\nFor those familiar with FAANG/Tech whiteboard interviews, advent of code feels a lot like a typical interview. I would even go further and say it\u0026rsquo;s a great way to practice for interviews.\nIt is a great moment to either (i) learn a new exciting programming language or (ii) improve your mastery on programming languages that you already know. I know several people (see below) that used AoC2 to learn Rust or Kotlin or whatever else was exciting for them at the time. It\u0026rsquo;s surprising that the official Kotlin Docs even contain a section called Advent of Code puzzles in idiomatic Kotlin.\nSome folks go even further and use it to practice their code golfing3 or even Google Sheets skills. I have a deep amount of respect for them as it\u0026rsquo;s quite a challenge. If you think it stops there, I\u0026rsquo;ve also seen solutions in awk and sed.\nAnother positive aspect of AoC is that it has an integrated dashboard that tracks your progress as you go. It\u0026rsquo;s a simple element of gamification that immensely improves motivation and fun. You really feel a big desire to collect all those 50 stars\u0026hellip;\nWhat about me? My goal for 2021 was relatively less ambitious than that, I just wanted to improve my Python skills, more specifically Python 3. I learned Python 2 during my first year in university and used it sparingly at work and for personal endeavours, but always had a knowledge gap in Python 3.\nI set up a public git repository with my solutions and aspired to write simple and elegant python, my only constraint was to limit myself to what is available in the standard library of a vanilla python3 installation in Alpine linux, with the exception of numpy which is widespread enough to deserve an entry in my requirements.txt, and of course devtools like debuggers, linters and auto formatters as needed.\nAs an additional, non-programming challenge I also limited myself to only use the command line. This basically meant no IDEs4. My programming environment was ultimately ssh to an Alpine Linux VPS + tmux + vim. To make my life easier, one of the first tasks I accomplished was to write a generic Makefile to help me test and run my scripts. A typical invocation would look like:\n$ make DEBUG=1 DAY=3 \u0026hellip;whereas I could choose between the sample input versus the real one with DEBUG, and the puzzle day with DAY.\nWas the experience worth it? Definitely yes! Even though I only completed ~8 puzzles out of the 25 ones due to having my attention split with another project I was working on at the time, the thematic submarine puzzles were hella fun and I learned a lot of python 3 on the way.\nA few highlights of what I learned and used from my python 2to3 transition were f-strings / string interpolation (print(f'The sum is {sum}')), \u0026ldquo;everything is an iterator now\u0026rdquo; even map and range, the standard library is awesome and sometimes you stumble upon useful abstractions like Counter and defaultdict, sort is different now (key instead of comparison function), this pdb debugger thingy, among other topics I can\u0026rsquo;t remember at the moment. I realized the only concept that was previously familiar was the different syntax of the print function (you have to use parentheses now).\nIn terms of workflow, I also learned that virtual environments are now supported natively5 (python -m venv), direnv is an amazing tool to automate/manage environments in git repositories and also happens to have first-class python integration, pylint and autopep8 are good integrations with vim to help spot basic errors and/or suggest best practices, and numpy takes forever to build from source.\nWhat about the community? AoC enjoys a lot of popularity and zeitgeist, especially during times of the COVID-19 pandemic, but even before then. There\u0026rsquo;s a large /r/adventofcode subreddit community, lots of people share their solution snippets and impressions on Twitter (#AdventOfCode), there\u0026rsquo;s a ton of public git repositories on GitHub where people share their coding solutions, in pretty much any programming language you can think of, and finally there are many screencasts on YouTube. The Internet in the 2020s sparks creativity in every unimaginable corner.\nThere\u0026rsquo;s so much information that it\u0026rsquo;s impossible to stay on top of everything. Here is a small list of repositories that I followed this year, most of those are acquaintances/friends and/or stumbled upon Twitter:\nC++:\nhttps://github.com/riuri/adventofcode Python:\nhttps://github.com/sjvrijn/AdventofCode https://github.com/oomenn/AOC Rust:\nhttps://github.com/dimo414/advent-2021 https://github.com/mfs/aoc Edit (2024-08-06): Additions from 2022:\nC++:\nhttps://github.com/eariassoto/advent-of-code-cpp Python:\nhttps://github.com/achrafmam2/adventofcode https://github.com/mcerdeiro/aoc2022 I find it\u0026rsquo;s really constructive and useful (and also fun) to peek at other people\u0026rsquo;s solutions after I coded my own. I have extensive (albeit kinda rusty these days) experience with C++ so I wanted to follow at least one repository coded with it; since I wrote my solutions in python it was also a natural choice to follow a few python repositories; and, finally, I wanted to peek at some languages I am not familiar with to get a gist of them. This year I watched Rust and a few bits of Clojure and Kotlin on Twitter.\nFinally, for some extra inspiration, there are also some 10x programmers6 out there that seem to be fans of AoC as well: Peter Norvig and Russ Cox (rsc). There are probably several others I am not aware of.\nFinal remarks I am hoping to participate in AoC this year (2022) as well, and possibly revisit the 2021 puzzles and resolve the rest of the ones I missed as time permits.\nHopefully this post encourages and motivates you to try Advent of Code as well! Happy coding.\nOr puzzle, if you will.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAcronym not to be confused with a certain annoying^W politician.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFor those unfamiliar with the concept, code golfing is all about writing a correct solution with the fewest amount of characters.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFor example: PyCharm, and also VSCode, which is getting so big these days I don\u0026rsquo;t even know if it\u0026rsquo;s possible to just call it a simple text editor anymore.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBack in the days, virtualenvwrapper was all the rage.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe 10x programmer thing is a well-known joke however in this instance the mentioned characters are indeed superb programmers that I immensely respect.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/01/advent-of-code/","summary":"\u003cp\u003eLast year I found out about \u003ca href=\"https://adventofcode.com/\"\u003eAdvent of Code\u003c/a\u003e.\u003c/p\u003e","title":"★ Advent of Code"},{"content":"ChromeOS hterm (\u0026ldquo;Secure Shell extension\u0026rdquo;) is one of my favorite chrome extensions. It is a bit dull with its out-of-the-box monospace font though. In this post we\u0026rsquo;ll learn how to customize it.\nGoogle Fonts The easiest way to customize the Secure Shell extension to use a custom font is to select one from Google Fonts. Once you select a font from there, it will give you information like this:\nUse on the web To embed a font, copy the code into the \u0026lt;head\u0026gt; of your html ( ) \u0026lt;link\u0026gt; (x) @import \u0026lt;style\u0026gt; @import url(\u0026#39;https://fonts.googleapis.com/css2?family=IBM+Plex+Sans\u0026amp;display=swap\u0026#39;); \u0026lt;/style\u0026gt; font-family: \u0026#39;IBM Plex Sans\u0026#39;, sans-serif; All we have to do is to copy the URL within the url('...') fragment above, go to the settings of the Secure Shell extension, and then paste it there:\n# Example 1: IBM Plex Sans Custom CSS (URI): https://fonts.googleapis.com/css2?family=IBM+Plex+Sans\u0026amp;display=swap # Example 2: Fira Code Custom CSS (URI): https://fonts.googleapis.com/css2?family=Fira+Code:wght@400;700\u0026amp;display=swap # Example 3: Combine both https://fonts.googleapis.com/css2?family=Fira+Code:wght@400;700\u0026amp;family=IBM+Plex+Sans\u0026amp;display=swap And then set the extension to use it:\nText font family: \u0026#34;IBM Plex Sans\u0026#34;, \u0026#34;Fira Code\u0026#34;, monospace From Secure Shell FAQ:\nBy default, we disable ligatures. Some fonts actively enable them like macOS\u0026rsquo;s Menlo (e.g. “ae” is rendered as “æ”). This messes up copying and pasting and is, arguably, not terribly legible for a terminal.\nIf your font supports ligatures, consider enabling them:\nCustom CSS (inline text): * { -webkit-font-feature-settings: \u0026#34;liga\u0026#34; on, \u0026#34;calt\u0026#34; on; -webkit-font-smoothing: antialiased; text-rendering: optimizeLegibility; } Not all fonts are available on Google Fonts though. For example, Hermit is one of my current monospace favorites: it\u0026rsquo;s not there1.\nGithub Many fonts are available on GitHub (or in other forges), checked into a git repository.\nIf you happen to find a .woff2 web font file laying therein, you could also use it in hterm:\nCustom CSS (inline text): @font-face { font-family: \u0026#34;Anonymous Pro\u0026#34;; src: url(https://cdn.rawgit.com/wernight/powerline-web-fonts/8040cf32c146c7cd4f776c1484d23dc40685c1bc/fonts/AnonymousPro.woff2); } And then set the extension to use it:\nText font family: \u0026#34;Anonymous Pro\u0026#34;, monospace Note: I couldn\u0026rsquo;t get this method to work with .ttf or .otf.\nhttps://github.com/pcaro90/hermit/issues/2\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/01/chromeos-hterm-customize-fonts/","summary":"\u003cp\u003e\u003ca href=\"https://chrome.google.com/webstore/detail/secure-shell/iodihamcpbpeioajjeobimgagajmlibd?hl=en\"\u003eChromeOS hterm\u003c/a\u003e (\u0026ldquo;Secure Shell extension\u0026rdquo;) is one of my favorite chrome extensions.\nIt is a bit dull with its out-of-the-box monospace font though.\nIn this post we\u0026rsquo;ll learn how to customize it.\u003c/p\u003e","title":"ChromeOS hterm: customize fonts"},{"content":"A brief list of disposable / throwaway account resources:\nEmail inbox https://dispostable.com/ https://10minutemail.com/ https://yadim.dismail.de/ NAT / public URLs https://ngrok.com/ Pastebin https://paste.debian.net/ https://paste.dismail.de/ https://paste.opensuse.org/ https://upaste.de/ (produces short URLs, deleted after one hour) SMS https://smsreceivefree.com/ Appendix The New Oil has some good tips on disinformation for throwaway and/or ephemeral accounts. ","permalink":"https://www.perrotta.dev/2022/01/throwaway-accounts-for-ephemeral-use-cases/","summary":"\u003cp\u003eA brief list of disposable / throwaway account resources:\u003c/p\u003e","title":"Throwaway accounts for ephemeral use cases"},{"content":"This post covers how to add DNS entries / mappings to a local network managed with pihole.\nThere are several ways to do so:\n1. The CLI way: /etc/pihole/ Edit /etc/pihole/custom.list, set one mapping per line, just as you would for /etc/hosts:\n$ cat /etc/pihole/custom.list 127.0.0.1 localhost.corp.google.com 192.168.1.75 myhostname.home.arpa This works because /etc/dnsmasq.d/01-pihole.conf contains addn-hosts=/etc/pihole/custom.list by default.\nFrom Gentoo Wiki:\nIt is possible to refer to an (additional) hosts file to use as source for DNS queries. To do so, add the -H /path/to/hostsfile (\u0026ndash;addn-hosts=/path/to/hostsfile) command line option. It is also possible to pass a directory; in that case, all files inside that directory will be treated as additional hosts files.\n2. The CLI way: /etc/dnsmasq.d/ $ cat /etc/dnsmasq.d/03-pihole-custom-dns.conf address=/localhost.corp.google.com/127.0.0.1 address=/myhostname.home.arpa/192.168.1.75 From ArchWiki:\nIn some cases, such as when operating a captive portal, it can be useful to resolve specific domains names to a hard-coded set of addresses. This is done with the address config.\n3. The Web way Navigate to http://pi.hole/admin/dns_records.php and set your DNS records there. From pihole docs:\nThe order of locally defined DNS records is:\nThe device\u0026rsquo;s host name (/etc/hostname) and pi.hole Configured in a config file in /etc/dnsmasq.d/ Read from /etc/hosts Read from the \u0026ldquo;Local (custom) DNS\u0026rdquo; list (stored in /etc/pihole/custom.list) (the aforementioned ways) Only the first record will trigger an address-to-name association.\nWrapping up Then restart pihole to apply changes:\n$ pihole restartdns ","permalink":"https://www.perrotta.dev/2022/01/pihole-add-custom-dns-mappings/","summary":"\u003cp\u003eThis post covers how to add DNS entries / mappings to a local network managed\nwith \u003ca href=\"https://pi-hole.net/\"\u003epihole\u003c/a\u003e.\u003c/p\u003e","title":"Pihole: Add custom DNS mappings"},{"content":"This document describes my workflow to manage APKBUILDs for the aports repository in Alpine Linux.\nDisclaimer First of all, this post is not a substitute to the AlpineWiki and it will likely get outdated at some point. In particular, refer to the following articles for up-to-date documentation that will outlive this blog:\nhttps://wiki.alpinelinux.org/wiki/APKBUILD_Reference https://wiki.alpinelinux.org/wiki/Abuild_and_Helpers https://wiki.alpinelinux.org/wiki/Aports_tree https://wiki.alpinelinux.org/wiki/Creating_an_Alpine_package This article is not a tutorial, as such it assumes you already know what an APKBUILD is and how to use abuild. In particular, you should have the alpine-sdk, atools and spdx-licenses-list packages installed in your system.\nStructure I manage my packages with git. Create a GitLab account on https://gitlab.alpinelinux.org/, fork the aports tree, and git clone your fork.\nThe structure follows Alpine Linux repositories:\n$ git clone https://gitlab.alpinelinux.org/alpine/aports.git \u0026amp;\u0026amp; tree -L 1 aports aports ├── CODINGSTYLE.md ├── COMMITSTYLE.md ├── README.md ├── community ├── main ├── non-free ├── scripts ├── testing └── unmaintained Bootstrapping I am going to illustrate with a package I added recently, sensible-utils:\nBefore you even begin, check if the package already exists, do a quick search in the Alpine Repositories1.\nStart by scaffolding a new APKBUILD from the base template:\n$ cd aports/testing # Always add new packages in testing/ first. $ newapkbuild sensible-utils $ cd sensible-utils $ $EDITOR APKBUILD Note: If you have a language-specific package (e.g. perl, python, rust), use the language-specific template instead of the base one. Run newapkbuild -h to list available templates. There are also some apkbuild-* helpers such as apkbuild-pypi and apkbuild-cpan.\nFill in APKBUILD metadata like pkgname=, url=, etc. Refer to the AlpineWiki for up-to-date best practices.\nBy doing so, I produced the following APKBUILD:\npkgname=sensible-utils pkgver=0.0.14 pkgrel=0 pkgdesc=\u0026#34;Utilities for sensible alternative selection\u0026#34; url=\u0026#34;https://packages.debian.org/source/sensible-utils\u0026#34; arch=\u0026#34;all\u0026#34; license=\u0026#34;GPL-2.0-or-later\u0026#34; makedepends=\u0026#34;po4a\u0026#34; subpackages=\u0026#34;$pkgname-doc\u0026#34; source=\u0026#34;http://ftp.debian.org/debian/pool/main/s/$pkgname/${pkgname}_$pkgver.tar.xz\u0026#34; builddir=\u0026#34;$srcdir/$pkgname.git\u0026#34; build() { ./configure --prefix=/usr make } check() { make -k check } package() { make DESTDIR=\u0026#34;$pkgdir/\u0026#34; install # only works with update-alternatives, specific to debian rm \u0026#34;$pkgdir/usr/bin/select-editor\u0026#34; } sha512sums=\u0026#34; 15ba996f811ab3a9c1f5726f35766d74aafdf925c5c2392b33c6643d6c439796a742f9d0f4625c79de640e6b5e4a6a032b768eb1bc4ac31b448f9767b0ceed44 sensible-utils_0.0.14.tar.xz \u0026#34; Note: $srcdir refers to the src/ directory within sensible-utils. $pkgdir refers to the pkg/ directory within sensible-utils.\nIf you\u0026rsquo;re used to Arch Linux PKGBUILDs you\u0026rsquo;ll notice a striking similarity to APKBUILDs. I highlighted a few notable differences in a previous post, My First APKBUILD.\nAdjustments Generate the checksums with abuild checksum. It will automatically update the APKBUILD inplace.\nDownload and extract package files with abuild unpack.\nls src/ and check the directory structure. Update $builddir in your APKBUILD to match it. Usually it will be $srcdir/$pkgname-$pkgver, but sometimes tiny adjustments are necessary. In this case, it was $srcdir/$pkgname.git.\nThen run abuild -r. If everything goes well, your package (and subpackages, if any) will be successfully built2 in an isolated environment and placed in ~/packages (sensible-utils-0.0.14-r0.apk and sensible-utils-doc-0.0.14-r0.apk), however that doesn\u0026rsquo;t mean it is a decent package yet.\nRun apkbuild-lint APKBUILD and abuild sanitycheck to lint your package and catch common errors. Fix the errors, if any.\nRequest feedback if needed If the package is only relevant to you, stop here. git commit, git push, and then you\u0026rsquo;re done. Install the package with doas apk add \u0026lt;pkg\u0026gt;.\nOtherwise, if the package might be potentially useful to other Alpine users, you could consider uploading it to the aports repository.\nBefore you do so, stop for a moment and make an honest judgment whether this is a high quality package and whether you\u0026rsquo;re confident it is clean and polished enough, following the best practices documented in the Wiki. The answer doesn\u0026rsquo;t need to be positive, it\u0026rsquo;s perfectly OK to commit mistakes and everyone is a newbie at some point.\nIf the answer is negative, or if you\u0026rsquo;re new to this process and would like some help, fear no more! There are at least two decent community resources wherein to ask for help:\n#alpine-devel on OFTC IRC Drew DeVault wrote a good post about IRC etiquette.\nalpine-devel mailing list.\nIf you\u0026rsquo;re part of any other community (e.g. Reddit, Discord) feel free to ask therein as well. Avoid posting everywhere though, pick one community, draft your post and then patiently wait.\nPublish your package If all is well, it\u0026rsquo;s time to publish your APKBUILD. Follow the up-to-date steps at https://wiki.alpinelinux.org/wiki/Creating_an_Alpine_package#Code_review. There are basically two options:\nSend a gitlab merge request (MR). This follows the standard git forge workflow (GitHub / BitBucket / GitLab) wherein you fork the main repository, create a branch in your own clone, push it and then initiate a pull request3.\nAlternatively, send an email with your patch to the aports mailing list with git send-email:\n$ git config sendemail.to \u0026#34;alpine-aports@lists.alpinelinux.org\u0026#34; $ git send-email -1 # Implicitly uses --to=alpine-linux@lists.alpinelinux.org as set above Tip: The second approach has a steep learning curve, however once you figure it out it\u0026rsquo;s actually faster, simpler and more streamlined. Whenever a new email is sent to the aports mailing list, a MR is automatically created on GitLab.\nNote: If you adopt the email workflow and need to send a follow-up to your initial patch, do not use --in-reply-to. Instead, create a new email thread. This is needed because as of this post new GitLab MRs are only created when new email threads are created. Replies to existing email threads do not update the MR patch.\nAnd that\u0026rsquo;s all! Other useful tips:\nUse repology to look for preexisting packages in other Linux (or even BSD) distributions, it\u0026rsquo;s very handy as a starting point if you have no idea how to package a given package. In particular, Arch Linux PKGBUILDs are very similar to APKBUILDs. Gentoo EBUILDs and FreeBSD Makefiles are also reasonable approximations. Use abump to bump pkgver in APKBUILD files if the package gets an update to a newer upstream release. Use apkgrel to bump or reset the pkgrel value of your APKBUILD. Use urlwatch to track upstream updates. If you use https://duckduckgo.com/, query for !alpine sensible-utils.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPackage debugging is out of scope of this post.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIn GitLab it\u0026rsquo;s called Merge Request (MR). The list of all aports MRs is here.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/01/alpine-linux-new-apkbuild-workflow/","summary":"\u003cp\u003eThis document describes my workflow to manage \u003ccode\u003eAPKBUILDs\u003c/code\u003e for the\n\u003ca href=\"https://gitlab.alpinelinux.org/alpine/aports\"\u003eaports\u003c/a\u003e repository in \u003ca href=\"https://alpinelinux.org/\"\u003eAlpine Linux\u003c/a\u003e.\u003c/p\u003e","title":"★ Alpine Linux: New APKBUILD Workflow"},{"content":"Recently I needed to figure out what the IP address of my pihole instance was in my Raspberry Pi in my local network.\nFinding the Raspberry Pi nmap nmap to the rescue!\n# nmap -sS 192.168.1.1-255 | tee network.txt | less The relevant snippets to the pihole look like this:\nNmap scan report for pi.hole (192.168.1.XX) Host is up (0.0052s latency). Not shown: 997 closed tcp ports (reset) PORT STATE SERVICE 22/tcp open ssh 53/tcp open domain 80/tcp open http MAC Address: AA:AA:AA:AA:AA:AA (Raspberry Pi Foundation) Nmap scan report for pi.hole (192.168.1.YY) Host is up (0.0059s latency). Not shown: 997 closed tcp ports (reset) PORT STATE SERVICE 22/tcp open ssh 53/tcp open domain 80/tcp open http MAC Address: BB:BB:BB:BB:BB:BB (Raspberry Pi Foundation) There are two IP addresses, one for the ethernet interface (eth0) and the other for the wifi (wlan0). Later on I would disable the wifi interface.\nThe 3 open ports are for services you would expect in a pihole:\nssh (port 22) for remote access / debugging / troubleshooting DNS server (port 53) for the dnsmasq server that pihole uses underneath for adblocking HTTP server (port 80) for the http://pi.hole/admin web management UI ip Another way is to use the ip command. In particular, ip neigh lists the neighbours, one of which should be the pihole.\nTesting the pihole One effective way to test the pihole is to see if analytics.google.com is blocked. There are several ways to do so:\nping should return a local address like 127.0.0.1 or 0.0.0.0. $ ping analytics.google.com PING analytics.google.com (127.0.0.1) 56(84) bytes of data. 64 bytes from localhost (127.0.0.1): icmp_seq=1 ttl=64 time=0.023 ms 64 bytes from localhost (127.0.0.1): icmp_seq=2 ttl=64 time=0.031 ms ^C --- analytics.google.com ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1002ms rtt min/avg/max/mdev = 0.023/0.027/0.031/0.004 ms Ditto for a DNS lookup utility such as dig: $ dig +short analytics.google.com 0.0.0.0 Other ways: drill, host, nslookup, systemd-resolve.\nhttps://d3ward.github.io/toolz/adblock.html seems to be a reasonable website to test whether your adblock blocklists are properly working. Alternatively, just visit any modern and large news corporation website, it will probably be full of ads.\nTroubleshooting the pihole If DNS resolution fails from the pihole itself, run pihole restartdns. Then ping google.com. The ping should work, if it doesn\u0026rsquo;t then there\u0026rsquo;s a bigger problem, out of scope of this post. If the ping works now but stops working later on once you eventually reboot the Pi, consider triggering this command at startup via cron or a systemd timer.\nIf DNS resolution works from the pihole but fails from a neighbouring device, double-check if the device is properly configured: its DNS should be set to the IP address of the pihole. Check these:\n/etc/resolv.conf If the system uses systemd-resolved, run resolvectl. Another possibility is that the pihole might be configured to only answer queries from eth0. Use the http://pi.hole/admin interface to ensure the pihole is configured to answer DNS queries from the local network.\nSetting a static IP in the pihole There are several ways to do so, in order of recommendation:\nStatic DHCP lease from your router. If running a modem, this will likely not work. Prefer running a DHCP server from the pihole.\ndhcpcd: This is typically done as part of the standard pihole setup.\n$ cat /etc/dhcpcd.conf ... # fallback to static profile on eth0 #interface eth0 #fallback static_eth0 interface eth0 static ip_address=192.168.1.XX/24 static routers=192.168.1.1 static domain_name_servers= Note: Restart dhcpcd to apply: systemctl restart dhcpcd.\n/etc/network/interfaces if running Raspberry Pi OS (debian): $ sudoedit /etc/network/interfaces.d/pihole auto lo iface lo inet loopback auto eth0 iface eth0 inet static address 192.168.1.XX netmask 255.255.255.0 gateway 192.168.1.1 Note: Reconfigure debian networking to apply: systemctl restart networking.\nStatic DHCP lease from the pihole itself if it\u0026rsquo;s running a DHCP server. This solution is a bit redundant and should only be applied as last resort. ","permalink":"https://www.perrotta.dev/2022/01/introspect-the-local-network-for-pihole/","summary":"\u003cp\u003eRecently I needed to figure out what the IP address of my \u003ca href=\"https://pi-hole.net/\"\u003epihole\u003c/a\u003e\ninstance was in my \u003ca href=\"https://www.raspberrypi.org/\"\u003eRaspberry Pi\u003c/a\u003e in my local network.\u003c/p\u003e","title":"Introspect the local network for Pihole"},{"content":"Here\u0026rsquo;s a situation that happens often during development:\nSuppose you committed something to git. A few commits later, you realized you forgot to add something to that commit, or possibly missed a link, or even spotted a typo. How do you go about fixing it?\nTeam If you\u0026rsquo;re working on a repository with a team, you should just git commit and git push. Write an eloquent commit message to refer to the previous commit in which you forgot to include your changes.\nSelf Now, if you\u0026rsquo;re working on a standalone repository, just for yourself1, this creates an opportunity to rewrite your history in a cleaner way. The workflow is as follows:\nMake the changes or fixes you had originally forgot to. git add them. Identify the commit id in which you originally wanted to make those changes. git log or tig are simple CLI-oriented ways to do so. Hereafter assume this id is abcdef. Commit your changes while referencing the original commit and then rewrite history: $ git commit --fixup=abcdef # Then pick one of: $ git rebase -i --root $ git rebase -i abcdef~1 # And then save the file as is. Double-check everything went as expected with git log and/or git show and/or tig. If you\u0026rsquo;re happy with the current state of your repository, commit the sin: git push --force. XKCD Courtesy of Randall Munroe\nReferences tig, in case you don\u0026rsquo;t know: tig is an ncurses-based text-mode interface for git. It functions mainly as a Git repository browser, but can also assist in staging changes for commit at chunk level and act as a pager for output from various Git commands.\ngit rebase --root: c.f. Stack Overflow. This is just a lazy way to make the rebase include abcdef. You could do something like git rebase -i HEAD~10 where 10 is an arbitrary guess, but this will only work if abcdef is within the most 10 recent commits. Alternatively git rebase -i abcdef~1 also works. For example: your dotfiles, or your personal blog.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/01/git-oops-i-forgot-to-add-this-thingy/","summary":"\u003cp\u003eHere\u0026rsquo;s a situation that happens often during development:\u003c/p\u003e\n\u003cp\u003eSuppose you committed something to \u003ccode\u003egit\u003c/code\u003e. A few commits later, you realized you\nforgot to add something to that commit, or possibly missed a link, or even\nspotted a typo. How do you go about fixing it?\u003c/p\u003e","title":"Git: Oops I forgot to add this thingy"},{"content":"This document describes my workflow to manage PKGBUILDs for the AUR (Arch User Repository) in Arch Linux.\nDisclaimer First of all, this post is not a substitute to the excellent ArchWiki and it will likely get outdated at some point. In particular, refer to the following articles for up-to-date documentation that will outlive this blog:\nhttps://wiki.archlinux.org/title/Arch_User_Repository https://wiki.archlinux.org/title/Arch_package_guidelines https://wiki.archlinux.org/title/Creating_packages https://wiki.archlinux.org/title/PKGBUILD This article is not a tutorial, as such it assumes you already know what a PKGBUILD is and how to use makepkg. In particular, you should have the base-devel and devtools packages installed in your system.\nStructure I manage my packages with git plus Eli Schwartz\u0026rsquo;s excellent aurpublish. The tree structure is simple, with one PKGBUILD per directory:\n$ git clone https://github.com/thiagowfx/PKGBUILDs \u0026amp;\u0026amp; tree PKGBUILDs PKGBUILDs ├── bkt │ └── PKGBUILD ├── fpp-git │ └── PKGBUILD ├── git-crecord │ └── PKGBUILD ├── i3a │ └── PKGBUILD ├── LICENSE ├── Makefile ├── README.md ├── ttf-camingocode │ └── PKGBUILD └── urlwatch.yml aurpublish is used solely to automate certain interactions with the AUR, more about it later.\nBootstrapping I am going to illustrate with a package I added recently, bkt:\nBefore you even begin, check if the package already exists, do a quick search in the AUR and also in the official repos1.\nStart by copying over the standard PKGBUILD template:\n$ cd PKGBUILDs $ mkdir bkt \u0026amp;\u0026amp; cd bkt $ cp /usr/share/pacman/PKGBUILD.proto PKGBUILD $ $EDITOR PKGBUILD Fill in PKGBUILD metadata like pkgname=, url=, etc. Refer to the ArchWiki for up-to-date best practices.\nThe most important step is to refer to https://wiki.archlinux.org/title/Category:Arch_package_guidelines to figure out the package type.\nbkt is a Rust package. This is my first time packaging for Rust, not a problem though, as I can just refer to https://wiki.archlinux.org/title/Rust_package_guidelines.\nThe rust package guidelines page contains the blueprint for prepare(), check(), build() and package(). Packaging is mostly a matter of gluing everything together. Read the project README.md and the wiki, and then combine the needed steps in the PKGBUILD.\nBy doing so, I produced the following PKGBUILD:\npkgname=bkt pkgver=0.5.0 pkgrel=1 pkgdesc=\u0026#34;A subprocess caching utility\u0026#34; arch=(\u0026#39;x86_64\u0026#39;) url=\u0026#34;https://www.bkt.rs/\u0026#34; license=(\u0026#39;MIT\u0026#39;) makedepends=(\u0026#39;cargo\u0026#39;) source=(\u0026#34;$pkgname-$pkgver.tar.gz::https://github.com/dimo414/$pkgname/archive/refs/tags/$pkgver.tar.gz\u0026#34;) sha256sums=() prepare() { cd \u0026#34;$srcdir/$pkgname-$pkgver\u0026#34; cargo fetch --locked --target \u0026#34;$CARCH-unknown-linux-gnu\u0026#34; } build() { cd \u0026#34;$srcdir/$pkgname-$pkgver\u0026#34; export RUSTUP_TOOLCHAIN=stable export CARGO_TARGET_DIR=target cargo build --frozen --release --all-features } check() { cd \u0026#34;$srcdir/$pkgname-$pkgver\u0026#34; export RUSTUP_TOOLCHAIN=stable cargo test --frozen --all-features } package() { cd \u0026#34;$srcdir/$pkgname-$pkgver\u0026#34; install -Dm0755 -t \u0026#34;$pkgdir/usr/bin/\u0026#34; \u0026#34;target/release/$pkgname\u0026#34; install -Dm644 LICENSE -t \u0026#34;$pkgdir/usr/share/licenses/$pkgname\u0026#34; } Note: $srcdir refers to the src/ directory within bkt. $pkgdir refers to the pkg/ directory within bkt.\nAdjustments Generate the checksums with updpkgsums. It will automatically update the PKGBUILD inplace.\nDownload and extract package files with makepkg -o.\nls src/ and check the directory structure. Update cd in your PKGBUILD to match it. Usually it will be cd $srcdir/$pkgname-$pkgver, but sometimes tiny adjustments are necessary.\nThen run makepkg -s. If everything goes well, your package will be successfully built2 (bkt-0.5.0-1-x86_64.pkg.tar.zst), however that doesn\u0026rsquo;t mean it is a decent package yet.\nRun namcap PKGBUILD and namcap *.pkg.tar.zst to lint your package and catch common errors. Fix the errors, if any.\nTo ensure all dependencies have been correctly declared and none of them are missing, run makepkg within a clean chroot. I like to use Graysky\u0026rsquo;s excellent ccm (Clean Chroot Manager) to do so. Run ccm s (=\u0026ldquo;run makepkg in the clean chroot\u0026rdquo;). If it produces any errors, it likely means you missed some dependencies. Adjust depends=, checkdepends= and makedepends= accordingly.\nRequest feedback if needed If the package is only relevant to you, stop here. git commit, git push, and then you\u0026rsquo;re done. Install the package with makepkg -i.\nOtherwise, if the package might be potentially useful to other Arch users, you could consider uploading it to the AUR.\nBefore you do so, stop for a moment and make an honest judgment whether this is a high quality package and whether you\u0026rsquo;re confident it is clean and polished enough, following the best practices documented in the Wiki. The answer doesn\u0026rsquo;t need to be positive, it\u0026rsquo;s perfectly OK to commit mistakes and everyone is a newbie at some point.\nIf the answer is negative, or if you\u0026rsquo;re new to this process and would like some help, fear no more! There are at least two decent community resources wherein to ask for help:\nAUR Issues, Discussion \u0026amp; PKGBUILD Requests BBS / Forums: Open a new thread, post your PKGBUILD (use [code][/code] tags if you paste it directly!) or a link to it3. Request folks to critique your work, mention that you\u0026rsquo;re looking for feedback. This kind of thread is generally well received in the official forums if you demonstrate you did diligent research before asking for help.\nAUR General Mailing List: Send an email to the mailing list asking for help. In general, follow proper mailing list etiquette, good resources for that are https://useplaintext.email/ and https://man.sr.ht/lists.sr.ht/etiquette.md. TL;DR: Use plain-text instead of HTML in your email.\nIf you\u0026rsquo;re part of any other community (e.g. Reddit, Discord) feel free to ask therein as well. Avoid posting everywhere though, pick one community, draft your post and then patiently wait.\nPublish your package If all is well, it\u0026rsquo;s time to publish your PKGBUILD to the AUR. Follow the up-to-date steps at https://wiki.archlinux.org/title/Arch_User_Repository#Submitting_packages.\nTL;DR: If you don\u0026rsquo;t use aurpublish, do:\n$ makepkg --printsrcinfo \u0026gt; .SRCINFO Then you\u0026rsquo;ll need both the PKGBUILD and the .SRCINFO file, it\u0026rsquo;s basically a matter of committing your changes and pushing them to the right repository.\nIf you do use aurpublish this process is much easier, it\u0026rsquo;s mostly a matter of doing git commit, git push and aurpublish bkt. Aurpublish automatically generates the .SRCINFO and a commit message by the means of git hooks.\nAnd that\u0026rsquo;s all! Other useful tips:\nUse repology to look for preexisting packages in other Linux (or even BSD) distributions, it\u0026rsquo;s very handy as a starting point if you have no idea how to package a given package. In particular, Alpine Linux APKBUILDs are very similar to PKGBUILDs. Gentoo EBUILDs and FreeBSD Makefiles are also reasonable approximations. Use makepkg -src to clean up after building a package. Bonus: Track upstream Use a software like urlwatch or nvchecker to track future upstream changes so that you can update your packages in a timely fashion4. There\u0026rsquo;s also a web service called Release Monitoring, part of Fedora Infra. I use urlwatch the following way:\n$ cat PKGBUILDs/urlwatch.yml # urls for urlwatch(1) --- name: \u0026#34;bkt\u0026#34; command: \u0026#34;git ls-remote --tags https://github.com/dimo414/bkt\u0026#34; --- name: \u0026#34;fpp\u0026#34; command: \u0026#34;git ls-remote --tags https://github.com/facebook/PathPicker\u0026#34; --- name: \u0026#34;git-crecord\u0026#34; command: \u0026#34;git ls-remote --tags https://github.com/andrewshadura/git-crecord\u0026#34; --- name: \u0026#34;i3a\u0026#34; command: \u0026#34;git ls-remote --tags https://git.goral.net.pl/mgoral/i3a\u0026#34; # --- # name: \u0026#34;ttf-camingocode\u0026#34; # N/A # Run this command periodically via cron or systemd timer. # Set up notifications e.g. via sendmail. $ urlwatch --urls urlwatch.yml If you use https://duckduckgo.com/, query for !aur bkt and !archpkg bkt. Handy!\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPackage debugging is out of scope of this post.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFor example, use https://gist.github.com or http://paste.opensuse.org/ or http://ix.io/.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIn 99% of the cases this is just a matter of bumping the pkgver= and updating the checksums. If pkgver= is the same but there\u0026rsquo;s a fix to the package itself, then bump pkgrel= instead.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/01/arch-linux-new-pkgbuild-workflow/","summary":"\u003cp\u003eThis document describes my workflow to manage \u003ccode\u003ePKGBUILDs\u003c/code\u003e for the \u003ca href=\"https://aur.archlinux.org/\"\u003eAUR\u003c/a\u003e (Arch User Repository) in \u003ca href=\"https://www.archlinux.org/\"\u003eArch Linux\u003c/a\u003e.\u003c/p\u003e","title":"★ Arch Linux: New PKGBUILD Workflow"},{"content":"As packages are upgraded over time, updated configs files under /etc may arise. Different package managers treat this issue differently.\nAlpine Linux apk creates .apk-new files, which can be located and merged by running doas update-conf. Here is a recent example:\n% doas update-conf --- //etc/securetty +++ //etc/securetty.apk-new @@ -12,3 +12,6 @@ tty11 hvc0 ttyS0 +ttyS1 +ttyAMA0 +ttyAMA1 New //etc/securetty available: Quit, Next, Show diff, Edit new, Zap new, Use new (q/n/s/e/z/u) [s] u Arch Linux pacman creates .pacnew files, which can be located and merged by running sudo pacdiff. Here is a recent example:\n% sudo pacdiff ==\u0026gt; pacnew file found for /etc/sudoers :: (V)iew, (S)kip, (R)emove pacnew, (O)verwrite with pacnew, (Q)uit: [v/s/r/o/q] Tip: The pacdiff-pacman-hook-git package helpfully adds a pacman hook that automatically checks whether there are any due .pacnew files upon upgrading the system (pacman -Syu), being a simple and effective way to automate this maintenance task. It looks like this:\n: Running post-transaction hooks... (1/5) Reloading system manager configuration... (2/5) Creating temporary files... (3/5) Arming ConditionNeedsUpdate... (4/5) Registering Haskell modules... (5/5) Reviewing .pacnew files... /etc/sudoers.pacnew ⟶ /etc/sudoers ──────────────────────────────────────────────────────────────────────────────── ────┐ 76: │ ────┘ ## ## User privilege specification ## -root ALL=(ALL:ALL) ALL +root ALL=(ALL) ALL ## Uncomment to allow members of group wheel to execute any command -# %wheel ALL=(ALL:ALL) ALL +# %wheel ALL=(ALL) ALL ## Same thing without a password -# %wheel ALL=(ALL:ALL) NOPASSWD: ALL +# %wheel ALL=(ALL) NOPASSWD: ALL ## Uncomment to allow members of group sudo to execute any command -# %sudo ALL=(ALL:ALL) ALL +# %sudo ALL=(ALL) ALL ## Uncomment to allow any user to run sudo if they know the password ## of the user they are running the command as (root by default). # Defaults targetpw # Ask for the password of the target user -# ALL ALL=(ALL:ALL) ALL # WARNING: only use this together with \u0026#39;Defaults targetpw\u0026#39; +# ALL ALL=(ALL) ALL # WARNING: only use this together with \u0026#39;Defaults targetpw\u0026#39; ## Read drop-in files from /etc/sudoers.d @includedir /etc/sudoers.d :: Searching databases for updates... :: Searching AUR for updates... there is nothing to do ","permalink":"https://www.perrotta.dev/2022/01/alpine-/-arch-linux-.apk-new-and-.pacnew-files/","summary":"\u003cp\u003eAs packages are upgraded over time, updated configs files under \u003ccode\u003e/etc\u003c/code\u003e may\narise. Different package managers treat this issue differently.\u003c/p\u003e","title":"Alpine / Arch Linux: .apk-new and .pacnew files"},{"content":"Here\u0026rsquo;s how we can enable automatic (unattended) package upgrades in Debian.\nHowto Install the unattended-upgrades package with apt(8):\n% apt install unattended-upgrades The service is then enabled and started automatically:\n$ systemctl status unattended-upgrades ● unattended-upgrades.service - Unattended Upgrades Shutdown Loaded: loaded (/lib/systemd/system/unattended-upgrades.service; enabled; vendor preset: enabled) Active: active (running) since Sun 2022-01-16 02:05:42 EST; 35s ago Docs: man:unattended-upgrade(8) Main PID: 22442 (unattended-upgr) Tasks: 2 (limit: 1597) CPU: 516ms CGroup: /system.slice/unattended-upgrades.serviceGk └─22442 /usr/bin/python3 /usr/share/unattended-upgrades/unattended-upgrade-shutdown --wait-for-signal By default, only security updates are enabled. We can enable all updates by uncommenting the applicable lines:\n$ sudoedit /etc/apt/apt.conf.d/50unattended-upgrades ... Unattended-Upgrade::Origins-Pattern { // Codename based matching: // This will follow the migration of a release through different // archives (e.g. from testing to stable and later oldstable). // Software will be the latest available for the named release, // but the Debian release itself will not be automatically upgraded. // \u0026#34;origin=Debian,codename=${distro_codename}-updates\u0026#34;; // \u0026#34;origin=Debian,codename=${distro_codename}-proposed-updates\u0026#34;; // \u0026#34;origin=Debian,codename=${distro_codename},label=Debian\u0026#34;; \u0026#34;origin=Debian,codename=${distro_codename},label=Debian-Security\u0026#34;; \u0026#34;origin=Debian,codename=${distro_codename}-security,label=Debian-Security\u0026#34;; ... For debugging, one should run:\n$ sudo unattended-upgrade -d We could go beyond and add logging by the means of etckeeper, just like how we did for Alpine Linux\u0026rsquo;s apk\n% apt install etckeeper Reading package lists... Done Building dependency tree... Done Reading state information... Done The following NEW packages will be installed: etckeeper 0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded. Need to get 54.4 kB of archives. After this operation, 180 kB of additional disk space will be used. Get:1 http://raspbian.freemirror.org/raspbian bullseye/main armhf etckeeper all 1.18.16-1 [54.4 kB] Fetched 54.4 kB in 1s (84.3 kB/s) Preconfiguring packages ... Selecting previously unselected package etckeeper. (Reading database ... 44403 files and directories currently installed.) Preparing to unpack .../etckeeper_1.18.16-1_all.deb ... Unpacking etckeeper (1.18.16-1) ... Setting up etckeeper (1.18.16-1) ... Created symlink /etc/systemd/system/multi-user.target.wants/etckeeper.timer → /lib/systemd/system/etckeeper.timer. etckeeper.service is a disabled or a static unit, not starting it. ... etckeeper is enabled and works out-of-the-box as well:\nsystemctl status etckeeper.timer ● etckeeper.timer - Daily autocommit of changes in /etc directory Loaded: loaded (/lib/systemd/system/etckeeper.timer; enabled; vendor preset: enabled) Active: active (waiting) since Sun 2022-01-16 02:28:44 EST; 2min 36s ago Trigger: Mon 2022-01-17 02:28:44 EST; 23h left Triggers: ● etckeeper.service Docs: man:etckeeper(8) Here\u0026rsquo;s what a typical log looks like:\n$ (cd /etc/etckeeper \u0026amp;\u0026amp; sudo git log) commit 8f9f5e31d9abb833cf645825c1cbda15336818b7 (HEAD -\u0026gt; master) Author: root \u0026lt;root@raspberry\u0026gt; Date: Sun Jan 16 06:25:28 2022 -0500 daily autocommit commit 5a6478711a1a1198535d5062ca309afb5c99c0eb Author: root \u0026lt;root@raspberry\u0026gt; Date: Sun Jan 16 02:29:01 2022 -0500 Initial commit References https://wiki.debian.org/UnattendedUpgrades ","permalink":"https://www.perrotta.dev/2022/01/debian-enable-unattended-upgrades/","summary":"\u003cp\u003eHere\u0026rsquo;s how we can enable automatic (unattended) package upgrades in Debian.\u003c/p\u003e","title":"Debian: Enable unattended upgrades"},{"content":"Use case: Given an Alpine Linux diskless1 installation meant for a Raspberry Pi setup, we would like to add a persistent storage component to it to make it survive across reboots.\nGoal The Alpine Linux Wiki covers most of the installation process, hence I will only document the bits that were lacking and/or confusing therein.\nMy use case is the following:\nGiven a Raspberry Pi 3B with an old 4GiB SD Card as CF storage2, install Alpine Linux in diskless mode. Find a way to preserve modifications in /etc and /var, as well as any installed packages through its apk package manager.\nLet\u0026rsquo;s follow the steps outlined in the wiki.\nCopy Alpine to the SD Card Grab the SD card and install Alpine Linux in it.\nAlpine provides officially supported images designed for the Raspberry Pi.\nMost Linux distributions provide an .iso or .img file to be installed with a tool like Balena Etcher, Rufus, Raspberry Pi Imager or plain dd3.\nAlpine is not like most Linux distributions: Instead, it provides a .tar.gz archive with files that should be copied directly to the SD card. Grab the latest version (3.15 at the time of this post) from https://alpinelinux.org/downloads/. There are 3 options:\narmhf: Works with all Pis, but may perform less optimally on recent versions.\narmv7: Works with the Pi 3B, 32-bit.\naarch64: Works with the Pi 3B, 64-bit.\nI opted for aarch64 to make it 64-bit, but armv7 would also have worked well for my setup. In fact, Raspberry Pi OS (Debian) uses armv7 (32-bit) at the time of this writing.\nBefore copying files over, format the SD Card. As I was doing this from a Windows machine because it was the only one I had readily available with a SD card slot, I just used the native Windows Disk Management tool to do so. I decided to allocate a 100MB4 FAT32 partition. The rest of the SD card would be blank for now. Alpine is surprisingly small, 100MB was more than enough for the kernel and other needed files.\nOnce the SD card is formatted, copy the files over to it. It turns out Windows cannot extract tarballs (.tar.gz); a tool like 7-zip should do the job. Copy the files over to the root of the newly allocated FAT32 partition, and then safely eject the SD card.\nBoot Alpine from the SD Card The next step is to insert the SD Card into the Pi and then boot. I had some trouble in this step and eventually figured out I didn\u0026rsquo;t mark the primary FAT32 partition as bootable. Unfortunately it\u0026rsquo;s not straightforward to mark the partition as bootable from Windows. On a Linux machine there\u0026rsquo;s a wide array of tools to do so: fdisk, cfdisk (TUI), sfdisk (scriptable fdisk), parted, gparted (GUI) are some of them. I worked around that by installing Raspberry Pi OS on the SD card with the Raspberry Pi imager, and then overwriting it with the Alpine files. This works because the Raspberry PI OS installation marks the FAT32 partition as bootable.\nInstall Alpine Installing Alpine is well documented in the wiki thus it won\u0026rsquo;t be covered here. It basically comes down to invoking setup-alpine, which then invokes other setup-* scripts.\nKeep in mind we\u0026rsquo;re not really \u0026ldquo;installing\u0026rdquo; Alpine as this is a diskless installation. A more accurate term here would be \u0026ldquo;configuring\u0026rdquo;.\nBefore invoking the installation script, I created a second primary partition in the SD card, set to ext4:\n# Configure networking to get working internet access. % setup-interfaces # Install some partitioning tools. % apk add cfdisk e2fsprogs # Create a second partition (mmcblk0p2) and write it. % cfdisk /dev/mmcblk0 # Format the partition as ext4. % mkfs.ext4 /dev/mmcblk0p2 # Mount the partition under /media. % mount /dev/mmcblk0p2 /media/mmcblk0p2 The installation is straightforward, we just need to pay attention to a few select steps:\nsetup-disk: Select none to ensure a diskless installation5. setup-apkcache: Select /media/mmcblk0p2/cache to persist downloaded apk packages. setup-lbu: Edit /etc/lbu/lbu.conf and set LBU_MEDIA=\u0026quot;mmcblk0p2\u0026quot;. Note: Do not add /media as it is implicit. Once the installation is complete, run lbu commit to persist the changes in the second partition. Once you do so, a \u0026lt;hostname\u0026gt;.apkovl.tar.gz6 file should materialize on /media/mmcblk0p2/.\nThis is a good moment to reboot. Before we do so, let\u0026rsquo;s cache the packages we had previously downloaded.\n# Cache packages. % apk cache download % reboot After the first reboot If everything worked as expected, once you reboot all your previously installed packages should have been preserved and automatically restored / reinstalled, as well as your modifications done to /etc.\nFrom this point on, whenever you install a new package that you want to be preserved for subsequent reboots, run lbu commit afterwards. For example:\n% apk add vim % lbu commit If you would like to see what is going to be committed, run lbu status or lbu diff before doing the actual commit. Whenever you commit, /media/mmcblk0p2/\u0026lt;hostname\u0026gt;.apkovl.tar.gz gets overwritten with your most recent modifications.\nIt\u0026rsquo;s possible to keep more than one backup file by changing BACKUP_LIMIT= in /etc/lbu/lbu.conf. This is specially handy if you decide to revert to an earlier system snapshot / state later on. The stock config looks like this:\n% cat /etc/lbu/lbu.conf # what cipher to use with -e option DEFAULT_CIPHER=aes-256-cbc # Uncomment the row below to encrypt config by default # ENCRYPTION=$DEFAULT_CIPHER # Uncomment below to avoid \u0026lt;media\u0026gt; option to \u0026#39;lbu commit\u0026#39; # Can also be set to \u0026#39;floppy\u0026#39; # LBU_MEDIA=usb # Set the LBU_BACKUPDIR variable in case you prefer to save the apkovls # in a normal directory instead of mounting an external media. # LBU_BACKUPDIR=/root/config-backups # Uncomment below to let lbu make up to 3 backups # BACKUP_LIMIT=3 Tip: You can find the list of all explicitly installed packages in /etc/apk/world.\nThe last piece: make /var persistent There are three natural ways that come to mind to make /var persistent:\nA) Separate partition (or file) Instead of two partitions (FAT32 and ext4), create 3 partitions: FAT32, ext4 and ext4. Use the latter one to mount /var on, saving this information in /etc/fstab. The main disadvantage of this setup is that you\u0026rsquo;ll need to allocate a fixed amount of space of each of the ext4 partitions and it may be difficult to figure out how to split the space between them.\nA variant of this approach is to just create the third partition as a file:\n# 500MB file % dd if=/dev/zero of=/media/mmcblk0p2/var.img bs=1M count=500 status=progress % mkfs.ext4 /media/mmcblk0p2/var.img % mount /media/mmcblk0p2/var.img /var This works because the Linux kernel supports mounting files as if they were device blocks, treating them as loop devices (pseudo-devices).\nI don\u0026rsquo;t like these approaches because they shadow the preexisting /var from the boot media, which in turn messes up with existing services that use it such as cron: % crontab -l would fail. One workaround would be to mount a /var subdirectory instead: for example, /var/lib/docker for docker.\nB) Bind mount This one is straightforward:\n% mount --bind /media/mmcblk0p2/var/lib/docker /var/lib/docker The actual partition lives in the SD card, however we make a bind mount under /var, which is like an alias. From Stack Exchange:\nA bind mount is an alternate view of a directory tree. Classically, mounting creates a view of a storage device as a directory tree. A bind mount instead takes an existing directory tree and replicates it under a different point. The directories and files in the bind mount are the same as the original. Any modification on one side is immediately reflected on the other side, since the two views show the same data.\nC) Overlay mount From ArchWiki:\nOverlayfs allows one, usually read-write, directory tree to be overlaid onto another, read-only directory tree. All modifications go to the upper, writable layer. This type of mechanism is most often used for live CDs but there is a wide variety of other uses.\nIt\u0026rsquo;s perfect for our use case, which uses a live bootable SD card for Alpine. It blends the preexisting, ephemeral, in-memory /var with the persistent in-disk /var.\nI wanted to mount /var directly but found it to be problematic for the same reasons mentioned earlier, therefore I just went with /var/lib/docker instead:\n# Create overlay upper and work directories. % mkdir -p /media/mmcblk0p2/var/lib/docker /media/mmcblk0p2/var/lib/docker-work # Add mountpoint entry to fstab. Note: The work dir must be an empty directory in the same filesystem mount as the upper directory. % echo \u0026#34;overlay /var/lib/docker overlay lowerdir=/var/lib/docker,upperdir=/media/mmcblk0p2/var/lib/docker,workdir=/media/mmcblk0p2/var/lib/docker-work 0 0\u0026#34; \u0026gt;\u0026gt; /etc/fstab # Mount all fstab entries, including our newly added one. % mount -a Conclusion I opted for the third approach, using an overlay mount, it was the most seamless one. A bind mount would have been fine as well.\nThe final setup works surprisingly well:\nAlpine Linux is very lightweight and runs mostly from RAM apk cache is persistent to the ext4 partition /var/ is persistent to the ext4 partition lbu commit persists changes in /etc/ and /home/ in the ext4 partition Every reboot fully resets the system sans persistent components above References https://vincentserpoul.github.io/post/alpine-linux-rpi0/ http://dahl-jacobsen.dk/tips/blog/2021-04-08-docker-on-alpine-linux/ http://dahl-jacobsen.dk/tips/blog/2018-03-15-alpine-on-raspberry-pi/ Running (almost) fully from RAM.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCF = Compact disk.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOn Linux I\u0026rsquo;d usually opt for dd, on Windows the Raspberry Pi Imager is a sensible choice.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n100MB is overly conservative, but keep in mind I had a very small SD Card, with only 4GiB storage. 250MB or even 500MB should be a more sensible default if you have a bigger SD Card (e.g. 32GiB).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAn alternative is to select data disk mode, but it didn\u0026rsquo;t work for me.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\novl is short for overlay. Not to be confused with vol for volume.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/01/alpine-linux-on-raspberry-pi-diskless-mode-with-persistent-storage/","summary":"\u003cp\u003eUse case: Given an Alpine Linux \u003cstrong\u003ediskless\u003c/strong\u003e\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e installation meant for\na Raspberry Pi setup, we would like to add a persistent storage component to it\nto make it survive across reboots.\u003c/p\u003e","title":"★ Alpine Linux on Raspberry Pi: Diskless Mode with persistent storage"},{"content":"It is possible to track/follow commits of git repositories on GitHub via RSS: https://github.com/\u0026lt;user\u0026gt;/\u0026lt;repo\u0026gt;/commits.atom.\nFor example, my dotfiles: https://github.com/thiagowfx/dotfiles/commits.atom\nI am using this to keep track of Miniflux commits from within Miniflux itself (a RSS webapp).\n","permalink":"https://www.perrotta.dev/2022/01/rss-follow-commit-updates-from-github/","summary":"\u003cp\u003eIt is possible to track/follow commits of \u003ccode\u003egit\u003c/code\u003e repositories\non \u003ca href=\"https://github.com/\"\u003eGitHub\u003c/a\u003e via RSS: \u003ccode\u003ehttps://github.com/\u0026lt;user\u0026gt;/\u0026lt;repo\u0026gt;/commits.atom\u003c/code\u003e.\u003c/p\u003e","title":"RSS: Follow commit updates from Github"},{"content":"This is a follow-up post of Keychron K2: Linux Setup. The previous post focused on the configuration of the keyboard, this one focuses on its user experience from the point of view of a Software Engineer.\nPreviously\u0026hellip; My first (and previous) mechanical keyboard was a Logitech G610 Orion Red1. It\u0026rsquo;s a full wired keyboard with Cherry MX Red switches (linear) and dedicated multimedia keys. In my opinion it is a solid choice for beginners because its interface is quite familiar thanks to the wide range of available keys. It is branded as a gaming keyboard but frankly it was a fine office keyboard as well. However after using it for a while I wanted to upgrade.\nThe natural upgrade path would contain one or more of the following features, in order of importance:\ngood support for both Linux and macOS, especially Linux compact: tenkeyless a.k.a. 80%, or 75%2 portable: wireless, either with a dongle or with bluetooth or both with decent battery life: should outlast at least a week of office work not overpriced: ~$200 CAD budget playful: with RGB backlight (instead of white LED) Given those, a natural upgrade path would have been the Logitech G915 TKL. Its main caveat is that it is relatively overpriced, ~$300 CAD. Above that $300 CAD point one should arguably be looking for ergonomic and/or fully programmable (QMK firmware) keyboards, like the Kinesis Advantage and/or the Ergodox EZ3. Even though the G915 TKL is rock solid, it didn\u0026rsquo;t have any fancy features to justify that investment.\nThe quest for the perfect keyboard\u0026hellip; I then proceeded to outsource my luck to the wisdom of the crowds, by asking for recommendations in a mailing list at $DAYJOB, giving them a subset of the requirements above.\nThe choice was then obvious: I\u0026rsquo;d get a Keychron. It fit all of my requirements perfectly. The issue was that Keychron had so many choices to pick from.\nXKCD Courtesy of Randall Munroe\nAfter some deliberation I had two options in mind: Keychron K2 and Keychron K1 TKL. The main difference between them is that the K1 is a low-profile keyboard. I didn\u0026rsquo;t know what low-profile meant at the time and had to do some research to figure it out4.\nIn the end I opted for the Keychron K2, red switches (linear), with RGB backlight.\nThe keyboard The keyboard met all my expectations, even surpassing them, I am quite satisfied overall:\nLinux support Great out-of-the-box support, it just works. Even though I tweaked a few configs, it wasn\u0026rsquo;t strictly necessary. In particular, there\u0026rsquo;s a physical toggle where you can choose between macOS (=Linux) or Windows mode. Furthermore they provide both macOS-style (command, option, etc) and Windows-style keycaps (super, alt, etc). For Linux I tend to stick with the Windows ones. Compact A 75% keyboard is compact by definition, what else could I add? I wouldn\u0026rsquo;t go lower than that though, in my opinion removing the function keys goes too far and makes the keyboard harder to use. A Tenkeyless / 80% option would also be compact enough while maybe increasing comfort a little bit, but I managed to adapt quickly to the 75% layout. Multimedia and OS keys are easily available by the means of Fn + F1, etc. Portable It has bluetooth, but can also be used while plugged in. There\u0026rsquo;s a toggle that controls which mode (wireless or wired) to use. The bluetooth has 3 channels and it\u0026rsquo;s very easy to switch between them: Fn + 1, Fn + 2, Fn + 3. This makes it easy to switch between laptops and/or workstation, work and/or personal. The cable connector is USB-C which in my opinion is a must these days (2020s). Battery life Battery lasts more than enough, to the point that I don\u0026rsquo;t even need to care about it. I tend to recharge it every 2 weeks or so. Fn + b will let me have a visual indication of how much juice is still left. The keyboard automatically sleeps after 10 minutes of inactivity in order to save battery, which I think is a nice bonus, I don\u0026rsquo;t need to worry about turning it off. This can be disabled if it ends up being annoying, though. Great value for money $90 USD at the time of this writing. Because I didn\u0026rsquo;t want to deal with international shipping, I ended up simply buying it from one of their official local retailers in Canada, OneOfZero. This slightly increased what I paid for it (~$150 CAD with taxes), on the other hand the shipping was really fast. Just beware, this particular retailer does not have a friendly return policy, if I recall correctly they charge a 25% fee and end up throwing the keyboard away (landfill), which is very depressing. Playful The RGB lighting is fluff and completely irrelevant in terms of productivity, however it adds a playful touch to the keyboard. I would say that white lighting is enough, but sometimes it\u0026rsquo;s just cool to change to different color(s). What can I say, we humans are visual creatures. You can easily adjust the light brightness and toggle it on/off (Fn + light), plus there are several patterns to choose from. I tend to use a still pattern because it isn\u0026rsquo;t distracting for programming or other type of work that requires focus. Finally: The keyboard keycaps are quite sturdy and stick well in place. I had some issues with my previous keyboard where some of its keycaps would easily fall off it when moving it within my backpack. I do not have this issue with the Keychron.\nFuture Mechanical keyboards are meant to last. I do not intend to upgrade it any time soon. However, if/when I ever do it, I will be looking for the following features:\nQMK firmware / programmable: would unlock more workflow possibilities. The Keychron Q1 would be a good candidate for this. With a dongle, in addition to bluetooth. Because sometimes bluetooth is just annoying and/or unreliable. The Logitech G915 TKL has a dongle. Other switches? So far I\u0026rsquo;ve only used red ones (linear). More silent switches could be useful. Ergonomic: Whether it\u0026rsquo;s a split, an ortholinear or just a curved keyboard, I figure that at some point it will be a good investment for my wrists. Adaptation is difficult but it may be necessary one day. I am not particularly attracted to custom keycaps, they are cute but not my cup of tea. And I also do not see the appeal of hot swappable keycaps. I can understand why some folks appreciate those features, customizability is powerful, but for me it\u0026rsquo;s less stressful to keep things simple.\nLinked to the .pdf because apparently the SKU isn\u0026rsquo;t listed in the Logitech product website anymore. At the time, it cost ~$120 CAD.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis requirement indirectly excluded all those gaming keyboards with dedicated macro and/or multimedia keys, if they ended up increasing the overall keyboard surface area.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIn the past, I had the opportunity to borrow these from coworkers for a week but couldn\u0026rsquo;t quite adapt to them, their learning curve is quite steep. Maybe I\u0026rsquo;ll try that again in the future.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIt basically means the keys are thinner than usual, comparable to laptop keyboard keys.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/01/keychron-k2-review/","summary":"\u003cp\u003eThis is a follow-up post of \u003ca href=\"https://www.perrotta.dev/2021/12/keychron-k2-linux-setup/\"\u003eKeychron K2: Linux Setup\u003c/a\u003e. The previous post focused on the configuration of the keyboard, this one focuses on its user experience from the point of view of a Software Engineer.\u003c/p\u003e","title":"★ Keychron K2: Review"},{"content":"apk(8) is the Alpine Linux package manager. Surprisingly, it lacks native logs. In this post we will learn how to work around this limitation.\nIn a distribution like Arch Linux that uses pacman(8), one would typically find logs in /var/log/pacman.log. You would expect Alpine Linux to follow suit and provide some /var/log/apk.log or similar, however that\u0026rsquo;s not the case.\nLogs are nowhere to be found, even in the apk-* man pages. I double-checked by asking on the #alpine-linux IRC and someone confirmed this is indeed the case, and there\u0026rsquo;s an (unconfirmed) possibility the next generation of apk may add logging support.\nMeanwhile, we will use etckeeper(8) to overcome this limitation.\netckeeper: set-up etckeeper is\na collection of tools to let /etc be stored in a git, mercurial, bazaar or darcs repository. This lets you use git to review or revert changes that were made to /etc. Or even push the repository elsewhere for backups or cherry-picking configuration changes.\nby Joey Hess.\nIt is available in the Alpine Linux repositories, just install it:\n% apk add etckeeper No configuration is needed, it works out-of-the-box, thanks to a post-install hook to initialize the git repository, and an apk commit hook to update it upon apk package operations.\netckeeper: viewing logs Just run git log as root. Root privilege is necessary because the git repository is initialized under /etc/etckeeper. Pick your poison:\n$ GIT_DIR=/etc/etckeeper doas git log $ doas git -C /etc/etckeeper log $ (cd /etc/etckeeper \u0026amp;\u0026amp; doas git log) Here\u0026rsquo;s what a typical log looks like, courtesy of apk-autoupdate(1)1:\ncommit f06255c4be4657481082406b2050ecd88e3da768 Author: root \u0026lt;root@localhost.localdomain\u0026gt; Date: Tue Jan 11 00:00:24 2022 -0500 committing changes in /etc after apk run Package changes: -libeconf-0.4.2-r0 -libeconf-doc-0.4.2-r0 +libeconf-0.4.4-r0 +libeconf-doc-0.4.4-r0 -mtools-4.0.36-r0 -mtools-doc-4.0.36-r0 +mtools-4.0.37-r0 +mtools-doc-4.0.37-r0 -perl-io-socket-ssl-2.073-r0 -perl-io-socket-ssl-doc-2.073-r0 +perl-io-socket-ssl-2.074-r0 +perl-io-socket-ssl-doc-2.074-r0 -py3-idna-3.3-r1 +py3-idna-3.3-r2 -py3-jinja2-3.0.1-r1 -py3-jinja2-doc-3.0.1-r1 +py3-jinja2-3.0.3-r0 +py3-jinja2-doc-3.0.3-r0 Which merely does apk upgrade, automatically. More details here.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/01/alpine-linux-apk-logs-with-etckeeper/","summary":"\u003cp\u003e\u003ccode\u003eapk(8)\u003c/code\u003e is the Alpine Linux package manager. Surprisingly, it lacks native\nlogs. In this post we will learn how to work around this limitation.\u003c/p\u003e","title":"Alpine Linux: apk logs with etckeeper"},{"content":"In the same spirit of my first PKGBUILD and Ebuild, herein I will describe my first APKBUILD.\nAt a glance Alpine Linux package management is very similar to Arch Linux, with tiny differences:\nPKGBUILD → APKBUILD: The filename is obviously different. Their format are very similar though, both of them are bash scripts with variables and functions. In particular, there\u0026rsquo;s check, patch, build and package. cp /usr/share/pacman/PKGBUILD.proto → newapkbuild: Template versus scaffolding. pacman → apk: The package manager is different. makepkg -s → abuild -r: makepkg drives all things package building for pacman. abuild drives package building for apk. makepkg -i → apk add \u0026lt;pkg\u0026gt;: makepkg can also drive package installations whereas abuild cannot, apk must be used. namcap → apkbuild-lint (from atools) + abuild sanitycheck1: Linters are different. updpkgsums → abuild checksum: Generate hashes for package sources. XKCD Courtesy of Randall Munroe\nOther than that, the process of writing an APKBUILD is very similar to writing a PKGBUILD. In fact, the Arch repositories (especially the AUR) tend to be much more comprehensive than Alpine\u0026rsquo;s in terms of number of packages, so chances are if you want to write a new package for Alpine, check in Arch\u0026rsquo;s repos first, it\u0026rsquo;s a good starting point.\nMy first package: fpp fpp stands for \u0026lsquo;Facebook Path Picker\u0026rsquo;.\nAs of the time of this post, I maintain fpp-git in the AUR. It looks like this:\npkgname=fpp-git pkgver=0.9.2.r130.ge0d5cfc pkgrel=1 pkgdesc=\u0026#39;TUI that lets you pick paths out of its stdin and run arbitrary commands on them\u0026#39; url=\u0026#39;https://facebook.github.io/PathPicker\u0026#39; license=(\u0026#39;MIT\u0026#39;) source=(\u0026#34;${pkgname%-git}::git+https://github.com/facebook/PathPicker.git\u0026#34;) sha256sums=(\u0026#39;SKIP\u0026#39;) arch=(\u0026#39;any\u0026#39;) makedepends=(\u0026#39;git\u0026#39;) depends=(\u0026#39;python\u0026#39;) conflicts=(\u0026#34;${pkgname%-git}\u0026#34;) provides=(\u0026#34;${pkgname%-git}\u0026#34;) prepare() { cd \u0026#34;$srcdir/${pkgname%-git}\u0026#34; rm -r \u0026#34;src/tests\u0026#34; } pkgver() { cd \u0026#34;$srcdir/${pkgname%-git}\u0026#34; git describe --long --tags | sed \u0026#39;s/\\([^-]*-g\\)/r\\1/;s/-/./g\u0026#39; } package() { cd \u0026#34;$srcdir/${pkgname%-git}\u0026#34; # library install -Dm755 \u0026#34;fpp\u0026#34; -t \u0026#34;$pkgdir/usr/share/fpp\u0026#34; cp -a src \u0026#34;$pkgdir/usr/share/fpp\u0026#34; # entrypoint install -dm755 \u0026#34;$pkgdir/usr/bin\u0026#34; ln -s \u0026#34;/usr/share/fpp/fpp\u0026#34; \u0026#34;$pkgdir/usr/bin\u0026#34; # documentation install -Dm644 LICENSE -t \u0026#34;$pkgdir/usr/share/licenses/$pkgname\u0026#34; install -Dm644 \u0026#34;debian/usr/share/man/man1/fpp.1\u0026#34; -t \u0026#34;$pkgdir/usr/share/man/man1\u0026#34; } PKGBUILD guidelines and instructions:\nhttps://wiki.archlinux.org/title/PKGBUILD https://wiki.archlinux.org/title/Arch_package_guidelines The equivalent APKBUILD I wrote looks like this:\npkgname=fpp pkgver=0.9.2 pkgrel=0 pkgdesc=\u0026#34;TUI that lets you pick paths out of its stdin and run arbitrary commands on them\u0026#34; url=\u0026#34;https://facebook.github.io/PathPicker\u0026#34; arch=\u0026#34;noarch\u0026#34; license=\u0026#34;MIT\u0026#34; depends=\u0026#34;bash python3\u0026#34; subpackages=\u0026#34;$pkgname-doc\u0026#34; source=\u0026#34;$pkgname-$pkgver.tar.gz::https://github.com/facebook/PathPicker/archive/$pkgver.tar.gz\u0026#34; builddir=\u0026#34;$srcdir/PathPicker-$pkgver\u0026#34; check() { fpp --version } prepare() { default_prepare rm -r \u0026#34;src/__tests__\u0026#34; } package() { # library install -Dm755 \u0026#34;fpp\u0026#34; -t \u0026#34;$pkgdir/usr/share/fpp\u0026#34; cp -a src \u0026#34;$pkgdir/usr/share/fpp\u0026#34; # entrypoint install -dm755 \u0026#34;$pkgdir/usr/bin\u0026#34; ln -s \u0026#34;/usr/share/fpp/fpp\u0026#34; \u0026#34;$pkgdir/usr/bin\u0026#34; # documentation install -Dm644 LICENSE -t \u0026#34;$pkgdir/usr/share/licenses/$pkgname\u0026#34; install -Dm644 \u0026#34;debian/usr/share/man/man1/fpp.1\u0026#34; -t \u0026#34;$pkgdir/usr/share/man/man1\u0026#34; } sha512sums=\u0026#34; 65b6b077f437bd642ebf94c55be901aabc73f7b9c89e4522c4f51970c4d63d744ad8fa29cac06816851f63bcb81d0480e61d405231c582e9aca0f4e650949a97 fpp-0.9.2.tar.gz \u0026#34; APKBUILD guidelines and instructions:\nhttps://wiki.alpinelinux.org/wiki/Creating_an_Alpine_package https://wiki.alpinelinux.org/wiki/APKBUILD_Reference Build Comparison Let\u0026rsquo;s highlight a few similarities and differences in them, excluding the fact that one is fetched from git and the other one fetches a point release directly2:\npackage metadata, by the means of bash variables, are almost equivalent one-to-one A notable difference is the architecture, Arch primarily supports x86_64 whereas Alpine has support for multiple architectures. When a package is architecture agnostic, Arch denotes it with any whereas alpine has both noarch and all, the latter is like any (=all architectures), the former means it\u0026rsquo;s agnostic (=e.g. a pure bash script or python package). APKBUILDs use flat strings, whereas PKGBUILDs use bash arrays Alpine encourages splitting larger packages into subpackages, as such APKBUILD has first-class support and syntactic sugar for that. -dev and -doc subpackages are very common. On the other hand, Arch tends to have monolithic packages in order to keep it simple, although it also supports subpackages. Alpine supports setting $builddir whereas Arch doesn\u0026rsquo;t. As a consequence, it\u0026rsquo;s often unneeded to cd in build() and package() in Alpine, whereas in Arch one does need to manually change directories to $srcdir/$pkgname before building. Alpine lacks optional dependencies, whereas Arch has optdepends. Alpine enforces the use of check in test packages, otherwise it needs to be explicitly disabled and documented with !check in options=. That\u0026rsquo;s not the case in Arch. check(), build() and package() are pretty much similar in both formats. $srcdir and $pkgdir are provided in both. The ArchWiki is way more documented in terms of packaging guidelines and examples than Alpine\u0026rsquo;s. If you use DuckDuckGo, you can query for !aw \u0026lt;foo\u0026gt; as a bang shortcut to search directly in the ArchWiki. Last but not least, in Arch one can install package tarballs3 with makepkg -i or pacman -U. In Alpine that approach doesn\u0026rsquo;t seem to be directly supported. The workflow is to add a local repository directory in /etc/apk/repositories (notice the last two lines):\n$ cat /etc/apk/repositories # http://dl-cdn.alpinelinux.org/alpine/v3.15/main # http://dl-cdn.alpinelinux.org/alpine/v3.15/community # http://dl-cdn.alpinelinux.org/alpine/latest-stable/main # http://dl-cdn.alpinelinux.org/alpine/latest-stable/community http://dl-cdn.alpinelinux.org/alpine/edge/main http://dl-cdn.alpinelinux.org/alpine/edge/community http://dl-cdn.alpinelinux.org/alpine/edge/testing /home/$USER/packages/community /home/$USER/packages/testing abuild will place the resulting package tarball in ~/packages, in this case:\n$ ls ~/packages/testing/x86_64/fpp* /home/$USER/packages/testing/x86_64/fpp-0.9.2-r0.apk /home/$USER/packages/testing/x86_64/fpp-doc-0.9.2-r0.apk \u0026hellip;and then apk add fpp will automagically recognize it\u0026rsquo;s in there and install it. The advantage of this approach is that it keeps a local package repository around and it\u0026rsquo;s well integrated with apk, way differently from pacman that has no integration with the AUR at all. One could also possibly set up a local repository in Arch, for example, with ccm, but it takes extra steps and it\u0026rsquo;s not officially supported.\nUpstream Contributions On Arch, to contribute a PKGBUILD upstream one just needs to create an account in the AUR. Armed with a git + ssh infrastructure, all you need to do is git push. There are no ACLs involved, anyone can do that4.\nOn Alpine there\u0026rsquo;s a bit more of politics involved5: Anyone can send a patch(1), either via mailing list or via a Gitlab MR (merge request). Patch works well with git send-email -1, being automatically cross-posted to a Gitlab MR. On the other hand the MR workflow is easier to be followed up on feedback from developers and other contributors (git push --force), and it\u0026rsquo;s also cross-posted, to the mailing list. An Alpine developer with the appropriate permissions must approve your patch/MR before it becomes available to other Alpine users.\nSadly at the time of this writing my patch hasn\u0026rsquo;t yet been approved (2 weeks later), however we\u0026rsquo;re in holiday season. This wouldn\u0026rsquo;t have been a problem in the AUR, where I could have just pushed it immediately, without any review. On the other hand the Alpine approach at least gives me some hope that the submitted packages have slightly higher quality than the average ones in the AUR, since they need to be manually reviewed/approved/vetted by at least one Alpine developer.\nInstall spdx-licenses-list to lint the licenses, it\u0026rsquo;s used by abuild sanitycheck as an optional dependency.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe AUR tends to have both non-vcs and vcs versions of a software, whereas Alpine is focused a bit more on stability and tends to have non-vcs only. This is not a hard rule though, exceptions may exist.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n.tar.xz or, more recently, .tar.zstd.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAnd this is one of the reasons why you should always inspect every PKGBUILD you install from the Arch User Repository, as it could have been tampered with and/or contain malicious code.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI\u0026rsquo;ll leave it open-ended whether that\u0026rsquo;s a bug or a feature. Depending on the lens you see through, it could be considered either gatekeeping (bureaucracy, control) or sanity (quality, stability). It has pros and cons, and even those are arguable.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/01/my-first-apkbuild/","summary":"\u003cp\u003eIn the same spirit of my first PKGBUILD and \u003ca href=\"https://www.perrotta.dev/2014/09/my-first-ebuild/\"\u003e\u003ccode\u003eEbuild\u003c/code\u003e\u003c/a\u003e, herein I will describe my first \u003ccode\u003eAPKBUILD\u003c/code\u003e.\u003c/p\u003e","title":"★ My First APKBUILD"},{"content":"Alternative title:\nVentoy: A keychain for all your live operating systems\nFrom the project website, ventoy is an:\nopen source tool to create bootable USB drive for ISO/WIM/IMG/VHD(x)/EFI files.\nWith ventoy, you don\u0026rsquo;t need to format the disk over and over, you just need to copy the ISO/WIM/IMG/VHD(x)/EFI files to the USB drive and boot them directly.\nBefore The typical linux desktop user workflow to try out new distros1 is to:\nDownload an .iso or .img file. Grab a USB stick2 and format/erase it. Run an application3 to write the image to the block device. Boot your workstation, press some key to change your BIOS/UEFI setup to boot via USB. Profit. This works well if you only need to do it once or twice, but carries a few caveats:\nThe USB flash drive contents are completely erased. Extra measures need to be taken in order to use it the rest of its storage persistently. You\u0026rsquo;ll have to redo this whole process whenever there\u0026rsquo;s a new release of your OS. Even for rolling release OSes like Arch Linux, Gentoo or Alpine Linux, you\u0026rsquo;ll typically want to have a more up-to-date image anyway, otherwise the installation process will eventually become problematic4, as the system gets more and more out-of-date. You can only keep one bootable image at a time in your USB stick. Unless you have multiple USB sticks, suddenly you will find that your 32 GB (or even 128 GB) storage is useless to hold multiple operating systems. Afterwards Compare that with Ventoy\u0026rsquo;s workflow:\nDownload Ventoy, and install it to your USB flash drive. This needs to be done only once5. Download an .iso or .img file. Copy it to the USB storage, using your file manager. You could use cp from the command line, you could also drag and drop using your file manager, whether it\u0026rsquo;s from Windows, Linux or macOS, doesn\u0026rsquo;t really matter. The destination is just an ordinary directory, nothing fancy. Boot your workstation, press some key to change your BIOS/UEFI setup to boot via USB. Profit. The number of steps is the same, but Ventoy really shines in the following aspects:\nThere\u0026rsquo;s no need to use any special software to write your images to the USB stick. You just copy and paste a file. Really! And you can do that from any OS. You can add as many images as you want. For example, you could keep your favorite desktop OS therein, alongside your favorite ARM OS (for your raspberry pi), alongside your favorite system rescue utility. They will all co-exist, and once you boot with the USB you\u0026rsquo;ll be able to choose which image you want to boot to, with a nice bootloader menu, à la GRUB or systemd-boot. Upgrading an OS is just a matter of deleting the old one and copying the new one over, exactly like you would do with a simple document file. Your USB flash drive is formatted in such a way that it\u0026rsquo;s possible to use its unused storage as persistent storage. So for example you could boot into your desktop OS, save your work, then reboot into your system rescue utility6 and pick up the leftover files therein. Ventoy is magic. It allows you to carry a single USB stick with all of your favorite operating systems - and they don\u0026rsquo;t even have to be Linux, most operating systems are supported:\nMost type of OS supported (Windows/WinPE/Linux/ChromeOS/Unix/VMware/Xen\u0026hellip;)\nThe full list of supported systems is here.\nIf you need some inspiration to fill in your Ventoy USB stick, head over to distrowatch.\nObviously excluding virtualized solutions like containers (like docker or lxc) and virtual machines. I am also excluding chroots like systemd-boot.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNo one burns CDs anymore, right? Right?\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIn 2020s, Balena Etcher is a pretty popular application to do so. Linux users often just resort to the command-line instead: % dd if=mydisk.img of=/dev/sdb status=progress; sync.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTry installing Arch from a 2010 .iso in 2020 and let me know how it goes.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nYou may want to occasionally re-image Ventoy, say, in 5, 10 years? It will probably keep working even if you don\u0026rsquo;t, though, at least with the operating systems supported at the time you installed it.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMy favorite one is GRML, which is debian-based.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/01/ventoy-automate-your-distro-hopping/","summary":"\u003cp\u003eAlternative title:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eVentoy: A keychain for all your live operating systems\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eFrom the project website, \u003ca href=\"https://www.ventoy.net/en/index.html\"\u003e\u003cstrong\u003eventoy\u003c/strong\u003e\u003c/a\u003e is an:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eopen source tool to create bootable USB drive for ISO/WIM/IMG/VHD(x)/EFI files.\u003c/p\u003e\n\u003cp\u003eWith ventoy, you don\u0026rsquo;t need to format the disk over and over, you just need to copy the ISO/WIM/IMG/VHD(x)/EFI files to the USB drive and boot them directly.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Ventoy: Automate your Distro hopping"},{"content":"In this post we will learn how to define a command-not-found hook to the apk(8) package manager in Alpine Linux.\nSneak peek Before:\n$ podman zsh: correct \u0026#39;podman\u0026#39; to \u0026#39;pod2man\u0026#39; [nyae]? n zsh: command not found: podman After:\n$ podman zsh: correct \u0026#39;podman\u0026#39; to \u0026#39;pod2man\u0026#39; [nyae]? n podman may be found in the following packages: \u0026lt;cmd:podman\u0026gt; podman-3.4.4-r1 x86_64 {podman} (Apache-2.0) Preamble Whenever you type a command that is not in your $PATH, usually your shell will yell at you that it wasn\u0026rsquo;t found.\nThe typical workflow in this scenario is to use the search functionality of your package manager in order to find which package provides the binary you\u0026rsquo;re interested in.\nIn Alpine Linux, one would do:\n$ apk search podman podman-doc-3.4.4-r1 podman-remote-3.4.4-r1 podman-docker-3.4.4-r1 openscap-1.3.5-r3 podman-3.4.4-r1 podman-compose-0.1.5-r4 podman-bash-completion-3.4.4-r1 podman-zsh-completion-3.4.4-r1 py3-podman-3.2.1-r1 podman-docker-doc-3.4.4-r1 podman-openrc-3.4.4-r1 podman-fish-completion-3.4.4-r1 The output is a bit noisy, but with a bit of experience you could easily figure out the package you\u0026rsquo;re looking for is simply called podman, given the output above.\nSurely this was an easy example, what if we tried something less obvious?\n$ vidir zsh: correct \u0026#39;vidir\u0026#39; to \u0026#39;vdir\u0026#39; [nyae]? n zsh: command not found: vidir There\u0026rsquo;s no vidir binary, then surely there\u0026rsquo;s a vidir package, right?\n$ doas apk add vidir ERROR: unable to select packages: vidir (no such package): required by: world[vidir] Er, no. You\u0026rsquo;ll need to use search again:\n$ apk search vidir moreutils-0.67-r0 There it is, moreutils. Great piece of software, by the way1.\nWhat if we could automate this?\nAutomating command-not-found: 1st try In bash, one can define a command_not_found_handle function. In zsh, one can define a command_not_found_handler function. I know, why can\u0026rsquo;t it be the same function, right? Just one r in the way. Regardless of whichever shell you use, the point is that the function is invoked whenever you run a command that is not in the $PATH (or that isn\u0026rsquo;t a shell built-in).\nIn principle, you could do:\ncommand_not_found_handle() { local cmd=\u0026#34;$1\u0026#34; apk search \u0026#34;$cmd\u0026#34; } It\u0026rsquo;s a good first try, and it surely works as expected, but it can be a bit noisy sometimes. Look at the podman output above, it outputs several unrelated packages, none of which provide the podman binary other than its homonym.\nAutomating command-not-found: 2nd try In Alpine, we can do slightly better. apk(8) has the concept of providers:\n$ apk list -P | awk \u0026#39;{print $1}\u0026#39; | egrep \u0026#39;\u0026lt;\\w+:\u0026#39; | cut -f 1 -d \u0026#39;:\u0026#39; | cut -c 2- | sort -u cmd dbus pc so -P above stands for --providers. This roughly means one can search for a package that provides a given shared library (so), or a package that provides a given binary (cmd), and so on. We\u0026rsquo;re interested in the cmd: provider.\nIf we tried it with podman, we would get the following output:\n$ apk list -P -- \u0026#34;cmd:podman\u0026#34; \u0026lt;cmd:podman\u0026gt; podman-3.4.4-r1 x86_64 {podman} (Apache-2.0) Look at how much shorter and direct it is, compared to the 1st approach!\nHere\u0026rsquo;s what it looks like if we try it with a binary provided by multiple packages:\n$ apk list -P -- \u0026#34;cmd:docker\u0026#34; \u0026lt;cmd:docker\u0026gt; docker-cli-20.10.11-r0 x86_64 {docker} (Apache-2.0) [installed] \u0026lt;cmd:docker\u0026gt; podman-docker-3.4.4-r1 x86_64 {podman} (Apache-2.0) It\u0026rsquo;s very easy to see that both docker-cli and podman-docker provide docker. If you just did a simple search, you\u0026rsquo;d get a lot of noise:\n$ apk search docker docker-bash-completion-20.10.11-r0 docker-cli-20.10.11-r0 docker-machine-driver-kvm2-1.24.0-r0 x11docker-6.9.0-r2 docker-volume-local-persist-1.3.0-r5 podman-docker-3.4.4-r1 openvswitch-2.12.3-r4 docker-engine-20.10.11-r0 docker-openrc-20.10.11-r0 dockerize-0.6.1-r9 docker-fish-completion-20.10.11-r0 openscap-1.3.5-r3 docker-py-5.0.3-r1 openvswitch-ovn-2.12.3-r4 docker-registry-openrc-2.7.1-r5 docker-doc-20.10.11-r0 rsyslog-imdocker-8.2108.0-r0 lazydocker-0.12-r2 docker-compose-bash-completion-1.29.2-r2 docker-compose-1.29.2-r2 py3-dockerpty-0.4.1-r4 docker-compose-zsh-completion-1.29.2-r2 docker-registry-2.7.1-r5 docker-credential-ecr-login-0.5.0-r2 dockerpy-creds-0.4.0-r3 docker-cli-compose-2.1.1-r0 docker-credential-ecr-login-doc-0.5.0-r2 podman-docker-doc-3.4.4-r1 docker-20.10.11-r0 docker-compose-fish-completion-1.29.2-r2 flannel-contrib-cni-0.15.1-r0 docker-zsh-completion-20.10.11-r0 docker-volume-local-persist-openrc-1.3.0-r5 docker-cli-buildx-0.7.1-r0 Packaging2 it all together I wrote the following scripts, which I source in my respective interactive shells, to achieve this behavior out-of-the-box:\n$ cat apk-command-not-found.bash #!/bin/bash # apk(8) from Alpine Linux command not found hook for bash command_not_found_handle () { local cmd=\u0026#34;$1\u0026#34; pkgs mapfile -t pkgs \u0026lt; \u0026lt;(apk list -P -- \u0026#34;cmd:$cmd\u0026#34; 2\u0026gt;/dev/null) if (( ${#pkgs[*]} )); then echo \u0026#34;$cmd may be found in the following packages:\u0026#34; printf \u0026#39; %s\\n\u0026#39; \u0026#34;${pkgs[@]}\u0026#34; else echo \u0026#34;bash: command not found: $cmd\u0026#34; fi 1\u0026gt;\u0026amp;2 return 127 } $ cat apk-command-not-found.zsh #!/bin/zsh # apk(8) from Alpine Linux command not found hook for zsh command_not_found_handler() { local cmd=\u0026#34;$1\u0026#34; local pkgs=(${(f)\u0026#34;$(apk list -P -- \u0026#34;cmd:$cmd\u0026#34; 2\u0026gt;/dev/null)\u0026#34;}) if [[ -n \u0026#34;$pkgs\u0026#34; ]]; then echo \u0026#34;$cmd may be found in the following packages:\u0026#34; printf \u0026#39; %s\\n\u0026#39; \u0026#34;${pkgs[@]}\u0026#34; else echo \u0026#34;zsh: command not found: $cmd\u0026#34; fi 1\u0026gt;\u0026amp;2 return 127 } The snippets above are snapshots intended for this post. I keep up-to-date versions of these files in my dotfiles repository, try out this query in case I ever move them elsewhere.\nhttps://joeyh.name/code/moreutils/: moreutils is a collection of the unix tools that nobody thought to write long ago when unix was young.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\npun intended\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/01/alpine-linux-apk-command-not-found-hook/","summary":"\u003cp\u003eIn this post we will learn how to define a command-not-found hook to the \u003ccode\u003eapk(8)\u003c/code\u003e package manager in Alpine Linux.\u003c/p\u003e\n\u003ch2 id=\"sneak-peek\"\u003eSneak peek\u003c/h2\u003e\n\u003cp\u003eBefore:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e$ podman\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ezsh: correct \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;podman\u0026#39;\u003c/span\u003e to \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;pod2man\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e[\u003c/span\u003enyae\u003cspan style=\"color:#f92672\"\u003e]\u003c/span\u003e? n\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ezsh: command not found: podman\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eAfter:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e$ podman\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ezsh: correct \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;podman\u0026#39;\u003c/span\u003e to \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;pod2man\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e[\u003c/span\u003enyae\u003cspan style=\"color:#f92672\"\u003e]\u003c/span\u003e? n\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epodman may be found in the following packages:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u0026lt;cmd:podman\u0026gt; podman-3.4.4-r1 x86_64 \u003cspan style=\"color:#f92672\"\u003e{\u003c/span\u003epodman\u003cspan style=\"color:#f92672\"\u003e}\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e(\u003c/span\u003eApache-2.0\u003cspan style=\"color:#f92672\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"★ Alpine Linux: apk command not found hook"},{"content":"Direnv is a tool to automate your shell to automatically load and unload environment variables on-the-fly, on a per-project (per-directory) basis.\nPreliminaries: Is it worth it? Questions I like to ask myself before deciding whether to invest my time into learning and potentially adopting a foreign tool are the following:\nIs it popular and stable enough? Is it abandonware?\nPopularity Popularity is relative, it doesn\u0026rsquo;t need to be worthy of the Hacker News frontpage nor Hotness on Reddit, but it needs to be widely available in popular Linux distributions and/or package managers, one install command away from my development environment.\nRepology is a good proxy for popularity. Looking at direnv therein, it\u0026rsquo;s available for Alpine, Arch, Debian, Fedora, FreeBSD, HomeBrew, Nix, OpenBSD, Ubuntu\u0026hellip;that\u0026rsquo;s more than enough, we can safely conclude direnv is widely popular.\nThe main takeaway we want to confirm is whether the project isn\u0026rsquo;t too niche and/or an one-man show. Seeing signs of a small-ish community and/or occasional contributions from external users/developers helps build confidence and give credibility to the project.\nStability and Abandonware Stability is easier to define than popularity and can often be determined just by taking a quick glance at the github (or whichever other forge it\u0026rsquo;s hosted in) page of the project.\nAt the time of this writing, the latest release of direnv was about a week ago (2.30.2, Dec 28th 2021). It\u0026rsquo;s definitely not abandonware and it\u0026rsquo;s well maintained. A few signs that help corroborate that:\nSeveral PRs were merged recently Its issue tracker is quite active, with a good mix of feature requests and bugs I don\u0026rsquo;t like to judge the project based on the number of issues it has, especially if it\u0026rsquo;s popular. Chromium has 60k+ issues at the time of this writing, yet I wouldn\u0026rsquo;t call it bleeding edge. Common sense applies. Since direnv has been around for a while and it\u0026rsquo;s relatively popular, 150+ open issue seems acceptable to me. Now that direnv passed the Litmus test for adoption1, let\u0026rsquo;s get our hands dirty.\nInstallation There\u0026rsquo;s nothing special here, as direnv is widely packaged. Pick your poison:\n$ sudo pacman -Syu direnv # Arch Linux $ doas apk add direnv # Alpine Linux $ sudo apt install direnv # Debian-based distros Is it lightweight?\n$ apk info -L direnv direnv-2.30.1-r0 contains: usr/bin/direnv Hell yes! More lightweight than that? Impossible. It\u0026rsquo;s a single binary thanks to Golang. No tons of files or dependencies. I mean:\n$ du -sh /usr/bin/direnv 7.5M /usr/bin/direnv \u0026hellip;it\u0026rsquo;s a 7MB binary, let\u0026rsquo;s not get ahead of ourselves. But that\u0026rsquo;s fine, really, it\u0026rsquo;s just a dev tool, we don\u0026rsquo;t really deploy it to prod.\nUse Cases Everything is controlled with a .envrc file within a repository root. A typical file could look like this:\nexport HOUSE=\u0026#34;ATREIDES\u0026#34; layout python3 The upstream website does a great job at summarizing use cases. I am not here to duplicate documentation, so please go ahead and read it. That said, here are some example use cases I found useful:\nUse Case: Python Python developers often need to create different virtual environments for different projects. For example, I was participating in Advent of Code last year and wrote my solutions in Python 3: https://github.com/thiagowfx/adventofcode.\nEach day2 I would cd ~/projects/adventofcode, and then do source ~/.venv/bin/activate. And guess what, that\u0026rsquo;s for the first terminal where I\u0026rsquo;d run make, I\u0026rsquo;d also spawn a second one with vim, thereby needing to activate the virtual environment twice.\nAnd this is assuming the virtual environment already exists. If it didn\u0026rsquo;t - for example, after a vanilla git clone, I\u0026rsquo;d have to do python -m venv .venv first.\nQuickly all of this became repetitive and annoying. I kinda \u0026ldquo;cheated\u0026rdquo; and stopped using the virtualenv for a few days, relying on my Linux distribution package manager instead:\n% apk add py3-{autopep8,pyflakes,numpy,pylint} This way, my\nimport numpy would correctly work and not yell that numpy was nowhere to be found.\nIt\u0026rsquo;s not very clean, but it worked. However eventually I wanted to become cleaner and leaner and automate my virtual environment setup. I uninstalled the aforementioned packages after a few days:\n% apk del py3-{autopep8,pyflakes,numpy,pylint} \u0026hellip;therefore forcing me to come up with a better setup. I always had direnv in my TODO list, and this was the perfect moment to try it out.\nHow does direnv address this?\nAdd the direnv hook to your shell. I actively use two shells3, bash and zsh, so I did it twice and then added it to my dotfiles: Bash:\n$ cat ~/.bashrc.d/direnv.bash #!/bin/bash # https://direnv.net/ if hash direnv \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; then eval \u0026#34;$(direnv hook bash)\u0026#34; fi Zsh:\n$ cat ~/.zshrc.d/direnv.zsh #!/bin/zsh # https://direnv.net/ if (( $+commands[direnv] )); then eval \u0026#34;$(direnv hook zsh)\u0026#34; fi Set up direnv in the AOC repository: $ cat ~/projects/adventofcode/.envrc layout python3 $ direnv allow # Only needs to be done once That\u0026rsquo;s it: It\u0026rsquo;s a single line of configuration. Now what does it do? All of the above. No magic: whenever you cd into the project directory or any of its subdirectories with one of the configured shells, if the venv doesn\u0026rsquo;t exist:\nit will be automatically created; then it will be sourced Now you may ask yourself: Why go through all this trouble? Why not simply create a shell script to do exactly that for you automatically? That\u0026rsquo;s perfectly fine, it\u0026rsquo;s a matter of taste. But then you\u0026rsquo;ll have to maintain that script. The python ecosystem keeps changing - a few years ago I was using virtualenvwrapper to manage virtual environments, these days it doesn\u0026rsquo;t exist anymore, people use either python -m env or pyenv or poetry or\u0026hellip;it never ends. Drew DeVault wrote a good piece about that.\nXKCD Courtesy of Randall Munroe\nMaintenance is not the only burden, scalability is also one: If you use python in several repositories, you\u0026rsquo;ll now have to include your script in all of them.\nConsidering that direnv is flexible enough in other scenarios, I consider its adoption in this situation a good trade-off to make.\nUse Case: Hugo This blog is written in Hugo. I have a Makefile with a bunch of environment variables to manage its setup:\n$ make dev Whenever I am working in my VPS, for reasons outside of the scope of this post I need to use a different port other than the default one for Hugo (1313). Since I am using variables, I could just do:\n$ make PORT=1234 dev However, to make this change permanent (\u0026ldquo;fire-and-forget\u0026rdquo;), I could also do:\n$ echo \u0026#39;export PORT=1234\u0026#39; | tee -a .envrc $ direnv allow # Only needs to be done once $ make dev This way, whenever I run make I wouldn\u0026rsquo;t even need to think twice about which port to use.\nOf course, a small improvement that should be done in this scenario is to add direnv related files to your .gitignore:\n$ git ignore direnv \u0026gt;\u0026gt; .gitignore # Created by https://www.toptal.com/developers/gitignore/api/direnv # Edit at https://www.toptal.com/developers/gitignore?templates=direnv ### direnv ### .direnv .envrc # End of https://www.toptal.com/developers/gitignore/api/direnv Other use cases? I don\u0026rsquo;t have other real use cases to share because only recently I became familiarized with direnv. That said, the direnv docs are very comprehensive of its full potential usage.\nSome use cases that I like:\ndotenv Automatically sources .env (note: not to confuse with .envrc) files, which are widely common in projects managed with docker-compose. source_env + env_vars_required Alongside .gitignore, this is a great way to source secrets (e.g. API keys or tokens) and not accidentally check them into your repository. fetchurl bash | curl is a cancer4 that should arguably be stopped due to its inherent security risks. That said, direnv provides a safer way to work with it because you can specify a hash to ensure you\u0026rsquo;re downloading the same script - if an attacker or malicious actor modified it, direnv would throw an error. path_add If your project outputs to e.g. build/\u0026lt;...\u0026gt;/bin or similar (typical in cmake projects and AFAIK in Rust ones too), you could add that directory to your PATH so that you could easily execute your binaries, without having to write the full subdirectory path each time. layout Besides python, direnv supports several other programming languages out-of-the-box. Popular examples include go, nix, node, perl and ruby. Downsides? One could call direnv bloated because of all of the aforementioned capabilities. If it doesn\u0026rsquo;t spark joy for your taste, consider using autoenv which is basically a leaner version of direnv, meant mostly for doing one thing and doing it well: setting and unsetting variables.\nOther than that, direnv is pretty much a great piece of software.\nOne thing I didn\u0026rsquo;t cover is how secure it is: You need to run direnv allow explicitly in order to tell direnv that you trust a given .envrc file. If you don\u0026rsquo;t do it, direnv will refuse to source it:\n$ touch .envrc direnv: error ~/projects/foo/.envrc is blocked. Run `direnv allow` to approve its content If you run direnv allow but later on the file is modified (for example, after git pull, whereby you retrieve a modification from a teammate), direnv will once again refuse to operate. You\u0026rsquo;ll need to whitelist it again by re-running direnv allow. Direnv will snapshot/hash the file contents of .envrc remember it across sessions.\nReferences Tools You Should Know About: direnv Obviously the aforementioned list was non-exhaustive. There are a few other questions that you may want to ask, out of scope of this article, such as: (i) does the project have an OSS or FLOSS license? (ii) does the project depend on Java?\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAdvent of code challenges are released one by one, thereby forcing you to wait until the next day in order to get the next challenge.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nmore on this another day\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nc.f. https://curlpipesh.tumblr.com/, https://gnu.moe/wallofshame.md\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/01/direnv-automate-your-environment-variables/","summary":"\u003cp\u003e\u003ca href=\"https://direnv.net/\"\u003eDirenv\u003c/a\u003e is a tool to automate your shell to automatically load and unload environment variables on-the-fly, on a per-project (per-directory) basis.\u003c/p\u003e","title":"★ Direnv: Automate your Environment Variables"},{"content":"This article describes my experience transitioning to, setting up and using the Miniflux RSS reader for the first time.\nPreamble I always kind of enjoyed following people and blogs via RSS, even though it has never been a key part of my workflow (nor of the mainstream web). That said, I am not here to convince you why RSS is great, there are good existing resources12 for that already.\nInitially I had used Commafeed, Feedly and Inoreader, which are hosted solutions. They are mostly OK, especially if you only have a handful of feeds. Their free offerings are quite decent, with a limit of a hundred or so feeds. They also have mobile clients (Android, iOS) which are a must these days. I was never fully converted to them though, and my workflow therein would only last for a few weeks or months. Some common barriers were:\ntheir recommendations and \u0026lsquo;machine learning\u0026rsquo; fluff were a consistent source of stress, with a fear of missing out (FOMO) akin to social media. I felt pressured to keep following new blogs just like I am pressured to constantly \u0026rsquo;like\u0026rsquo; and \u0026lsquo;follow\u0026rsquo; new pages in traditional American social media.\nthere was a lot of context switching: many upstream RSS feeds aren\u0026rsquo;t great, for example, by providing excerpts (summaries) only I\u0026rsquo;d always have to visit the website directly if I wanted to read full articles. This doesn\u0026rsquo;t scale well long term, attention is a precious resource and our brains aren\u0026rsquo;t great at keeping steady and focused attention if we constantly context switch. A classical feed like this is Paul Graham\u0026rsquo;s.\nthere was no ability to filter out (exclude) posts from feeds. For example, deleting posts with a certain title (Sponsor, Ad, some boring mainstream topic). A classical example is John Gruber\u0026rsquo;s Daring Fireball sponsored posts which usually have \u0026lsquo;Sponsor\u0026rsquo; in their titles. Why do I have to manually skip these posts, why can\u0026rsquo;t I teach my RSS reader to do it automatically for me?\nlock-in: whenever I starred/saved posts that I liked for future reference, they would be stuck in the specific cloud provider I chose.\nThese were some of my gripes.\nDuring those years I had also tried to self-host TinyTinyRSS but it didn\u0026rsquo;t really last for me:\nfirst, its stack is relatively bloated: hosting and maintaining a typical LAMP stack takes some considerable amount of effort — TinyTinyRSS requires a full PHP installation alongside a webserver (apache, nginx or similar) and a database. Suddenly there was a lot of complexity to maintain all that.\nsecond, I didn\u0026rsquo;t have any cloud resources (VPS), nor a local server in my home (e.g. a Raspberry Pi or a NUC or a NAS Appliance). An instance in my personal laptop wouldn\u0026rsquo;t really scale either as I would have needed it to be always on if I wanted to have continuous access to it (e.g. from my phone).\nthird, I wasn\u0026rsquo;t a seasoned sysadmin at the time and wasn\u0026rsquo;t really looking forward to self-host.\nThen 2020 and the COVID-19 pandemic came along with all of its imposed government lockdowns worldwide. Suddenly many people had a lot of free time on their hands.\nSelf-hosting at home Having an Arch Linux workstation at home, it felt natural to try out Miniflux there first.\nMiniflux has great upstream documentation already, therefore it\u0026rsquo;s just a matter of following it. It\u0026rsquo;s out of scope of this post to duplicate the installation process here, however I will add a bit of color regarding my initial setup.\nDisclaimer: Those instructions will probably get out-of-date at some point.\nThankfully there\u0026rsquo;s already a miniflux package for Arch, making my job much easier. Installing miniflux alone isn\u0026rsquo;t enough though, we will also need to install a database server (PostgreSQL):\n$ sudo pacman -Syu miniflux postgresql The next step is to configure the PostgreSQL server. Refer to the upstream documentation for that, but the TL;DR is:\ncreate a miniflux user create a miniflux database owned by the miniflux user perform a few tweaks (extension hstore) Then configure miniflux:\n$ cat /etc/miniflux.conf # Purge articles after a few days: These values are actually the default. Listed here just for reference. CLEANUP_ARCHIVE_READ_DAYS=30 CLEANUP_ARCHIVE_UNREAD_DAYS=90 # Database configuration DATABASE_URL=user=miniflux password=\u0026lt;password\u0026gt; dbname=miniflux sslmode=disable RUN_MIGRATIONS=yes We will also need to create an admin user for miniflux with miniflux --create-admin.\nThen we start the database server and miniflux:\n$ sudo systemctl enable --now postgresql miniflux Afterwards it\u0026rsquo;s just a matter of navigating to http://localhost:8080 and logging in with your newly created admin user.\nMiniflux is a pleasure to use, and it\u0026rsquo;s very easy to get acquainted with it.\nIt\u0026rsquo;s also possible to add custom CSS in its Settings. I added the following tweaks3 for improved typography:\n:root { --system-font-family: system-ui, -apple-system, BlinkMacSystemFont, \u0026#34;Segoe UI\u0026#34;, Roboto, Helvetica, Arial, \u0026#34;Noto Sans\u0026#34;, sans-serif, \u0026#34;Apple Color Emoji\u0026#34;, \u0026#34;Segoe UI Emoji\u0026#34;, \u0026#34;Segoe UI Symbol\u0026#34;, \u0026#34;Noto Color Emoji\u0026#34;; --font-family: var(--system-font-family); --entry-content-font-family: var(--system-font-family); } body { max-width: 900px; } textarea[name=\u0026#34;custom_css\u0026#34;] { min-height: 300px; width: -webkit-fill-available; } Update(2022-02-18): It turns out the CSS above isn\u0026rsquo;t really needed as a system font stack is set out-of-the-box.\nThe one main limitation of running Miniflux this way is that you\u0026rsquo;ll need your workstation to be always on if you want to have continuous access to it. This means if you want to read late at night you\u0026rsquo;ll need to leave your computer on. Not only this is impractical and inconvenient, it\u0026rsquo;s also not much environmentally friendly.\nAnother limitation is that in principle you\u0026rsquo;ll only be able to access Miniflux from home, unless you take extra measures4 to make your workstation accessible from outside your home network.\nOther resources Miniflux clients:\nMiniflux web app (PWA), works well enough Unread (iOS) via Fever API, gesture based Reeder (iOS) via Fever API newsboat (CLI) Common self-hosted alternatives to miniflux:\nFreshRSS TinyTinyRSS Feedbin (harder to self-host) https://kevq.uk/please-add-rss-support-to-your-site/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://laurakalbag.com/subscribe/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://css-tricks.com/snippets/css/system-font-stack/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nfor example: a VPN like OpenVPN, Wireguard or tailscale; or a tool like ngrok.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/01/miniflux-rolling-my-own-rss-reader/","summary":"\u003cp\u003eThis article describes my experience transitioning to, setting up and using the\n\u003ca href=\"https://miniflux.app/\"\u003eMiniflux\u003c/a\u003e RSS reader for the first time.\u003c/p\u003e","title":"★ Miniflux: Rolling my own RSS Reader"},{"content":"If you ever find yourself in a hurry to get access to a Linux shell but don\u0026rsquo;t easily have one at hand (e.g. a workstation or a raspberry pi or a VPS), it\u0026rsquo;s possible to quickly get access to an ephemeral instance in the cloud.\nHere\u0026rsquo;s a non-exhaustive list of trusted providers:\nGoogle Cloud Shell Welcome to Google Cloud Shell, a tool for managing resources hosted on Google Cloud Platform! The machine comes pre-installed with the Google Cloud SDK and other popular developer tools.\nYour 5GB home directory will persist across sessions, but the VM is ephemeral and will be reset approximately 20 minutes after your session ends. No system-wide change will persist beyond that.\nType \u0026ldquo;gcloud help\u0026rdquo; to get help on using Cloud SDK. For more examples, visit https://cloud.google.com/shell/docs/quickstart and https://cloud.google.com/shell/docs/examples\nType \u0026ldquo;cloudshell help\u0026rdquo; to get help on using the \u0026ldquo;cloudshell\u0026rdquo; utility. Common functionality is aliased to short commands in your shell, for example, you can type dl \u0026lt;filename\u0026gt; at Bash prompt to download a file. Type \u0026ldquo;cloudshell aliases\u0026rdquo; to see these commands.\nType \u0026ldquo;help\u0026rdquo; to see this message any time. Type \u0026ldquo;builtin help\u0026rdquo; to see Bash interpreter help.\nSee also: https://cloud.google.com/shell\nGithub Codespaces 👋 Welcome to Codespaces! You are on our default image.\nIt includes runtimes and tools for Python, Node.js, Docker, and more. See the full list here: https://aka.ms/ghcs-default-image Want to use a custom image instead? Learn more here: https://aka.ms/configure-codespace 🔍 To explore VS Code to its fullest, search using the Command Palette (Cmd/Ctrl + Shift + P or F1).\n📝 Edit away, run your app as usual, and we\u0026rsquo;ll automatically make it available for you to access.\nSee also: https://github.com/codespaces\nTip: Replace https://github.com/username/repository with https://github.dev/username/repository to automatically open the repository within a github codespace. Alternatively, if you have github shortcuts enabled, press . while in the repository page.\n","permalink":"https://www.perrotta.dev/2022/01/ephemeral-linux-shell-access-in-the-cloud/","summary":"\u003cp\u003eIf you ever find yourself in a hurry to get access to a Linux shell but don\u0026rsquo;t\neasily have one at hand (e.g. a workstation or a raspberry pi or a VPS), it\u0026rsquo;s\npossible to quickly get access to an ephemeral instance in the cloud.\u003c/p\u003e","title":"Ephemeral Linux Shell Access in the Cloud"},{"content":" Ufw stands for Uncomplicated Firewall, and is a program for managing a netfilter firewall. It provides a command line interface and aims to be uncomplicated and easy to use.\nThe firewall makes justice to its name as it is really uncomplicated, and a pleasure to set up.\nInstall Install and set up ufw1, which should be packaged for most linux distributions:\nOpenRC-based (Alpine Linux, Gentoo) # Install ufw and ufw-extras $ doas apk install ufw{,-extras} # Enable ufw daemon $ doas rc-update add ufw # Start ufw daemon $ doas rc-service ufw start # Enable firewall $ doas ufw enable Systemd-based (Arch Linux, Debian) # Install ufw and ufw-extras $ sudo pacman -Syu ufw{,-extras} # Enable and start ufw daemon $ sudo systemctl enable --now ufw # Enable firewall $ sudo ufw enable Add rules Firewall rules can be added with ufw allow [port] or ufw allow [name]. Named profiles (for example: ssh, http) live in /etc/ufw/applications.d/, or you can query all of them with ufw app list.\n% ufw app list Available applications: AIM Bonjour CIFS DNS Deluge IMAP IMAPS IPP KTorrent Kerberos Admin Kerberos Full Kerberos KDC Kerberos Password LDAP LDAPS LPD MSN MSN SSL Mail submission NFS POP3 POP3S PeopleNearby SMTP SSH Socks Telnet Transmission Transparent Proxy VNC WWW WWW Cache WWW Full WWW Secure XMPP Yahoo qBittorrent svnserve Ufw also supports ufw limit [port | name] which is like add but with the added ability to \u0026ldquo;deny connections from IP addresses that attempt to initiate 6 or more connections in the last 30 seconds\u0026rdquo;. It\u0026rsquo;s a good measure to mitigate brute-force and/or DDOS attacks.\n# either use named profiles % ufw allow http-alt % ufw limit ssh # or port numbers % ufw allow 8080/tcp % ufw limit 22 Remove rules Firewall rules can be removed by merely adding \u0026lsquo;delete\u0026rsquo; between ufw and the verb.\n% ufw delete allow 8080/tcp % ufw delete limit ssh Check status One status command to rule them all, \u0026ldquo;verbose\u0026rdquo; is optional:\n% ufw status verbose Status: active Logging: on (low) Default: deny (incoming), allow (outgoing), disabled (routed) New profiles: skip To Action From -- ------ ---- 22/tcp LIMIT IN Anywhere 8080 ALLOW IN Anywhere 22/tcp (v6) LIMIT IN Anywhere (v6) 8080 (v6) ALLOW IN Anywhere (v6) Ufw uses iptables under the hood. Inspect the underlying iptables rules:\n% iptables -S | egrep \u0026#39;\\b(22|8080)\\b\u0026#39; -A ufw-user-input -p tcp -m tcp --dport 22 -m conntrack --ctstate NEW -m recent --set --name DEFAULT --mask 255.255.255.255 --rsource -A ufw-user-input -p tcp -m tcp --dport 22 -m conntrack --ctstate NEW -m recent --update --seconds 30 --hitcount 6 --name DEFAULT --mask 255.255.255.255 --rsource -j ufw-user-limit -A ufw-user-input -p tcp -m tcp --dport 22 -j ufw-user-limit-accept -A ufw-user-input -p tcp -m tcp --dport 8080 -j ACCEPT References https://help.ubuntu.com/community/UFW https://wiki.archlinux.org/title/Uncomplicated_Firewall ufw-extras is optional, it contains additional rules (e.g. mosh, tailscale).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2021/12/ufw-firewall/","summary":"\u003cblockquote\u003e\n\u003cp\u003eUfw stands for Uncomplicated Firewall, and is a program for managing a netfilter firewall. It provides a command line interface and aims to be uncomplicated and easy to use.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Ufw: Firewall"},{"content":"Every modern terminal emulator supports ANSI OSC-52 out-of-the-box. xterm is not one of those1.\nOSC-52 allows one to copy text into the system clipboard. It\u0026rsquo;s a very handy escape sequence to be used alongside terminal emulators and terminal multiplexers such as tmux/screen.\nIt\u0026rsquo;s also possible to enable OSC-52 in vim, making copy-and-paste a first-class citizen therein.\nAs mentioned in the intro, most modern applications already support it out-of-the-box, as such there\u0026rsquo;s no need to configure them. We would like to configure xterm as well though, because it is widely available in pretty much every Unix out there.\n$ grep -B 1 -i allowWindowOps ~/.Xresources ! osc-52 support *.allowWindowOps: true Then apply:\n$ xrdb -merge ~/.Xresources All new xterm applications should then pick up the new resource.\nmaybe because it\u0026rsquo;s not modern, and it\u0026rsquo;s not decent? ;)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2021/12/xterm-enable-ansi-osc-52/","summary":"\u003cp\u003eEvery modern terminal emulator supports \u003ca href=\"https://invisible-island.net/xterm/ctlseqs/ctlseqs.html#h3-Operating-System-Commands\"\u003eANSI OSC-52\u003c/a\u003e out-of-the-box. \u003ccode\u003exterm\u003c/code\u003e is not one of those\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e","title":"Xterm: Enable ANSI OSC-52"},{"content":"Whenever I connect/disconnect my bluetooth headphones to my Linux workstation, I need to manually switch to/off them.\nEvery commercial OS I\u0026rsquo;m aware of does that automatically, including Chrome OS, macOS and Windows.\nTo automate this behavior on Linux, it depends whether we\u0026rsquo;re using PulseAudio or Pipewire. It boils down to loading the module-switch-on-connect pulseaudio module.\nNote: Tested on Arch Linux only.\nPulseAudio $ sudoedit /etc/pulse/default.pa ... load-module module-switch-on-connect ... Then restart pulseaudio:\n$ systemctl --user restart pulseaudio PipeWire $ sudo mkdir -p /etc/pipewire $ sudo cp /usr/share/pipewire/pipewire-pulse.conf /etc/pipewire/pipewire-pulse.conf $ sudoedit /etc/pipewire/pipewire-pulse.conf ... # Extra modules can be loaded here. Setup in default.pa can be moved here context.exec = [ { path = \u0026#34;pactl\u0026#34; args = \u0026#34;load-module module-switch-on-connect\u0026#34; } ] ... Alternatively, ~/.config/pipewire/pipewire-pulse.conf should also work. We should not edit the file in /usr because it will not survive package upgrades.\nThen restart pipewire:\n$ systemctl --user restart pipewire{,-pulse} References https://wiki.archlinux.org/title/PulseAudio#Switch_on_connect https://gitlab.freedesktop.org/pipewire/pipewire/-/wikis/Config-PulseAudio#configuration-file-pipewire-pulseconf ","permalink":"https://www.perrotta.dev/2021/12/linux-auto-switch-to-bluetooth-headset/","summary":"\u003cp\u003eWhenever I connect/disconnect my bluetooth headphones to my Linux workstation, I need to manually switch to/off them.\u003c/p\u003e","title":"Linux: Auto Switch to Bluetooth Headset"},{"content":"I recently purchased a Keychron K2 mechanical keyboard. It is a 75% keyboard that has bluetooth. This article describes some quality-of-life improvements to use it with a Linux system.\nNote: Tested on Arch Linux only.\nKernel Module Keychron keyboards are recognized out-of-the-box as Apple keyboards on Linux systems.\nThe corresponding kernel module is hid_apple.\n$ modinfo hid_apple Ensure the module is loaded within the initram for setups that use LUKS:\n$ grep MODULES -B 1 /etc/mkinitcpio.conf # hid_apple for Keychron K2 MODULES=(hid_apple) This works for wired mode but alas it\u0026rsquo;s not possible to use the keyboard wirelessly to type in your LUKS password unless extra steps are taken:\nInstall the mkinitcpio-bluetooth package, currently available in the AUR. Add the bluetooth hook to your mkinitcpio, ensuring it comes before encrypt. $ grep HOOKS -B 1 /etc/mkinitcpio.conf # bluetooth for Keychron K2 HOOKS=(base udev autodetect keyboard bluetooth modconf block encrypt filesystems resume fsck) Regenerate it: % mkinitcpio -P. Bluetooth There are 3 bluetooth slots, corresponding to the 1, 2 and 3 keys.\nTo put the keyboard in pairing mode, hold Fn + \u0026lt;n\u0026gt; for a few seconds. The key will persistently blink. From the computer, connect to the device named Keychron K2. Trust the keyboard.\nSwitch slots by pressing Fn + \u0026lt;n\u0026gt; once. There is some light feedback to indicate the switch.\nBluetooth works well out-of-the-box, but the keyboard automatically sleeps after 10 minutes of inactivity to save energy. While it is possible to disable this behavior, I find it welcome. It is a hassle though because the bluetooth device refuses to reconnect once the keyboard is awaken. The naive solution is to pair it again from scratch, but a better user experience is to change bluetooth settings:\n$ cat /etc/bluetooth/main.conf ... [General] FastConnectable=true [Policy] UserspaceHID=true ... And then restart bluetooth to apply them:\n% systemctl restart bluetooth This way the keyboard always automatically reconnects to the computer.\nI also find it convenient to leave bluetooth enabled on startup:\n# Enable the bluetooth daemon. % systemctl enable bluetooth # Enable the bluetooth adapter. $ cat /etc/bluetooth/main.conf ... [Policy] AutoEnable=true ... Finally, another tweak is to make the bluetooth adapter stay awake otherwise it may periodically disconnect:\n$ cat /etc/modprobe.d/btusb.conf # Turn off bluetooth autosuspend. options btusb enable_autosuspend=0 Function Keys and Multimedia Keys The default fnmode is set to multimedia keys:\n0 = disabled 1 = normally media keys, switchable to function keys by holding Fn key (Default) 2 = normally function keys, switchable to media keys by holding Fn key I prefer to set it to function keys. One of the reasons for that is to make switching TTYs (Ctrl+Alt+Fn) possible. For some reason, Fn doesn\u0026rsquo;t seem to work in TTYs.\nChange it in the current session only:\n% echo 2 \u0026gt;\u0026gt; /sys/module/hid_apple/parameters/fnmode If you do not have a root shell, use sudo / doas + tee:\n% echo 2 | sudo tee /sys/module/hid_apple/parameters/fnmode Change it permanently:\n$ cat /etc/modprobe.d/hid_apple.conf # Use function keys by default. Press Fn to use multimedia keys. options hid_apple fnmode=2 And then reload the kernel module:\n% modprobe -r hid_apple \u0026amp;\u0026amp; modprobe hid_apple Insert Key By default there is no native Insert key. Use Fn + Del to trigger Insert. For example, Fn + Shift + Del works like Shift + Insert in X11, yielding paste selection.\nBattery Check the battery level programmatically:\n% pacman -S upower $ upower --dump | grep -i keyboard -A 7 | grep percentage percentage: 71% This only works in bluetooth mode (not in wired mode).\nShortcuts Hold Fn + [1 | 2 | 3] for a few seconds: Put bluetooth slot in pairing mode. The corresponding LED will persistently blink until pairing is complete. Fn + [1 | 2 | 3]: Toggle bluetooth slot. The corresponding LED will briefly blink. Fn + b: Check battery level visually. Green is more than 70%, blue is more than 30%, otherwise it will flash. Fn + Light: Toggle keyboard lights on/off. Fn + [Left | Right]: Browse keyboard light color schemes. References https://wiki.archlinux.org/title/Apple_Keyboard https://github.com/kurgol/keychron https://gist.github.com/andrebrait/961cefe730f4a2c41f57911e6195e444 https://mikeshade.com/posts/keychron-linux-function-keys/ https://github.com/kurgol/keychron/blob/master/k2.md ","permalink":"https://www.perrotta.dev/2021/12/keychron-k2-linux-setup/","summary":"\u003cp\u003eI recently purchased a \u003ca href=\"https://www.keychron.com/products/keychron-k2-wireless-mechanical-keyboard\"\u003eKeychron K2\u003c/a\u003e mechanical keyboard. It is a 75% keyboard that has bluetooth. This article describes some quality-of-life improvements to use it with a Linux system.\u003c/p\u003e","title":"★ Keychron K2: Linux Setup"},{"content":"Hello World from Hugo!\nI am joining the indieweb and the JAMStack by rolling my own blog backed by a static site generator (SSG) and deployed with git.\nPreviously I used to have a blog hosted on Wordpress called Everyday Serendipity.\n","permalink":"https://www.perrotta.dev/2021/12/hello-world-from-hugo/","summary":"Hello World from Hugo!\nI am joining the indieweb and the JAMStack by rolling my own blog backed by a static site generator (SSG) and deployed with git.\nPreviously I used to have a blog hosted on Wordpress called Everyday Serendipity.","title":"Hello World from Hugo"},{"content":"Recentemente participei de uma CTF promovida pelo ELT (Epic Leet Team). Uma das challs que consegui resolver completamente foi a matroshka, e aqui está um breve write-up sobre a mesma.\nDado um arquivo matroshka.tar.gz, precisávamos encontrar a flag.\nNão era difícil desconfiar do que esse arquivo / chall se tratava: matroshkas são aquelas bonecas russas que se encaixam umas dentro das outras. Então\u0026hellip;de cara, logo já desconfiei: provavelmente existe um arquivo compactado dentro de outro, dentro de outro, dentro de outro, e assim por diante\u0026hellip;\nPor experiência, não valeria a pena tentar descompactar tudo manualmente, pois sabe-se lá quantos níveis de compactação esse negócio iria ter (provavelmente mais do que 100).\nDe cara logo pensei em usar o dtrx, que é um excelente programa (não perco tempo e sempre rodo um port install dtrx) para extrair arquivos sem ter que ficar se lembrando das sintaxes individuais de cada programa. Nesse caso, não iria rolar: os arquivos eram renomeados de forma a trickear o dtrx, que funciona através de heurísticas, uma delas é a \u0026rsquo;extensão\u0026rsquo; do nome do arquivo. Por exemplo, vários arquivos (após descompactados) eram renomeados na forma *.elt.\nA segunda alternativa foi (serendipidade, não conhecia essa ferramenta antes) tentar utilizar o atool. Por motivos similares ao dtrx, não rolou.\nPois bem, então o jeito ia ser descompactar tudo na marra. Pensei em escrever um programa que faria o seguinte:\ntry { unzip \u0026lt;file\u0026gt; } catch { try { tar xf \u0026lt;file\u0026gt; } catch { // ...e assim por diante } } Obviamente eu utilizaria os programas diretamente, então a coisa poderia ficar um pouco mais simples, utilizando os return codes dos mesmos para detectar se descompactaram o arquivo com sucesso. Por exemplo, tar xf \u0026lt;file\u0026gt; retorna 0 se rodou corretamente, do contrário ele retorna algo diferente de zero. Isso se mostrou válido para todos os programas de descompactação que utilizei, exceto o lha, que insistia em retornar 0 de qualquer jeito, mesmo quando falhava.\nPara automatizar essa tarefa, resolvi utilizar python2. C/C++ provavelmente também seriam bons candidatos, mas eu queria praticar o meu python.\nApós algumas inspeções, notei que cada arquivo continha um e somente um arquivo dentro dele, então a ideia base seria:\nmantenha uma lista com todos os arquivos conhecidos até então (no começo, só haveria um); descompacte esse arquivo; detecte qual arquivo acabou de ser descompactado continue fazendo isso até encontrar a flag Meu código ficou assim:\n#!/usr/bin/env python import os import subprocess TARGET_DIR = \u0026#39;mat\u0026#39; def uncompress_kgb(file): return subprocess.call([\u0026#34;kgb\u0026#34;, file]) def uncompress_gzip(file): return subprocess.call([\u0026#34;gunzip\u0026#34;, \u0026#34;-S\u0026#34;, \u0026#39;.\u0026#39; + file.split(\u0026#39;.\u0026#39;)[-1], file]) def uncompress_tar(file): return subprocess.call([\u0026#34;tar\u0026#34;, \u0026#34;xvf\u0026#34;, file]) def uncompress_rar(file): return subprocess.call([\u0026#34;unrar\u0026#34;, \u0026#34;x\u0026#34;, file]) def uncompress_lha(file): return subprocess.call([\u0026#34;lha\u0026#34;, \u0026#34;e\u0026#34;, file]) def uncompress_zip(file): return subprocess.call([\u0026#34;unzip\u0026#34;, file]) def uncompress_arj(file): subprocess.call([\u0026#34;cp\u0026#34;, file, file + \u0026#34;.arj\u0026#34;]) err = subprocess.call([\u0026#34;arj\u0026#34;, \u0026#34;x\u0026#34;, file]) subprocess.call([\u0026#34;rm\u0026#34;, file + \u0026#34;.arj\u0026#34;]) return err def uncompress_7z(file): subprocess.call([\u0026#34;7z\u0026#34;, \u0026#34;x\u0026#34;, file]) def colorprint(s): print \u0026#39;\\033[93m\u0026#39; + repr(s) + \u0026#39;\\033[0m\u0026#39; os.chdir(TARGET_DIR) base = set() while True: newbase = set(os.listdir(\u0026#39;.\u0026#39;)) diff = newbase - base colorprint(diff) if len(diff) \u0026gt; 1: raise Exception(\u0026#34;len(diff) \u0026gt; 1\u0026#34;) elif len(diff) == 0: print \u0026#34;len(diff) == 0\u0026#34; break for file in diff: err = uncompress_kgb(file) if err != 0: err = uncompress_gzip(file) if err != 0: err = uncompress_tar(file) if err != 0: err = uncompress_rar(file) if err != 0: err = uncompress_zip(file) if err != 0: err = uncompress_arj(file) if err != 0: err = uncompress_7z(file) if err != 0: err = uncompress_lha(file) if err != 0: print \u0026#34;lha fail\u0026#34; base = newbase Essa ideia funcionou bastante bem. A única coisa overkill foi que eu não deletei arquivos anteriores; isso poderia ter simplificado significativamente o problema (e os sets no python).\nAdemais, uma das coisas chatas do arj é que ele só é capaz de extrair arquivos que terminam em *.arj, então fui obrigado a renomear/copiar um arquivo antes de tentar utilizá-lo para extrair seu conteúdo.\n","permalink":"https://www.perrotta.dev/2016/10/matroshka/","summary":"\u003cp\u003eRecentemente participei de uma\n\u003ca href=\"https://ctf.tecland.com.br/Pwn2Win/game/scoreboard/\"\u003eCTF\u003c/a\u003e promovida pelo\n\u003ca href=\"https://ctf-br.org/elt\"\u003eELT\u003c/a\u003e (Epic Leet Team). Uma das \u003cem\u003echalls\u003c/em\u003e que consegui\nresolver completamente foi a \u003cstrong\u003ematroshka\u003c/strong\u003e, e aqui está um breve \u003cem\u003ewrite-up\u003c/em\u003e\nsobre a mesma.\u003c/p\u003e\n\u003cp\u003eDado um arquivo \u003ccode\u003ematroshka.tar.gz\u003c/code\u003e, precisávamos encontrar a \u003cem\u003eflag\u003c/em\u003e.\u003c/p\u003e","title":"Matroshka"},{"content":"Chromebooks are excellent for testing and playing with Linux userland[1] stuff. And, even better, (almost) every change you make to it can be reset back to its factory state — even in the worst case of completely wiping ChromeOS from your computer. As long as you don\u0026rsquo;t mess up with its firmware, you can do whatever you want and still be safe. Most models are cheap, battery life is amazing for a cheap Linux laptop (8+ hours, depending on the device and on usage)[2], Linux support is great (well, it ships with a Linux OS, right?) — even for most touchscreen models [citation needed] — and it is simple (as in KISS).\nNow, the previous paragraph sounded a little repetitive, but I\u0026rsquo;d like to focus on two of my previous points here:\nYou can do whatever you want and still remain safe. Yes. Indeed, you can reset your chromebook to its factory state with just a few clicks — this is still hard to do with Windows and OS X, as far as I know. I think Windows 10 is introducing a feature to ease this process, however I am not sure about it yet. This is super important (and convenient!) for people playing with a ChromeOS device — like me. My usual way to reset the system back to \u0026ldquo;factory state\u0026rdquo; was, usually, to install a Linux distro with the btrfs filesystem so I could create a snapshot upon the installation was finished, and then revert back to it at anytime I wanted. However, this solution is a little cumbersome. The second usual solution would be to simply use a virtual machine, but I don\u0026rsquo;t need to reiterate that this is slow and limited in many ways. Simple. This is really important if you don\u0026rsquo;t want to have headaches in the long run. For starters, simplicity is always a trade-off that involves performance: it is hard to be simple with a core i7 CPU, so if you choose the KISS path, you are deliberately sacrificing some of your performance. Once you accept that, we can move forward: Chromebooks are very simple devices. They\u0026rsquo;re almost directly comparable to a tablet with a keyboard. Sometimes they feel like a full-featured arduino. These were some small highlights about my decision to get one. Now here\u0026rsquo;s the good stuff:\nThere be dragons Disclaimer/Warning: these will potentially void the warranty of your device, and they are only recommended if you know what you\u0026rsquo;re doing. Be a good user and research before messing up with your system. I will provide upstream links with documentation as much as possible.\nEnable developer mode — it\u0026rsquo;s a must; however, it weakens the security of your device. Beware and research! This can be totally reverted with a single keystroke (SPACE) at the login screen that will appear in every subsequent boot of your system. dev_install — to get fdisk, python, tmux, nano, emerge (for binaries), and so on. These are just a few developer tools, they won\u0026rsquo;t scale and you won\u0026rsquo;t get whatever package you want in your vanilla ChromeOS, however these might help a lot with simple tasks. For example, you can format a SD card or an USB Flash Drive directly from ChromeOS upon running this tool — otherwise, you\u0026rsquo;d usually have to use another computer, with another OS to do it. It uses just a little space (~100M+ or so), and it can be totally reverted with devinstall --uninstall Switch to the beta channel if you want to be slightly more into the bleeding edge — but not too much as in the dev channel. In fact, at the time of this post, select devices in beta channel have support for Android apps in ChromeOS. Open a new shell (crosh) with Ctrl + Alt + T. The default username is \u0026lsquo;chronos\u0026rsquo;, without any password. User-writable locations are: /usr/local (in particular, /usr/local/bin is in your PATH env variable out-of-the-box (echo $PATH)), /tmp and /home/chronos/user/Downloads. There might be more, but these are enough. You can also insert removable media (USB flash drives, SD cards), which would show up on /media. ChromeOS can read/write filesystems formatted as ext{2,3,4} and FAT{,32} at least. These are the basics. However, you will soon find out that you cannot do too much with ChromeOS, even with these modifications. In this case, there are three options, and all of them involve getting a Linux distribution in some way:\n(RECOMMENDED) Install a Linux distro on either a SD card or an USB Flash Drive and dual-boot from it. It is really easy to dual-boot, you just press C-d to boot from the internal eMMC / SSD, or C-u to boot from external storage. There\u0026rsquo;s no need to play with grub, EFI, or any (other) fancy bootloader. If you change your mind later on, just format your SD card: there is no need to change anything on ChromeOS. (NOT RECOMMENDED BY ME) Completely wipe ChromeOS by installing another Linux distribution on its place. Although it\u0026rsquo;s easy to revert this process through recovery media, it completely defeats the purpose of getting a Chromebook in the first place, especially now that Android apps are starting to become available on them (for touchscreen devices, of course). (PROBABLY WOULDN\u0026rsquo;T RECOMMEND, BUT YOU MAY LIKE IT) Install a chroot alongside your ChromeOS, such as crouton. I tried it however I hated it. It is very buggy and opinionated, somewhat similar to oh-my-zsh: both of them promise to deliver a good framework and they\u0026rsquo;re kinda large open source projects, however they do lots of automatic things for you, and you end up not knowing what is happening to your system. It opposes the Arch Linux philosophy, being user-friendly rather than user-centered. Some people like it, I won\u0026rsquo;t argue this, it can work for them; but not for me. An alternative would be to install a chroot manually, without the help of crouton. I\u0026rsquo;ve chosen the first option, installing Arch Linux ARM on my Chromebook. So far, so good: it\u0026rsquo;s working very well. Storage is easily shared between the two OSes (by using the SD card). There\u0026rsquo;s more to get out of my system, and I intend to share new findings in this blog. In particular, the most important one: to find an easy way (not crouton!) of running whatever Linux-compatible application I want from within Chrome OS, without dual-booting. I have a few ideas (well, it\u0026rsquo;s just a matter of creating a KISS chroot), I just want to polish them a little more before trying them out.\nSee also Generic Chrome OS Troubleshooter Chart v0.3 Footnotes I wouldn\u0026rsquo;t say the same for kernel stuff though. It is not trivial to install a bootloader (such as grub) or to boot a custom kernel in a Chromebook. It\u0026rsquo;s possible indeed, it\u0026rsquo;s just not as nice or as easy as an average PC or Mac laptop from today, especially if you have the intention of keeping the upstream OS (Chrome OS) in the device. People probably would cite a Macbook, a Surface Pro, or something else similar here. For starters, did you see how much they cost, in comparison to an average Chromebook? Such a comparison wouldn\u0026rsquo;t be fair. ","permalink":"https://www.perrotta.dev/2016/09/linux-goodness-in-your-chromebook/","summary":"\u003cp\u003e\u003ca href=\"https://www.google.com.br/chromebook/\"\u003eChromebooks\u003c/a\u003e are excellent for testing\nand playing with Linux userland[1] stuff. And, even better, (almost) every\nchange you make to it can be reset back to its factory state — even in the worst\ncase of completely wiping ChromeOS from your computer. As long as you don\u0026rsquo;t mess\nup with its \u003ca href=\"https://en.wikipedia.org/wiki/Firmware\"\u003efirmware\u003c/a\u003e, you can do\nwhatever you want and still be safe. Most models are cheap, battery life is\namazing for a cheap Linux laptop (8+ hours, depending on the device and on\nusage)[2], Linux support is great (well, it ships with a Linux OS, right?) —\neven for most touchscreen models [citation needed] — and it is simple (as in\nKISS).\u003c/p\u003e","title":"Linux goodness in your Chromebook"},{"content":"What’s Haiku in the first place? It is an operating system, period. What might cause a small surprise is that it is not based either on Linux or on BSD — and yet it is (probably) runnable in your modern computer/laptop!\nWhat I most like on it is that the system is integrated with the GUI, so the end user gets a nice experience. This is not so common as it sounds: most Linux distributions are simply a collection of programs and utilities put together in one place, but they are not necessarily integrated — however, you can integrate them. This makes all the difference between user-friendly and user-centered paradigms1.\nAnyway, Haiku is simple, so for me this post is more a hobby than something useful that I will use in the future; however, I find that knowing about more and more about different operating systems has its own advantages.\nFor more, see https://www.haiku-os.org/about.\nInstalling Haiku Note: most of those instructions come from here. Also, thanks David Couzelis for kindly giving me some advice and pointing me out to them.\nGet Haiku. You can do it either from here, or get nightly releases from here. I personally recommend the nightly releases; I first installed the latest non-nightly one, but later on I discovered that is was very old (circa 2012): it didn’t even have a package manager. Which image should you download? In this post, I am assuming we’ll install Haiku directly to a (real) disk, so I’m downloading the raw image. Actually, the anyboot image can also be downloaded — and this is the one you will get if you opted for a non-nightly version; however, the anyboot image will be converted to a raw one in step 3 before we proceed. So, skip the next step if you downloaded the raw image. Got your anyboot image? Now, convert it to raw (run this from a bash compatible shell): $ dd if=haiku-anyboot.image of=haiku.raw bs=1M skip=$(expr $(od -j 454 -N4 -i -A n haiku-anyboot.image) / 2048) $ dd if=/dev/zero of=haiku.raw bs=1 seek=506 count=4 conv=notrunc Update (2022): These days dd supports status=progress to display the image writing progress.\nNow that we got a raw image, we are writing it directly to a disk partition. First things first: find 3GB or more of free space in your disk. Then create room for a partition in there (for example, with fdisk or gparted). Got it? Now create a partition in there. I’ll assume the partition is /dev/sda42. Please change 42 for the appropriate number in your case. Copy the raw image to the partition (this should be done as ROOT); WARNING: double check the partition and the disk number, otherwise you might lose data. $ dd if=haiku.raw of=/dev/sda42 bs=1M conv=notrunc Make the installation bootable: you’ll need to compile and run the makebootabletiny program, which can be downloaded from here. It is a simple C program, so: $ gcc makebootabletiny.c -o makebootabletiny % ./makebootabletiny /dev/sda42 # this one: as ROOT Make your bootloader know about Haiku. If you’re using grub2z, you can add something such as the following lines to /etc/grub.d/40_custom: menuentry \u0026#34;Haiku OS\u0026#34; { set root(hd0,42) chainloader +1 } Then run as root: grub-mkconfig -o /boot/grub/grub.cfg Done! Now you should be able to boot into Haiku.\nNow what? This post is not a review of Haiku, so I’m stopping here. However, if I write a review about Haiku, I’ll do that from Haiku 🙂.\nI’m just leaving this here: https://www.haiku-os.org/slideshows/haiku-1\nBy the way, Arch Linux is user-centered, which means that you’re supposed to integrate the system as you wish. If you don’t wish that then you’re screwed anyway, so go away 🙂\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2015/04/installing-haiku-from-an-existing-linux/","summary":"\u003ch2 id=\"what8217s-haiku-in-the-first-place\"\u003eWhat’s Haiku in the first place?\u003c/h2\u003e\n\u003cp\u003eIt is an operating system, period. What might cause a small surprise is that it is not based either on Linux or on BSD — and yet it is (probably) runnable in your modern computer/laptop!\u003c/p\u003e","title":"Installing Haiku from an existing linux"},{"content":"Update: I stopped at task 06 on February. Little seems to have stopped responding e-mails. Which is sad, because I was liking those tasks =/\nI am participating in the Eudyptula Challenge, which is not something exactly new, but it is as able to get you out of your comfort zone as if it were1.\nIt puts you into the mind of a Linux Kernel developer, where you are required to write and submit patches and to complete some tasks.\nSubscribing to the challenge is a mini challenge itself2: you must write to little (the superuser behind the thing) an e-mail in plain text. This means: NO HTML. This looks like a silly requirement, but it’s the way how the (real) Linux developers work, and it is actually slightly fancier than you probably imagine. Little himself recommends that the participants don\u0026rsquo;t use either the Gmail web interface or Outlook.\nI intend to document here my own experiences with Eudyptula3.\nSetting up The first thing I went after was a decent and updated Linux distro; doing the challenge within a Linux rather than anything else offers a better experience. Zero effort here, as I’ve already had Arch Linux4 installed.\nNext: a good text editor. Wait…why not two? I am using vim to compose e-mails, and emacs to write code.\nWhat’s next? Oh yes…a decent sendmail program. It is not mandatory to have one of those, but it eases a lot the process of writing e-mail in plain text. I grabbed msmtp plus its MTA. Writing a config file for it is pretty straightforward.\nAnd now I was ready: I subscribed to the challenge.\nThe task #01 Hours later, little5 acknowledged my e-mail and sent me the instructions for the first task.\nI should write my first module for the Linux Kernel.\nA good start. I didn’t even know how to begin, oh no!\nThe task was to write a module that would write ‘Hello World’ to the debug level log of the running kernel when loaded.\nFirst, I had to install some dependencies to be able to compile a couple of C programs. Actually, the base-devel group from Arch already contained most of the things I would need. linux-headers and bc were also relevant.\nMy first module, written in C6, looked like this in the end:\n#include \u0026lt;linux/init.h\u0026gt;; #include \u0026lt;linux/kernel.h\u0026gt;; #include \u0026lt;linux/module.h\u0026gt;; MODULE_AUTHOR(\u0026#34;Thiago Perrotta\u0026#34;); MODULE_DESCRIPTION(\u0026#34;A gentle Hello World module\u0026#34;); MODULE_LICENSE(\u0026#34;GPL\u0026#34;); static int __init hello_init(void) { printk(KERN_DEBUG \u0026#34;Hello world!\\n\u0026#34;); return 0; } static void __exit hello_cleanup(void) { printk(KERN_DEBUG \u0026#34;Goodbye World!\\n\u0026#34;); } module_init(hello_init); module_exit(hello_cleanup); I didn’t get it right in my first trial, of course. In particular, I wrote ‘Hello World’ to the INFO log level, using KERN_INFO, instead of KERN_DEBUG.\nAnd I also needed a Makefile to be able to compile it:\nobj-m += hello.o KERNEL ?= /lib/modules/$(shell uname -r)/build all: make -C $(KERNEL) M=$(PWD) modules clean: make -C $(KERNEL) M=$(PWD) clean The Makefile wasn\u0026rsquo;t right in my first trial, either. Another requirement of this task was that this file should provide an environment variable, as an alternative location for the build directory of the kernel, opposed to being a hard coded value. I provided one from the beginning, but the way I wrote it was slightly wrong.\nNow, this code didn\u0026rsquo;t just pop out of my head out of nothing. Although I know how to program in C, I never did kernel programming before. I had to look up for some documentation about how to write a module. It wasn\u0026rsquo;t super straightforward, because I found many documentation sources about writing modules for the Linux 2.6.x version, which is outdated now7, and the modern version of writing modules is a bit different from it.\nAlso, I was careful not to accidentally find a solution for this task directly. Of course, some people have already done it, and they could have written something about their experiences, as I am doing right now.\nAnyways, after writing those two files and testing that they really worked as expected, I loaded my freshly built module. And yay, a simple dmesg would reveal my hello message out there. It was really cool to see it worked:\nthiago@archpad ~//01 % sudo modinfo hello.ko filename: /home/thiago//01/hello.ko license: GPL description: A gentle Hello World module author: Thiago Perrotta depends: vermagic: 3.17.6-1-ARCH SMP preempt mod_unload modversions After that, I’ve sent everything I should to little, and today I got its reply, about the second task. So, let’s go…\nWhat have I learned with this task? how to write a Makefile to compile a kernel module how to execute shell commands within a Makefile how to use an environment variable within a Makefile how to write e-mails in plain text and send them with msmtp how to send attachments without base64 encoding8 how to load, unload and get information about modules modules are so dynamic and easy to load, huh? Like USB devices. I would never imagine that. It is also a excuse for me to write something here :-)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPun intended.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI can’t write this in one shot. Really. Eudi←yptull←a. EUDYPTULA.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe distro that will conquer the moon.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWhich turned out to be a bot.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDamn, everything will be written in C from now on. That’s what we get with kernel programming, I guess.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOnly lazy or paranoid Linux sysadmins will tell you otherwise 😉\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis turned out to be the hardest thing of this task, yet it wasn’t even part of it.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2015/01/the-eudyptula-challenge/","summary":"Update: I stopped at task 06 on February. Little seems to have stopped responding e-mails. Which is sad, because I was liking those tasks =/\nI am participating in the Eudyptula Challenge, which is not something exactly new, but it is as able to get you out of your comfort zone as if it were1.\nIt puts you into the mind of a Linux Kernel developer, where you are required to write and submit patches and to complete some tasks.","title":"The Eudyptula Challenge"},{"content":" Ebuilds are not evil\n— Larry, the Cow\nFrom now on, this will be my overlay repository: https://github.com/thiagowfx/overlay\nI might change its name in the future, however it will probably remain on GitHub. From my experience with this blog, I realized it would be better to leave a copy of this ebuild here:\n# Copyright 1999-2014 Gentoo Foundation # Distributed under the terms of the GNU General Public License v2 # $Header: $ EAPI=\u0026#34;5\u0026#34; inherit font DESCRIPTION=\u0026#34;A font specially designed for comfortably reading on any computer or device\u0026#34; HOMEPAGE=\u0026#34;http://www.huertatipografica.com/fonts/bitter-ht\u0026#34; SRC_URI=\u0026#34;http://www.fontsquirrel.com/fonts/download/${PN}\u0026#34; LICENSE=\u0026#34;OFL\u0026#34; SLOT=\u0026#34;0\u0026#34; KEYWORDS=\u0026#34;amd64 ~arm ~arm64 ~ppc ~ppc64 x86\u0026#34; IUSE=\u0026#34;\u0026#34; HDEPEND=\u0026#34;app-arch/unzip\u0026#34; FONT_SUFFIX=\u0026#34;otf\u0026#34; S=\u0026#34;${WORKDIR}\u0026#34; src_unpack() { mv \u0026#34;${DISTDIR}/${A}\u0026#34; \u0026#34;${DISTDIR}/${A}.zip\u0026#34; unpack \u0026#34;${A}.zip\u0026#34; } src_install() { FONT_S=\u0026#34;${WORKDIR}\u0026#34; font_src_install } My conclusion? Writing ebuilds is nice, very nice (yeah, more complex than writing PKGBUILDs, I know, but at least this complexity is justified).\nEdit: Thanks Buss for pointing me out a small error (RDEPEND, instead of HDEPEND).\n","permalink":"https://www.perrotta.dev/2014/09/my-first-ebuild/","summary":"Ebuilds are not evil\n— Larry, the Cow\nFrom now on, this will be my overlay repository: https://github.com/thiagowfx/overlay\nI might change its name in the future, however it will probably remain on GitHub. From my experience with this blog, I realized it would be better to leave a copy of this ebuild here:\n# Copyright 1999-2014 Gentoo Foundation # Distributed under the terms of the GNU General Public License v2 # $Header: $ EAPI=\u0026#34;5\u0026#34; inherit font DESCRIPTION=\u0026#34;A font specially designed for comfortably reading on any computer or device\u0026#34; HOMEPAGE=\u0026#34;http://www.","title":"My first ebuild"},{"content":"TL;DR: pequena TODO list sobre como instalar o Gentoo. Você vai passar 90% do seu tempo olhando para texto dando scroll na tela (processo demorado…).\nEsse é o método mais tradicional possível (e que me interessa) que pude constatar. Adapte-o para as suas próprias necessidades:\nBoote em um ambiente com o Arch (na verdade, você pode fazer isso a partir de qualquer distro decente).\nCrie uma partição /, do tipo ext4, para acomodar a instalação do Gentoo. De preferência, coloque um label decente lá. Sugestões de ferramentas para isso: cfdisk ou gparted.\nMonte essa partição em /mnt/gentoo.\nContinue a partir do handbook oficial do Gentoo – nesse caso, para a arquitetura amd64. No entanto, atenção. Use esse guia para perceber quais etapas de instalação e configuração são diferentes (em relação a se estivéssemos instalando pelo método oficial, a partir do live environment do gentoo), mais especificamente da parte 4 à parte 6.\nBaixe o PKGBUILD gentoo-mirrorselect do AUR.\nUse mirrorselect -i -o e então selecione interativamente os mirrors mais próximos da sua localização atual. Copie o valor da variável GENTOO_MIRRORS para o final do seu /mnt/gentoo/etc/portage/make.conf.\nCopie o seu /etc/resolv.conf para /mnt/gentoo/etc/resolv.conf. Isso é importante para ter conectividade dentro do ambiente de chroot.\nCFLAGS que recomendo para o /etc/portake/make.conf (helper):\n-march=native -O2 -pipe A partir daqui, nada especial, apenas continue seguindo o handbook do gentoo, até a parte do bootloader.\nChegando na parte do bootloader, optei por deixar o Arch gerenciá-lo. Nesse caso, basta rodar um típico grub-mkconfig -o /boot/grub/grb.cfg que o Gentoo deverá ser automaticamente detectado (supondo que o pacote os-prober esteja instalado).\nConfigure a rede no Gentoo. Isso é bastante específico, mas o procedimento é bem parecido com a configuração da rede no Arch. A única questão é que, ao dar emerge no wpa_supplicant (no caso de você utilizar Wi-fi), vai demorar bastante até todas as dependências serem instaladas (esse é o ponto principal que me afastou do Gentoo até hoje. Sem pacotes binários, ter que compilar tudo localmente…ao menos, se no final a otimização do sistema for maior, poderia ter valido mais a pena.)\nTeste o Gentoo durante uma semana e diga o que você achou dele 😉\nDecida se você gosta mais do Gentoo ou do Arch. Isso vale tanto para o sistema, tanto para a comunidade. Não tenho ideia de como seja a comunidade do Gentoo (a do Arch eu já tinha uma boa noção de como era mesmo antes de ter o sistema instalado).\nValeu pessoal. Como sou 100% newbie no Gentoo, apreciarei quaisquer dicas! (Enquanto as dicas não aparecem, Wiki Pages e Forums Threads me esperam). Sabe, é bom, de vez em quando, ser newbie em alguma coisa. Claro que ser expert / muito bom / hacker em algumas aplicações é ótimo, mas eu acredito na importância de possuir uma boa base de conhecimento em geral (não necessariamente apenas para fins acadêmicos ou financeiros, mas também para satisfazer a mente: desafios são importantes!).\nAh, mais dois comentários:\nTerminando de escrever esse post e de editá-lo, a compilação dos (130) pacotes que o wpa_supplicant puxou não chegou nem na metade… (eu teria instalado uns três ou quatro Archs automatizados nesse tempo – tá bom, não vale comparar pacotes binários com código-fonte, eu sei)\nEsse é o meu terceiro ou quarto post utilizando o org2blog (do emacs). Já ficou bastante claro para mim que vou passar a usá-lo de maneira fixa. O único problema é que eu ainda não sei a sintaxe de formatação dele direito, é muita informação. Não é nem que não seja intuitiva, mas já tem LaTeX, BBCode, Markdown, aí eu tenho que aprender mais uma markup language. Mas, provavelmente vou me submeter a isso, o orgmode é sensacional para management em geral.\n","permalink":"https://www.perrotta.dev/2014/05/instalando-o-gentoo-a-partir-do-arch/","summary":"\u003cp\u003e\u003cstrong\u003eTL;DR\u003c/strong\u003e: pequena TODO list sobre como instalar o Gentoo. Você vai passar 90% do seu tempo olhando para \u003cstrong\u003etexto\u003c/strong\u003e dando scroll na tela (processo demorado…).\u003c/p\u003e","title":"Instalando o Gentoo a partir do Arch"},{"content":" Meu computador não boota! E agora? Possíveis sintomas: tela preta congelada, tela de splash congelada, systemd travado, upstart travado, corrupção (fsck não ficou satisfeito), bootloader (grub, syslinux, EFI) mal configurado (ou não configurado) Obter uma distro de Linux e gravá-la num USB Flash Drive (pendrive). Meu gosto pessoal: System rescue cd, Parted magic, Slitaz ou Arch Linux. Bootar a distro e imediatamente abrir um console / emulador de terminal. Com interface gráfica ou não, a gosto. fdisk -l para detectar os discos do computador. Detectar o disco cujo sistema está com problema. Geralmente o que contém a partição /boot ou /. Identificar as partições também é usualmente importante. Se você precisar formatar alguma partição, use cfdisk ou parted. Exemplo: cfdisk /dev/sda1. Se você precisar (re)criar algum filesystem, use mkfs (por exemplo, mkfs.ext4 -L \u0026quot;archroot\u0026quot; /dev/sda1). Para (re)montar o seu sistema de arquivos: (por exemplo) mount /dev/sda1 /mnt. chroot no sistema que você acabou de montar: chroot /mnt. Para recuperar (na verdade, gerar novamente) o arquivo de configuração do grub dentro do chroot: grub-mkconfig -o /boot/grub/grub.cfg. Para reinstalar o grub (fora do chroot!), use grub-install. Explorar o diretório /etc/systemd/system. Usualmente um desses passos é um caminho para resolver o problema. No final das contas, as coisas são bastante específicas, dependem do contexto.\n","permalink":"https://www.perrotta.dev/2014/04/recovery-t%C3%ADpico-via-usb/","summary":"Meu computador não boota! E agora? Possíveis sintomas: tela preta congelada, tela de splash congelada, systemd travado, upstart travado, corrupção (fsck não ficou satisfeito), bootloader (grub, syslinux, EFI) mal configurado (ou não configurado) Obter uma distro de Linux e gravá-la num USB Flash Drive (pendrive). Meu gosto pessoal: System rescue cd, Parted magic, Slitaz ou Arch Linux. Bootar a distro e imediatamente abrir um console / emulador de terminal. Com interface gráfica ou não, a gosto.","title":"Recovery típico via USB"},{"content":"Não imagino que seja incomum o seguinte cenário:\nNews: uma nova versão da distro Debisuse está disponível. Usuário: vou baixar a ISO, criar uma máquina virtual no VirtualBox (ou no VMWare, vai que), Bootar a ISO a partir dela. Isso tudo é muito mais prático do que gravar a ISO num Flash (Pen) Drive e então testá-la com um novo boot. No entanto, podemos ser mais práticos ainda se utilizarmos, para isso, um único comando, com o qemu! O comando típico é:\nqemu-system-x86_64 --enable-kvm -m 512M -cdrom ~/Downloads/debisuse-latest.iso Se sua arquitetura for de 32 bits, você vai querer qemu-system-i386. O parâmetro m regula a quantidade de memória a ser alocada para a máquina virtual.\nPara utilizar o QEMU, você vai precisar encontrar o pacote adequado na sua distro.\nNo Debian/Ubuntu e openSUSE: qemu e kvm (não testei, mas tudo indica que são esses) No Arch: qemu (veja https://wiki.archlinux.org/index.php/Kvm e https://wiki.archlinux.org/index.php/QEMU) OBS.: O KVM é para deixar a execução ainda mais rápida. Mas ele não é obrigatório, OK? Para poder usá-lo existe uma série de peculiaridades, tais como habilitar as opções de virtualização na sua BIOS (isso é mais comum em laptops) e assegurar-se de que o módulo adequado do kernel foi carregado (geralmente kvm\\_intel ou kvm\\_amd). Você pode conferir isso com o comando:\nlsmod | grep kvm Se ver alguma saída, existem boas chances de o módulo correto do KVM já ter sido carregado pelo seu kernel. Para fins de comparação, essa é a minha saída:\nkvm_intel 131191 3 kvm 388773 1 kvm_intel Happy hacking!\n","permalink":"https://www.perrotta.dev/2014/01/testando-uma-iso-no-linux-sem-o-virtualbox/","summary":"Não imagino que seja incomum o seguinte cenário:\nNews: uma nova versão da distro Debisuse está disponível. Usuário: vou baixar a ISO, criar uma máquina virtual no VirtualBox (ou no VMWare, vai que), Bootar a ISO a partir dela. Isso tudo é muito mais prático do que gravar a ISO num Flash (Pen) Drive e então testá-la com um novo boot. No entanto, podemos ser mais práticos ainda se utilizarmos, para isso, um único comando, com o qemu!","title":"Testando uma ISO no Linux sem o VirtualBox"}]