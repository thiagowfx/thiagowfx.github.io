[{"content":"Given, for example, 0 0 * * *, how do you figure out when it will run?\nOption 1: Read the docs! The ArchWiki is frequently a great reference. Alternatively, use your favorite search engine. Option 2: Ask ChatGPT! A simple cron: '0 0 * * * prompt is enough. No need to embezzle it with explain what this does or what does this do?. Option 3: Paste it into https://crontab.guru/. ","permalink":"https://www.perrotta.dev/2024/07/explain-a-crontab-expression/","summary":"\u003cp\u003eGiven, for example, \u003ccode\u003e0 0 * * *\u003c/code\u003e, how do you figure out when it will run?\u003c/p\u003e","title":"Explain a crontab expression"},{"content":"If you find yourself in a situation wherein http://localhost:1313 has issues, you can use a domain that redirects to localhost. For example:\nhttp://localdev.me:1313/ http://demo.localdev.me:1313/ When I’m doing local development, I sometimes need a domain name that routes back to localhost. I’ve long run into cases where I need subdomains and ended up modifying my local hosts file. I’ve used this for a variety of situations going back for a long time. From Kubernetes ingress work to web development.\nlocaldev.me DNS is served through amazon. The domain name and any subdomains point to 127.0.0.1.\nThe next time you need a custom domain or subdomain for local development, instead of hancking your hosts file you might consider localdev.me.\nSource: https://codeengineered.com/blog/2022/localdev-me/\n","permalink":"https://www.perrotta.dev/2024/07/localhost-domain/","summary":"\u003cp\u003eIf you find yourself in a situation wherein http://localhost:1313 has issues,\nyou can use a domain that redirects to localhost. For example:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://localdev.me:1313/\"\u003ehttp://localdev.me:1313/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://demo.localdev.me:1313/\"\u003ehttp://demo.localdev.me:1313/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","title":"Localhost domain"},{"content":"Let\u0026rsquo;s say the files you want to copy are in ~/Downloads.\nStart a local HTTP server on your laptop:\n$ cd ~/Downloads $ python3 -m http.server Serving HTTP on :: port 8000 (http://[::]:8000/) ... Find the IP address of your laptop within your LAN:\n$ ifconfig # macOS $ ip addr # linux Now go to your Steam Deck, access http://\u0026lt;ip\u0026gt;:8000 via the installed web browser, and download your files.\nAlternatively, run wget / curl in a terminal.\n","permalink":"https://www.perrotta.dev/2024/06/copy-files-from-laptop-to-steam-deck/","summary":"\u003cp\u003eLet\u0026rsquo;s say the files you want to copy are in \u003ccode\u003e~/Downloads\u003c/code\u003e.\u003c/p\u003e","title":"Copy files from laptop to Steam Deck"},{"content":"When asking questions or filing bugs / feature requests in the internet, I often refer to these two invaluable resources:\nHow To Ask Questions The Smart Way by Eric S. Raymond XY problem: The XY problem is asking about your attempted solution rather than your actual problem. This leads to enormous amounts of wasted time and energy, both on the part of people asking for help, and on the part of those providing help. Here is a recent example: https://github.com/23andMe/Yamale/issues/250\nhttps://xyproblem.info / Motivation: Somewhat related to #228: It\u0026rsquo;s not currently possible to exclude files from a given directory. In the context of developing Kubernetes GitOps repository this is often an issue. The ability to run yamale on individual files would address it.\n","permalink":"https://www.perrotta.dev/2024/06/xy-problem/","summary":"\u003cp\u003eWhen asking questions or filing bugs / feature requests in the internet, I often\nrefer to these two invaluable resources:\u003c/p\u003e","title":"★ XY problem"},{"content":"Whenever the files are in the same directory, vidir from moreutils is the best interactive tool.\nIf files are scattered across multiple directories, consider using the rename utility from util-linux.\nA simple example to rename all readme.md files to README.md for consistency:\nrename \u0026#39;s/readme\\.md/README.md/\u0026#39; **/* ","permalink":"https://www.perrotta.dev/2024/06/rename-files-in-bulk/","summary":"\u003cp\u003eWhenever the files are in the same directory, \u003ccode\u003evidir\u003c/code\u003e from \u003ca href=\"https://www.perrotta.dev/2022/05/tools-you-should-know-about-moreutils/\"\u003emoreutils\u003c/a\u003e is the best\ninteractive tool.\u003c/p\u003e\n\u003cp\u003eIf files are scattered across multiple directories, consider using the\n\u003ca href=\"https://man.archlinux.org/man/rename.1.en\"\u003e\u003ccode\u003erename\u003c/code\u003e\u003c/a\u003e utility from \u003ccode\u003eutil-linux\u003c/code\u003e.\u003c/p\u003e","title":"Rename files in bulk"},{"content":"The following commands will make the shell sleep indefinitely:\nsleep inf sleep infinity Previously I would call a command such as a while true loop or the yes utility, but sleep is also handy.\n","permalink":"https://www.perrotta.dev/2024/06/sleep-forever/","summary":"\u003cp\u003eThe following commands will make the shell sleep indefinitely:\u003c/p\u003e","title":"Sleep forever"},{"content":"Given the password correct horse battery staple, we would like to bcrypt-hash it.\nXKCD Courtesy of Randall Munroe\nHere\u0026rsquo;s one way to do so via the command line:\n$ htpasswd -nbBC 10 \u0026#34;\u0026#34; \u0026#39;correct horse battery staple\u0026#39; | tr -d \u0026#39;:\\n\u0026#39; | sed \u0026#39;s/$2y/$2a/\u0026#39; \u0026hellip;which yields:\n$2a$10$HKSHfLu4l7TvOmnLkhUngu2U1pJUUw7hEU0LE1iN84S09fJsZowHm You could verify it matches e.g. via https://bcrypt-generator.com/.\nContext: ArgoCD expects a bcrypt-hashed password in its config file.\n","permalink":"https://www.perrotta.dev/2024/06/bcrypt-hash-a-password/","summary":"\u003cp\u003eGiven the password \u003ccode\u003ecorrect horse battery staple\u003c/code\u003e, we would like to bcrypt-hash\nit.\u003c/p\u003e\n\u003cfigure class=\"align-center \"\u003e\u003ca href=\"https://xkcd.com/936/\"\u003e\n    \u003cimg loading=\"lazy\" src=\"https://imgs.xkcd.com/comics/password_strength.png#center\"\n         alt=\"Through 20 years of effort, we\u0026#39;ve successfully trained everyone to use passwords that are hard for humans to remember, but easy for computers to guess.\"/\u003e \u003c/a\u003e\u003cfigcaption\u003e\n            \u003cp\u003eXKCD Courtesy of Randall Munroe\u003c/p\u003e\n        \u003c/figcaption\u003e\n\u003c/figure\u003e","title":"Bcrypt-hash a password"},{"content":"This post exemplifies an efficient workflow to shorten the edit-refresh loop when dealing with github actions.\nAssumptions You have git, jq and the gh command-line tool installed (brew install gh) You have a github actions file in ~/.github/workflows/package-release-dispatch.yml. The action has the following form: on: workflow_dispatch: inputs: ref: description: \u0026#39;Ref to build from. This can either be a SHA or a branch/tag\u0026#39; required: true type: string push: branches: - master paths: - \u0026#39;.github/workflows/package-release-dispatch.yml\u0026#39; - \u0026#39;helm/**\u0026#39; The goal is to iterate on the jobs: section of the action.\nWorkflow Trigger an action run from the command line:\ngh workflow run package-release-dispatch.yml [--ref master] [-f ref=master] --ref should be the git branch you\u0026rsquo;re working on, for example, thiagowfx/my-cool-feature.\n-f provides an input to the workflow. In this case, there\u0026rsquo;s an input named ref, which is meant to be the branch the action will act upon.\nThe action takes a little while to trigger, we can sleep to give it some time. I found that sleep 3 is a sensible value (3 seconds).\nHow to view the action?\nFirst we need to get its ID. Here\u0026rsquo;s one way to do so:\ngh run list --workflow=package-release-dispatch.yml --json databaseId --jq \u0026#39;.[0].databaseId\u0026#39; Option 1) Via the command-line gh run watch \u0026lt;id\u0026gt; It\u0026rsquo;s analogous to watch, continuously refreshing the action progress in the background, step by step. Example:\n* master Package release dispatch · 12345678 Triggered via workflow_dispatch about 1 minute ago JOBS * build (ID 23456789) ✓ Set up job ✓ Checkout source code ✓ Run azure/setup-helm@v4 ✓ Install yq ✓ Install helm cm-push plugin ✓ Set up Helm repos * Helm package all charts sans blacklist * Publish all helm packages * Post Checkout source code Option 2) Via the web browser gh run view \u0026lt;id\u0026gt; -w It will open the system web browser in the right page, pertaining to the action run.\nPutting everything together Option 1) gh workflow run package-release-dispatch.yml --ref master -f ref=master \u0026amp;\u0026amp; \\ sleep 3 \u0026amp;\u0026amp; \\ gh run watch $(gh run list --workflow=package-release-dispatch.yml --json databaseId --jq \u0026#39;.[0].databaseId\u0026#39;) Option 2) gh workflow run package-release-dispatch.yml --ref master -f ref=master \u0026amp;\u0026amp; \\ sleep 3 \u0026amp;\u0026amp; \\ gh run view $(gh run list --workflow=package-release-dispatch.yml --json databaseId --jq \u0026#39;.[0].databaseId\u0026#39;) -w ","permalink":"https://www.perrotta.dev/2024/05/github-actions-workflow/","summary":"\u003cp\u003eThis post exemplifies an efficient workflow to shorten the edit-refresh loop\nwhen dealing with github actions.\u003c/p\u003e","title":"★ Github actions workflow"},{"content":"A common scenario: there\u0026rsquo;s a new deployment you would like to roll out to AWS. Let\u0026rsquo;s say you pick \u0026ldquo;us-east-1\u0026rdquo; as your cloud region. There are multiple availability zones within it:\nus-east-1a us-east-1b us-east-1c us-east-1d us-east-1e us-east-1f Suppose you want to pick two of them for your service/app, and you don\u0026rsquo;t particularly care about which one. How to proceed?\nOption #1: Hard-coding Pick two arbitrary zones and hard-code them.\nvariable \u0026#34;availability_zones\u0026#34; { type = list(string) default = [\u0026#34;us-east-1a\u0026#34;, \u0026#34;us-east-1b\u0026#34;] } resource \u0026#34;aws_subnet\u0026#34; \u0026#34;private\u0026#34; { vpc_id = aws_vpc.chartmuseum.id cidr_block = element(var.private_subnets, count.index) availability_zone = element(var.availability_zones, count.index) count = length(var.private_subnets) } Caveat: The paradox of choice, unnecessary decision fatigue.\nOption #2: Pick the first two Use the AWS data source to dynamically find all zones, and pick the first two.\ndata \u0026#34;aws_availability_zones\u0026#34; \u0026#34;available\u0026#34; { state = \u0026#34;available\u0026#34; } resource \u0026#34;aws_subnet\u0026#34; \u0026#34;private\u0026#34; { vpc_id = aws_vpc.chartmuseum.id cidr_block = element(var.private_subnets, count.index) availability_zone = element(data.aws_availability_zones.available.names, count.index) count = length(var.private_subnets) } Note that terraform plan should display the full zone list.\nCaveat: Heavily biased towards the first two zones.\nOption #3: Random shuffling Pick two zones at random!\ndata \u0026#34;aws_availability_zones\u0026#34; \u0026#34;available\u0026#34; { state = \u0026#34;available\u0026#34; } resource \u0026#34;random_shuffle\u0026#34; \u0026#34;aws_availability_zone_names\u0026#34; { input = data.aws_availability_zones.available.names result_count = 2 } resource \u0026#34;aws_subnet\u0026#34; \u0026#34;private\u0026#34; { vpc_id = aws_vpc.chartmuseum.id cidr_block = element(var.private_subnets, count.index) availability_zone = element(random_shuffle.aws_availability_zone_names.result, count.index) count = length(var.private_subnets) } Winner: In my opinion, this is the most elegant approach.\nrandom_shuffle will output the selected regions upon running terraform apply.\n","permalink":"https://www.perrotta.dev/2024/05/terraform-aws-deployment-to-random-availability-zones/","summary":"\u003cp\u003eA common scenario: there\u0026rsquo;s a new deployment you would like to roll out to AWS.\nLet\u0026rsquo;s say you pick \u0026ldquo;us-east-1\u0026rdquo; as your cloud region. There are multiple\navailability zones within it:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eus-east-1a\u003c/li\u003e\n\u003cli\u003eus-east-1b\u003c/li\u003e\n\u003cli\u003eus-east-1c\u003c/li\u003e\n\u003cli\u003eus-east-1d\u003c/li\u003e\n\u003cli\u003eus-east-1e\u003c/li\u003e\n\u003cli\u003eus-east-1f\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSuppose you want to pick two of them for your service/app, and you don\u0026rsquo;t\nparticularly care about which one. How to proceed?\u003c/p\u003e","title":"Terraform: AWS deployment to random availability zones"},{"content":"Assume that you have a Chartmuseum container running in AWS Fargate.\nChartmuseum is a repository for helm charts. AWS Fargate is an Amazon service to run containers (\u0026ldquo;serverless\u0026rdquo;), being part of ECS (Elastic Container Service).\nProblem statement: Add a container healthcheck to the chartmuseum task definition associated with the container.\nThe official docs suggest using curl:\n[\u0026#34;CMD-SHELL\u0026#34;, \u0026#34;curl -f http://localhost/ || exit 1\u0026#34;] For Chartmuseum specifically we\u0026rsquo;re interested in its /health endpoint, as per this reference:\n[\u0026#34;CMD-SHELL\u0026#34;, \u0026#34;curl -f http://localhost/health || exit 1\u0026#34;] But we\u0026rsquo;re using port 8080:\n[\u0026#34;CMD-SHELL\u0026#34;, \u0026#34;curl -f http://localhost:8080/health || exit 1\u0026#34;] If you use this healthcheck for the official chartmuseum image (ghcr.io/helm/chartmuseum) it will fail, because the Alpine Linux environment it uses does not contain curl.\nA straightforward fix is to use wget instead:\n[\u0026#34;CMD-SHELL\u0026#34;, \u0026#34;wget -q --spider http://localhost:8080/health || exit 1\u0026#34;] --spider is needed because we do not want to download anything, -q is optional and short for \u0026ldquo;quiet\u0026rdquo;.\nThe /health endpoint merely returns a simple JSON:\n{\u0026#34;healthy\u0026#34;:true} References: https://stackoverflow.com/questions/47722898/how-can-i-make-a-docker-healthcheck-with-wget-instead-of-curl\n","permalink":"https://www.perrotta.dev/2024/05/adding-a-healthcheck-to-chartmuseum-in-aws-fargate/","summary":"\u003cp\u003eAssume that you have a \u003ca href=\"https://chartmuseum.com/\"\u003eChartmuseum\u003c/a\u003e container running\nin \u003ca href=\"https://aws.amazon.com/fargate/\"\u003eAWS Fargate\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eChartmuseum is a repository for helm charts. AWS Fargate is an Amazon service to\nrun containers (\u0026ldquo;serverless\u0026rdquo;), being part of ECS (Elastic Container Service).\u003c/p\u003e\n\u003cp\u003eProblem statement: Add a container \u003cem\u003ehealthcheck\u003c/em\u003e to the chartmuseum task\ndefinition associated with the container.\u003c/p\u003e","title":"Adding a healthcheck to chartmuseum in AWS Fargate"},{"content":"Pritunl is an Enterprise Distributed OpenVPN server.\nIn order to run its client on macOS, an .app is provided. However, using it to log in every day is quite tedious.\nLet\u0026rsquo;s automate it so that we can connect to the VPN with a single command.\nIn order to fetch the credentials in this example in a secure manner, we\u0026rsquo;ll be using 1password. pass would have worked just as fine.\nFirst, configure Pritunl via its app UI, creating a profile for your corp credentials.\nThen figure out what the profile ID you just created is:\nprofile_id=$(/Applications/Pritunl.app/Contents/Resources/pritunl-client list --json | jq -r \u0026#39;.[0].id\u0026#39;) Now, create an entry in 1Password for your Pritunl credentials associated with the profile above.\nLet\u0026rsquo;s use the op official CLI tool from 1Password to fetch the password and the OTP (one-time password) for the \u0026ldquo;Pritunl (VPN)\u0026rdquo; entry (change it accordingly).\nop_id=\u0026#34;$(op item get \u0026#39;Pritunl (VPN)\u0026#39; --format json | jq -r \u0026#39;.id\u0026#39;)\u0026#34; password=\u0026#34;$(op read \u0026#34;op://private/$op_id/password\u0026#34;)\u0026#34; otp=\u0026#34;$(op item get \u0026#34;$op_id\u0026#34; --totp)\u0026#34; Now we can use the pritunl-client to log in programatically:\npritunl-client start \u0026#34;$profile_id\u0026#34; --password \u0026#34;$password$otp\u0026#34; Then verify it has indeed connected:\npritunl-client list The trick is that it accepts the concatenation of the password with the OTP as the password. There\u0026rsquo;s not a separate --otp flag.\nPutting everything together, we can create a function for our favorite shell:\n# Log into corp VPN pritunl_login() { local profile_id=$(/Applications/Pritunl.app/Contents/Resources/pritunl-client list --json | jq -r \u0026#39;.[0].id\u0026#39;) local op_id=\u0026#34;$(op item get \u0026#39;Pritunl (VPN)\u0026#39; --format json | jq -r \u0026#39;.id\u0026#39;)\u0026#34; local password=\u0026#34;$(op read \u0026#34;op://private/$op_id/password\u0026#34;)\u0026#34; local otp=\u0026#34;$(op item get \u0026#34;$op_id\u0026#34; --totp)\u0026#34; pritunl-client start \u0026#34;$profile_id\u0026#34; --password \u0026#34;$password$otp\u0026#34; pritunl-client list } ","permalink":"https://www.perrotta.dev/2024/05/pritunl-log-in-via-cli/","summary":"\u003cp\u003e\u003ca href=\"https://pritunl.com/\"\u003ePritunl\u003c/a\u003e is an Enterprise Distributed OpenVPN server.\u003c/p\u003e\n\u003cp\u003eIn order to run its client on macOS, an \u003ccode\u003e.app\u003c/code\u003e is provided. However, using it to\nlog in every day is quite tedious.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s automate it so that we can connect to the VPN with a single command.\u003c/p\u003e","title":"Pritunl log in via CLI"},{"content":"If you have a GitHub account configured with SSH, your public keys are available at https://github.com/$USERNAME.keys.\nFor example, mine: https://github.com/thiagowfx.keys\nAnd then let\u0026rsquo;s say you also use your full name on GitHub.\n% ssh whoami.filippo.io The authenticity of host \u0026#39;whoami.filippo.io (2a09:8280:1::a:5d6)\u0026#39; can\u0026#39;t be established. ED25519 key fingerprint is SHA256:qGAqPqtlvFBCt4LfMME3IgJqZWlcrlBMxNmGjhLVYzY. This key is not known by any other names. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added \u0026#39;whoami.filippo.io\u0026#39; (ED25519) to the list of known hosts. +---------------------------------------------------------------------+ | | | _o/ Hello Thiago Perrotta! | | | | | Did you know that ssh sends all your public keys to any server | | it tries to authenticate to? | | | | We matched them to the keys of your GitHub account, | | @thiagowfx, which are available via the GraphQL API | and at https://github.com/thiagowfx.keys | | | -- Filippo (https://filippo.io) | | | | | | P.S. The source of this server is at | | https://github.com/FiloSottile/whoami.filippo.io | | | +---------------------------------------------------------------------+ Shared connection to whoami.filippo.io closed. Then be careful when connecting to random public ssh servers when you have an intent to be anonymous.\nIt\u0026rsquo;s possible to \u0026ldquo;hide\u0026rdquo; yourself by either setting the IdentitiesOnly=yes option, or by removing all your local ssh keys altogether, even if only temporarily.\n","permalink":"https://www.perrotta.dev/2024/05/be-aware-that-your-public-ssh-keys-can-reveal-your-identity/","summary":"\u003cp\u003eIf you have a GitHub account configured with SSH, your public keys are available\nat \u003ccode\u003ehttps://github.com/$USERNAME.keys\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eFor example, mine: \u003ca href=\"https://github.com/thiagowfx.keys\"\u003ehttps://github.com/thiagowfx.keys\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAnd then let\u0026rsquo;s say you also use your full name on GitHub.\u003c/p\u003e","title":"Be aware that your public SSH keys can reveal your identity"},{"content":"I learned a neat shell trick this week. In both bash and zsh you can use the circumflex / caret (^) symbol to find \u0026amp; replace a word from the previous command.\nUsage: ^prev^next. It\u0026rsquo;s best illustrated with examples:\nterraform Instead of running:\nterraform init -var-file /path/to/foo.tfvars terraform plan -var-file /path/to/foo.tfvars terraform apply -var-file /path/to/foo.tfvars Run:\nterraform init -var-file /path/to/foo.tfvars ^init^plan ^plan^apply systemd Instead of running:\nsudo systemctl restart nginx sudo systemctl status nginx Run:\nsudo systemctl restart nginx ^restart^status one observation zsh will run the substitution right away, whereas bash will allow you to review and edit the replaced command before running it.\n","permalink":"https://www.perrotta.dev/2024/04/shell-text-substitution/","summary":"\u003cp\u003eI learned a neat shell trick this week. In both \u003ccode\u003ebash\u003c/code\u003e and \u003ccode\u003ezsh\u003c/code\u003e you can use the\ncircumflex / caret (\u003ccode\u003e^\u003c/code\u003e) symbol to find \u0026amp; replace a word from the previous\ncommand.\u003c/p\u003e\n\u003cp\u003eUsage: \u003ccode\u003e^prev^next\u003c/code\u003e. It\u0026rsquo;s best illustrated with examples:\u003c/p\u003e","title":"Shell text substitution"},{"content":"When working on Python projects, pyenv is a great python environment / version manager, especially on macOS wherein you cannot easily control the python system version.\nI\u0026rsquo;d recommend to install it with homebrew (brew install pyenv).\nThe upstream documentation is great. The commands you\u0026rsquo;ll typically use are:\npyenv versions: list all installed versions pyenv global \u0026lt;version\u0026gt;: set a specific python version for your whole system pyenv local: set a specific python version only for a specific project (directory) And then it\u0026rsquo;s handy to add the following blurb to your shell rc file to make pyenv work properly out-of-the-box:\n# pyenv: https://github.com/pyenv/pyenv if hash pyenv \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; then export PYENV_ROOT=\u0026#34;$HOME/.pyenv\u0026#34; path_munge \u0026#34;$PYENV_ROOT/bin\u0026#34; eval \u0026#34;$(pyenv init -)\u0026#34; fi Note that path_munge is a custom function, it merely appends the given argument to the $PATH.\n","permalink":"https://www.perrotta.dev/2024/03/pyenv/","summary":"\u003cp\u003eWhen working on Python projects, \u003ca href=\"https://github.com/pyenv/pyenv\"\u003e\u003ccode\u003epyenv\u003c/code\u003e\u003c/a\u003e is a\ngreat python environment / version manager, especially on macOS wherein you\ncannot easily control the python system version.\u003c/p\u003e","title":"pyenv"},{"content":"I used to have the following handy script to launch a new (fresh!) instance of Google Chrome when working on Chrome for Testing in the Browser Automation team at Google:\n#!/bin/bash # start chrome with ephemeral settings (every run of Chrome is empty) # usage: chrome-fresh out/Default/chrome TMPDIR=\u0026#34;$(mktemp -d)\u0026#34; trap \u0026#39;rm -rf \u0026#34;${TMPDIR}\u0026#34;\u0026#39; EXIT CHROME=\u0026#34;${1:-google-chrome}\u0026#34;; shift # https://github.com/GoogleChrome/chrome-launcher/blob/main/docs/chrome-flags-for-tools.md CHROME_FLAGS=\u0026#34;--use-mock-keychain\u0026#34; if [[ \u0026#34;$(uname -s)\u0026#34; == \u0026#34;Darwin\u0026#34; \u0026amp;\u0026amp; \u0026#34;$CHROME\u0026#34; == *.app ]]; then open -n \u0026#34;$CHROME\u0026#34; --args \u0026#34;$CHROME_FLAGS\u0026#34; --user-data-dir=\u0026#34;$TMPDIR\u0026#34; \u0026#34;$@\u0026#34; else # \u0026#34;Linux\u0026#34; \u0026#34;$CHROME\u0026#34; \u0026#34;$CHROME_FLAGS\u0026#34; --user-data-dir=\u0026#34;$TMPDIR\u0026#34; \u0026#34;$@\u0026#34; fi The script is self-documenting, it was properly tested on both Linux and macOS.\nThe typical use case would be to compile a new Google Chrome binary (/out/Default/chrome), and then use the script to launch it with a fresh user data directory, to ensure the previous launch settings do not interfere with the current one.\n","permalink":"https://www.perrotta.dev/2024/03/chrome-fresh-start-a-fresh-instance-of-google-chrome/","summary":"\u003cp\u003eI used to have the following handy script to launch a new (fresh!) instance of\nGoogle Chrome when working on \u003ca href=\"https://www.perrotta.dev/2024/01/google-chrome-for-testing-reliable-downloads-for-browser-automation/\"\u003eChrome for Testing\u003c/a\u003e in the Browser Automation team at\nGoogle:\u003c/p\u003e","title":"chrome-fresh: start a fresh instance of Google Chrome"},{"content":"I changed my macOS system language to German, with the intent of getting more exposure to it.\nOne negative side effect is that most binaries I execute with my shell (for example: git) are now outputting German text as well1. In hindsight, this should have been expected.\nA simple fix is to override the environment locale with English. I made the following addition to my dotfiles:\n# Force the system-wide language to English. # Both \u0026#34;en_US\u0026#34; and \u0026#34;en_CA\u0026#34; work here. # We could also optionally set LC_ALL but it is not necessary. # Verify current locale settings with `locale`. export LANG=\u0026#34;en_US\u0026#34; You could argue that it\u0026rsquo;s actually desirable to have terminal applications output German. However, nothing beats the standardization, consistency and familiarity of English for developer tooling. If I really need to have this extreme exposure one day, it\u0026rsquo;s always possible to just unset LANG.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/02/set-terminal-language-to-english/","summary":"\u003cp\u003eI changed my macOS system language to German, with the intent of \u003ca href=\"https://www.perrotta.dev/2022/04/translating-german-to-english/\"\u003egetting more\nexposure to it\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOne negative side effect is that most binaries I execute with my shell (for\nexample: \u003ccode\u003egit\u003c/code\u003e) are now outputting German text as well\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e. In hindsight, this\nshould have been expected.\u003c/p\u003e","title":"Set terminal language to English"},{"content":"Sometimes, when connecting to public Wi-Fi hotposts, especially in airports and coffee shops, the captive portal gateway required to get internet access will simply not show up.\nThe simplest way to force it to do so is to turn the Wi-Fi off and on again.\nShould it not work, the second way is to open your favorite browser and navigate to captive.apple.com (macOS, iOS).\n","permalink":"https://www.perrotta.dev/2024/02/force-captive-portal-to-open/","summary":"\u003cp\u003eSometimes, when connecting to public Wi-Fi hotposts, especially in airports and\ncoffee shops, the captive portal gateway required to get internet access will\nsimply not show up.\u003c/p\u003e","title":"Force captive portal to open"},{"content":"https://organicmaps.app/:\nOrganic Maps is a free Android \u0026amp; iOS offline maps app for travelers, tourists, hikers, drivers and cyclists based on OpenStreetMap data created by the community. It is a privacy-focused, open-source fork of [\u0026hellip;]\nOrganic Maps is one of the few applications nowadays that supports 100% of features without an active Internet connection. Install Organic Maps, download maps, throw away your SIM card, and go for a weeklong trip on a single battery charge without any byte sent to the network.\nPositive impressions:\nworks great on iOS (didn\u0026rsquo;t test on Android yet) a great companion for cycling within the city more fun and more lightweight than Google Maps low impact on device battery life extremely privacy friendly (no ads, no tracking, no push notifications, etc) extremely mobile data friendly (works fully offline) open-source, and without any IAP works out-of-the-box, no sign up necessary Negative impressions:\ndo not use it for public transit, it is terrible as it does not have real-time data integration ","permalink":"https://www.perrotta.dev/2024/02/organic-maps-off-line-maps/","summary":"\u003cp\u003e\u003ca href=\"https://organicmaps.app/\"\u003ehttps://organicmaps.app/\u003c/a\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eOrganic Maps is a free Android \u0026amp; iOS offline maps app for travelers,\ntourists, hikers, drivers and cyclists based on OpenStreetMap data created\nby the community. It is a privacy-focused, open-source fork of [\u0026hellip;]\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003eOrganic Maps is one of the few applications nowadays that supports 100% of\nfeatures without an active Internet connection. Install Organic Maps,\ndownload maps, throw away your SIM card, and go for a weeklong trip on a\nsingle battery charge without any byte sent to the network.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003ePositive impressions:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eworks great on iOS (didn\u0026rsquo;t test on Android yet)\u003c/li\u003e\n\u003cli\u003ea great companion for \u003cstrong\u003ecycling\u003c/strong\u003e within the city\u003c/li\u003e\n\u003cli\u003emore fun and more lightweight than Google Maps\u003c/li\u003e\n\u003cli\u003elow impact on device battery life\u003c/li\u003e\n\u003cli\u003eextremely privacy friendly (no ads, no tracking, no push notifications, etc)\u003c/li\u003e\n\u003cli\u003eextremely mobile data friendly (works fully offline)\u003c/li\u003e\n\u003cli\u003eopen-source, and without any IAP\u003c/li\u003e\n\u003cli\u003eworks out-of-the-box, no sign up necessary\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNegative impressions:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003edo not use it for public transit, it is terrible as it does not have\nreal-time data integration\u003c/li\u003e\n\u003c/ul\u003e","title":"Organic Maps: off-line maps"},{"content":"I got a new domain! This blog is now available on https://www.perrotta.dev/. There\u0026rsquo;s also a redirect to it from https://blog.perrotta.dev/.\nI don\u0026rsquo;t know which one I like best, so www is the canonical subdomain for now. Feel free to update the RSS in your feed reader, although https://thiagowfx.github.io/ should keep working for a little longer, until (if ever) I decide to migrate the static hosting off Github Pages.\nThis is a project I wanted to do since ages ago, and my goals go beyond merely making my blog available under it. Stay tuned for more updates.\n","permalink":"https://www.perrotta.dev/2024/02/new-domain/","summary":"\u003cp\u003eI got a new domain! This blog is now available on \u003ca href=\"https://www.perrotta.dev/\"\u003ehttps://www.perrotta.dev/\u003c/a\u003e.\nThere\u0026rsquo;s also a redirect to it from \u003ca href=\"https://blog.perrotta.dev/\"\u003ehttps://blog.perrotta.dev/\u003c/a\u003e.\u003c/p\u003e","title":"New domain"},{"content":"I got myself a brand new domain! As I play with it, expect documentation to be added.\nHow to query the WHOIS for the domain?\nFrom the command line: $ whois \u0026lt;domain\u0026gt; From the registrar WHOIS, e.g. https://porkbun.com/whois, https://www.gandi.net/en/domain/p/whois From the registry WHOIS, e.g. https://lookup.icann.org/, https://www.registry.google/whois-lookup/ It\u0026rsquo;s a good idea to set up WHOIS privacy, so that your domain registration details stay private. Some registrars such as Porkbun and NearlyFreeSpeech will gladly offer an option for that, either for free or at a low cost, respectively.\n","permalink":"https://www.perrotta.dev/2024/02/whois/","summary":"\u003cp\u003eI got myself a brand new domain! As I play with it, expect documentation to\nbe added.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eHow to query the WHOIS for the domain?\u003c/strong\u003e\u003c/p\u003e","title":"WHOIS"},{"content":"The more time you spent playing with Anki, the more opinionated you become.\nUpon reading Fluent Forever by Gabriel Wyner, I got an itch to create my own Anki note template for learning languages.\nThe template There\u0026rsquo;s no point explaining how to create a new template; the excellent Anki documentation already does so. Instead, I\u0026rsquo;ll just list and explain the template I created.\nThe template is called \u0026ldquo;Deutsch Language Card 🇩🇪\u0026rdquo;. It has four fields:\nFront Front Example Back Striked Front and Back come from the built-in template. There\u0026rsquo;s nothing special about them. I use \u0026ldquo;Front\u0026rdquo; for the canonical term in the foreign language I\u0026rsquo;m learning, and \u0026ldquo;Back\u0026rdquo; for the explanation in the base / native language I\u0026rsquo;m mostly familiar with1.\nHere\u0026rsquo;s an example:\nFront: das Buch Back: book 📚 Whenever possible I include one or more emojis 😃 in the \u0026ldquo;Back\u0026rdquo; field.\nThe canonicalization of the \u0026ldquo;Front\u0026rdquo; field is important, and one of the best (key, even!) features of Anki. It will smartly detect (and prevent!) duplicates from being created. It is case sensitive, therefore it\u0026rsquo;s important to create one convention and stick to it.\n\u0026ldquo;Front Example\u0026rdquo; is used to complement the \u0026ldquo;Front\u0026rdquo; field. It consists of one or both of the following:\nA phrase or sentence containing the Front term. A picture representing the Front term. To increase overall retention, it\u0026rsquo;s always best to add cues familiar to your context.\nAdd phrases that resonate with you or that you find in textbooks or blog posts that resonate with you. In my experience, adding random phrases is not effective.\nAdd images that represent well that you\u0026rsquo;re describing and that resonate with you. Photos that you take yourself are also fair game!\n\u0026ldquo;Striked\u0026rdquo; is to disambiguate synonyms or false cognates. For example:\nFront: der Sturm Back: storm ⛈️ Striked: das Gewitter, das Unwetter When I am reviewing the Back card, I want to cue myself not to think about the striked terms.\nThe source code The Front card {{Front}} {{tts de_DE:Front}} {{#Front Example}} \u0026lt;br\u0026gt; \u0026lt;i\u0026gt;{{Front Example}}\u0026lt;/i\u0026gt; {{tts de_DE:Front Example}} {{/Front Example}} The front card includes a text-to-speech sample that is generated on-the-fly. It works very well on macOS and iOS. In fact, that\u0026rsquo;s main reason why the template is called \u0026ldquo;Deutsch Language Card\u0026rdquo; instead of just \u0026ldquo;Language Card\u0026rdquo;. The text-to-speech engine is customized to have an accent in the given language. For (High) German, that is de_DE.\nThe Back card {{Back}} {{#Striked}} \u0026lt;br\u0026gt;\u0026lt;br\u0026gt; \u0026lt;s\u0026gt;{{Striked}}\u0026lt;/s\u0026gt; {{/Striked}} The striked terms are striked, as you would expect.\nInterestingly I prefer to use English most of the time, even though it is not my mother tongue.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/02/anki-custom-language-card/","summary":"\u003cp\u003eThe more time you spent playing with \u003ca href=\"https://apps.ankiweb.net/\"\u003eAnki\u003c/a\u003e, the\nmore opinionated you become.\u003c/p\u003e\n\u003cp\u003eUpon reading \u003ca href=\"https://fluent-forever.com/index.html\"\u003eFluent Forever\u003c/a\u003e by Gabriel\nWyner, I got an itch to create my own Anki note template for learning\nlanguages.\u003c/p\u003e","title":"Anki: custom language card"},{"content":"This post is a follow-up of Terraforming a Linode: hello world.\nIn a future post, we will continue from here by using Ansible to install and set up Miniflux in our new Linode.\nBefore we extensively use Ansible to configure our VPS instance, first let\u0026rsquo;s set up a basic integration between Terraform and Ansible.\nFirst of all, here\u0026rsquo;s an overview of where I stopped last time. There were a couple of lightweight modifications since then. I\u0026rsquo;ll explain some of them below.\n% cat variables.tf variable \u0026#34;github_username\u0026#34; { type = string default = \u0026#34;thiagowfx\u0026#34; } variable \u0026#34;linode_hostname\u0026#34; { type = string default = \u0026#34;coruscant\u0026#34; } variable \u0026#34;linode_region\u0026#34; { type = string default = \u0026#34;eu-central\u0026#34; } All variables were moved to a variables.tf file. This is to follow standard terraform conventions / recommendations for module structures. Furthermore, it becomes easier to manage variables when they are all stored in a single place.\nThe main module file now looks like this:\n% cat main.tf terraform { required_providers { http = { source = \u0026#34;hashicorp/http\u0026#34; } linode = { source = \u0026#34;linode/linode\u0026#34; } } } provider \u0026#34;linode\u0026#34; {} data \u0026#34;http\u0026#34; \u0026#34;github_keys\u0026#34; { url = \u0026#34;https://api.github.com/users/${var.github_username}/keys\u0026#34; } locals { keys = jsondecode(data.http.github_keys.response_body)[*].key } resource \u0026#34;linode_instance\u0026#34; \u0026#34;nanode\u0026#34; { type = \u0026#34;g6-nanode-1\u0026#34; image = \u0026#34;linode/alpine3.19\u0026#34; label = var.linode_hostname region = var.linode_region authorized_keys = local.keys backups_enabled = \u0026#34;false\u0026#34; booted = \u0026#34;true\u0026#34; watchdog_enabled = \u0026#34;true\u0026#34; } I removed the token from the linode provider. Now it is supplied via the LINODE_TOKEN environment variable. In order to automatically populate that variable, I use direnv. There\u0026rsquo;s an .envrc file that provides its value, like so:\n#!/bin/sh # terraform init export LINODE_TOKEN=\u0026#34;my-token-here\u0026#34; I also created a repository for this project: https://github.com/thiagowfx/knol. That\u0026rsquo;s enough for preliminaries, now let\u0026rsquo;s go back to Ansible.\nThe first component we\u0026rsquo;ll need is an Ansible inventory file, containing the IP address of the host we\u0026rsquo;ll manage. It could look like this:\n[all] 1.2.3.4 ansible_user=root \u0026hellip;wherein 1.2.3.4 is the IP address of our VPS.\nThat said, due to the fact the VPS instance is created dynamically, maintaining that IP address manually would be tedious. Therefore, let\u0026rsquo;s have Terraform manage it.\nWe can do so with a local_file. Heck, we could even use a template_file, however it would be overkill as there are only two simple lines in our inventory at this point. A local_file is created upon terraform apply and deleted upon terraform destroy. Therefore it doesn\u0026rsquo;t even need to be tracked by our VCS:\nresource \u0026#34;local_file\u0026#34; \u0026#34;ansible_inventory\u0026#34; { content = \u0026lt;\u0026lt;-EOF [all] ${linode_instance.nanode.ip_address} ansible_user=root EOF filename = \u0026#34;inventory.ini\u0026#34; file_permission = \u0026#34;0644\u0026#34; } Once we run terraform (plan + apply), an inventory.ini file should be created with the above contents.\nBecause the IP address is ephemeral and dynamic, we should have a straightforward way to see its value. A terraform output is perfect for that:\n% cat outputs.tf output \u0026#34;ip_address\u0026#34; { value = linode_instance.nanode.ip_address } Later on (after terraforming) we will be able to use terraform output to see the server IP address.\nWe have the inventory file. Now we need a playbook. A playbook contains a sequence of tasks to be applied to our server.\nLet\u0026rsquo;s start with a basic playbook that just installs and starts nginx:\n--- - hosts: all tasks: - name: Install the web server (nginx) community.general.apk: name: nginx state: present - name: Start the web server service: name: nginx state: started Save this to a playbook.yml file.\nAfter terraforming, we should now be able to run ansible:\n% ansible-playbook -i inventory.ini playbook.yml In order to make this setup more ergonomic, let\u0026rsquo;s create a Makefile:\nTERRAFORM := terraform all: terraform ansible ansible: ansible-playbook -i inventory.ini playbook.yml terraform: $(TERRAFORM) init $(TERRAFORM) plan $(TERRAFORM) apply clean: $(TERRAFORM) destroy .PHONY: all ansible terraform clean Then we can just run make terraform or make ansible for granular steps. Or just make to run everything in the right order.\nI extracted the terraform binary to its own variable because it facilitates the use of OpenTofu (a fork) in lieu of terraform.\nAnd that\u0026rsquo;s it for today! In a future post, we\u0026rsquo;ll look into extending our Ansible usage to fully bootstrap Miniflux on the server.\n","permalink":"https://www.perrotta.dev/2024/02/integrating-terraform-with-ansible/","summary":"\u003cp\u003eThis post is a follow-up of \u003ca href=\"https://www.perrotta.dev/2024/01/terraforming-a-linode-hello-world/\"\u003eTerraforming a Linode: hello world\u003c/a\u003e.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIn a future post, we will continue from here by using Ansible to install and\nset up Miniflux in our new Linode.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eBefore we extensively use Ansible to configure our VPS instance, first let\u0026rsquo;s\nset up a basic integration between Terraform and Ansible.\u003c/p\u003e","title":"★ Integrating terraform with ansible"},{"content":"Wordle made a splash during the pandemic. By now people became quite accostumed to it, and maybe even bored somewhat.\nJumblie is somewhat similar, and arguably more difficult challenging. It was created by Cassidy Williams.\n","permalink":"https://www.perrotta.dev/2024/02/jumblie/","summary":"\u003cp\u003e\u003ca href=\"https://www.nytimes.com/games/wordle/index.html\"\u003eWordle\u003c/a\u003e made a splash during\nthe pandemic. By now people became quite accostumed to it, and maybe even bored\nsomewhat.\u003c/p\u003e","title":"Jumblie"},{"content":"This blog is rendered by the means of a static site generator (SSG) called Hugo. Each blog post has a set of one or more tags associated to it. The more posts I create, the more consolidated the tags become.\nSometimes I need to rename tags after-the-fact to better reflect the underlying posts they represent.\nThis is how I typically do it. Start from the root of the git repository, then do:\n% for file in content/posts/**/*.md; do gsed -i -e \u0026#39;s/- german/- deutsch/g\u0026#39; \u0026#34;$file\u0026#34;; done The example above renames german -\u0026gt; deutsch.\nThis isn\u0026rsquo;t the most robust way to do so, but it\u0026rsquo;s the quickest one. For extra robustness, I\u0026rsquo;d do:\n% fd -t f -e md -e gsed -i -x \u0026#39;s/- german/- deutsch/g\u0026#39; \u0026hellip;however it\u0026rsquo;s always easier to remember the for loop syntax than the fd one.\nWhy fd instead of a for loop? fd(1) is more elegant than shell wildcards. Although, in practice, both ways are equivalent and should yield no difference.\nWhy gsed instead of sed? I am on macOS. The GNU version of sed does not create backup files, which is what I want in most cases. There\u0026rsquo;s no need for backups because everything is checked into git already; if I make a mistake, I can always git reset --hard or git checkout. The BSD version of sed will leave this mess behind:\n% fd -t f -e md -x /usr/bin/sed -i -e \u0026#39;s/- german/- deutsch/g\u0026#39; % git st On branch master Your branch is up to date with \u0026#39;origin/master\u0026#39;. Changes not staged for commit: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to update what will be committed) (use \u0026#34;git restore \u0026lt;file\u0026gt;...\u0026#34; to discard changes in working directory) modified: 2022-02-27-linux-us-international-keyboard-layout.md modified: 2022-04-03-translating-german-to-english.md modified: 2024-01-29-anki-find-all-notes-with-an-empty-field.md Untracked files: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to include in what will be committed) 2014-01-07-testando-uma-iso-no-linux-sem-o-virtualbox.md-e 2014-04-18-mini-recovery-tipico-via-usb.md-e 2014-05-01-instalando-o-gentoo-a-partir-do-arch.md-e 2014-09-28-my-first-ebuild.md-e 2015-01-07-the-eudyptula-challenge.md-e [...] There are even more of these *-e files, and they are super annoying. It\u0026rsquo;s easy to get rid of them:\n% rm **/*-e \u0026hellip;but why bother, if we can just stick to the more familiar GNU sed anyway?\nCaveats Finally, note the caveat: this find and replace is naive and could end up replacing false positives! Nonetheless, I\u0026rsquo;m still a big fan of this approach, because it\u0026rsquo;s the quickest one. As my blog is checked into git anyway, I can always easily review the changes before committing them:\n% git diff If there are too many diffs, then prefer an incremental approach:\n% git add -p Happy tag renaming! Well, this only happens every once in a while anyway.\n","permalink":"https://www.perrotta.dev/2024/01/hugo-rename-a-tag/","summary":"\u003cp\u003eThis blog is rendered by the means of a static site generator (SSG) called\n\u003ca href=\"https://gohugo.io/\"\u003eHugo\u003c/a\u003e. Each blog post has a set of one or more tags\nassociated to it. The more posts I create, the more consolidated the tags become.\u003c/p\u003e\n\u003cp\u003eSometimes I need to rename tags after-the-fact to better reflect the underlying\nposts they represent.\u003c/p\u003e","title":"Hugo: rename a tag"},{"content":"Gabriel Wyner recommends, in its Fluent Forever book, that each Anki note (card) has at least one image associated to it. This is intended to improve overall retention.\nBack when I started my German Anki deck for language learning, I did not add any images. Now I find myself slowly backfilling my already existing notes with images. However, the more images I add, the harder it becomes to find notes without images.\nUpon reading the Anki manual I figured out a way to find out which notes are still missing images:\nOpen Anki. Open the deck you want to modify – in my case, Languages::German. Click \u0026ldquo;Browse\u0026rdquo;. Type in deck:current \u0026quot;Front Example:\u0026quot;. \u0026ldquo;Front Example\u0026rdquo; is the name of the field of my note template wherein I add images; you should replace it with the corresponding one you use. This syntax isn\u0026rsquo;t intuitive at all. Initially I was trying something like -\u0026quot;Front Example:*\u0026quot;.\n","permalink":"https://www.perrotta.dev/2024/01/anki-find-all-notes-with-an-empty-field/","summary":"\u003cp\u003eGabriel Wyner recommends, in its \u003ca href=\"https://fluent-forever.com/index.html\"\u003eFluent\nForever\u003c/a\u003e book, that each\n\u003ca href=\"https://apps.ankiweb.net/\"\u003eAnki\u003c/a\u003e note (card) has at least one image\nassociated to it. This is intended to improve overall retention.\u003c/p\u003e","title":"Anki: find all notes with an empty field"},{"content":"Whenever I want to upgrade any one of my systems, I run sd-world.\nYou can find the current version of sd-world here in my dotfiles.\nHere\u0026rsquo;s a snapshot1:\n#!/usr/bin/env bash # perform a full system upgrade set -euo pipefail log() { local bold=$(tput bold) normal=$(tput sgr0) echo \u0026#34;${bold}$*${normal}\u0026#34; } run_if_exists() { if command -v \u0026#34;$1\u0026#34; \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; then if [[ $# -eq 1 ]]; then log \u0026#34;Running $1...\u0026#34; \u0026#34;$1\u0026#34; else shift log \u0026#34;Running $*...\u0026#34; \u0026#34;$@\u0026#34; fi elif [[ -d \u0026#34;$1\u0026#34; ]]; then shift log \u0026#34;Running $*...\u0026#34; \u0026#34;$@\u0026#34; fi } # usage: do_git \u0026lt;path/to/git/repo\u0026gt; do_git() { if [[ -d \u0026#34;$1\u0026#34; ]]; then run_if_exists \u0026#34;$1\u0026#34; git -C \u0026#34;$1\u0026#34; pull origin master fi } case \u0026#34;$(uname)\u0026#34; in # linux Linux) # alpine linux run_if_exists \u0026#34;apk\u0026#34; doas apk upgrade # arch linux run_if_exists \u0026#34;pacman\u0026#34; sudo pacman -Syu # debian linux # warning: macos has /usr/bin/apt which is a Java thing run_if_exists \u0026#34;apt-get\u0026#34; sudo apt-get upgrade -y run_if_exists \u0026#34;apt-get\u0026#34; sudo apt-get autoremove ;; # macOS Darwin) # homebrew run_if_exists \u0026#34;brew\u0026#34; brew upgrade run_if_exists \u0026#34;brew\u0026#34; brew cleanup # system update and app store # run_if_exists \u0026#34;softwareupdate\u0026#34; softwareupdate --install --all run_if_exists \u0026#34;mas\u0026#34; mas upgrade ;; # windows MINGW*) # third-party package manager run_if_exists \u0026#34;scoop\u0026#34; scoop update ;; esac # flatpaks run_if_exists \u0026#34;flatpak\u0026#34; flatpak update # nix run_if_exists \u0026#34;nix-channel\u0026#34; nix-channel --update run_if_exists \u0026#34;nix-env\u0026#34; nix-env -u # pihole # update pihole itself and gravity lists run_if_exists \u0026#34;pihole\u0026#34; pihole -up # dotfiles do_git \u0026#34;$HOME/.dotfiles\u0026#34; do_git \u0026#34;$HOME/.dotfiles_corp\u0026#34; There\u0026rsquo;s a lot to unpack here.\nWhy is it called sd-world? world is an inspiration taken from Gentoo Linux. To upgrade a typical gentoo system, you usually run:\nemerge --ask --quiet --update --changed-use --deep @world There\u0026rsquo;s something deeply inspiring about saying it out loud: \u0026ldquo;emerge the world\u0026rdquo;. As if the whole world is at your fingertips.\nsd stands for \u0026ldquo;script directory\u0026rdquo;, it\u0026rsquo;s an inspiration taken from Ian Henry.\nRationale: I tend to put scripts I run semi-frequently in a .bin directory that is in my system $PATH. However, there\u0026rsquo;s always a chance their name could clash with a built-in one (e.g. in /usr/bin/). In order to prevent (or mitigate) it from happening, a prefix is added. For a long time in my life I used the t- prefix, merely because of my first name initial. At some point I migrated to sd-. That\u0026rsquo;s all, nothing fancy about it.\nWhy bash? bash is the de-facto standard shell in most Linux distributions I care about. And it\u0026rsquo;s also easily available in macOS and BSDs. And it\u0026rsquo;s POSIX compliant.\nTherefore: availability, portability and compatibility.\nWhy /usr/bin/env bash instead of /bin/bash? Because the env shebang is more portable. This is more relevant when working with BSDs. On Linux /bin/bash should be mostly fine.\nWhy set -euo pipefail? A well-established good practice.\nWhy use a separare log function? Old habits die hard. Consistent formatting. Why run_if_exists? Since the script attempts to upgrade (potentially) many package managers, at the very least we try to skip the ones that aren\u0026rsquo;t installed. For example, there\u0026rsquo;s no need to attempt to run pacman in a macOS system.\nWhat else? The rest should be quite straightforward to understand. Some design decisions:\nsudo permissions are not asked upfront, because not every system uses sudo. Notably, Alpine Linux and OpenBSD use doas by default. Also, laziness is OK as the script is intended for interactive use. git is there merely for convenience. Updating my dotfiles could be done from a separate script, but that would be overkill for my simple use case. There\u0026rsquo;s no concurrency / parallelism, and that\u0026rsquo;s intentional. I prefer output readability and system stability in this case. The world is yours.\nPrefer to refer to the up-to-date version in my dotfiles repository though. I included a snapshot merely because there\u0026rsquo;s a non-zero chance the git version could be moved elsewhere someday.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/01/sd-world-perform-a-full-system-upgrade/","summary":"\u003cp\u003eWhenever I want to upgrade any one of my systems, I run \u003ccode\u003esd-world\u003c/code\u003e.\u003c/p\u003e","title":"sd-world: perform a full system upgrade"},{"content":"Let\u0026rsquo;s discuss the raison d\u0026rsquo;etre of Google Chrome for Testing, a project I was the Tech Lead of during my tenure on the Chrome Tooling / Browser Automation team at Google.\nOnce upon a time, a few (debugging) mistakes ago, web developers would run (web) integration tests with WebDriver Classic using Google Chrome (or Chromium)1. This was a chaotic era.\n\u0026ldquo;Why?\u0026rdquo;, you may rightfully ask.\nThe web browser and/or its components / extensions / etc could auto-update in-between successive test runs, yielding different test results, i.e. tests were not guaranteed to be hermetic / deterministic due to their (potentially) changing environment, yielding test flakiness Chrome adds an info bar whenever it is controlled in an automated fashion, which changes the CSS viewport, resulting in changes compared to a production environment. For example: an automated test that takes a screenshot would have a slightly smaller height whenever an infobar is present. There are no versioned Chrome builds for download. There\u0026rsquo;s no browser pinning. As a developer you always download the latest version. This makes it hard to reason about invariants, especially when new browser versions introduce breaking changes, even seemingly small ones. Corollary: The lack of versioned Chrome builds makes it hard to obtain a corresponding (matching) Chromedriver version for Chrome. The mismatch (delta) could provoke testing inconsistencies whenever browser APIs diverge2. In order to address these (and other) issues, Chrome for Testing (hereafter \u0026ldquo;CfT\u0026rdquo;) was born. To clarify, today where are three flavours of Chrom*:\nChromium: the open-source project, https://chromium.org/. The root of all derivatives (Microsoft Edge, Brave, etc). It is available in full source form, but there are no (official) pre-built binaries for it. Google Chrome: the proprietary, closed-source version of Chromium developed by Google. Think of it as Chromium on steroids. Google distributes pre-built Chrome binaries for every platform it supports. Google Chrome for Testing: think of it as \u0026ldquo;reproducible (or pinned, or frozen) Google Chrome\u0026rdquo;. It is basically a snapshot of Google Chrome in a fixed time in the past, plus a few bits of developer-oriented features mentioned in this article. There are other niceties that Chrome for Testing accomplishes as of today:\nThe CDP (Chrome DevTools Protocol) experiment (\u0026ldquo;Protocol Monitor\u0026rdquo;) is enabled by default, out-of-the-box. This kind of experiment, which enriches your debugging toolbox, is exactly the sensible state you want during the development cycle. Mechanisms such as self-XSS confirmation prompts are disabled by default, which is the desired behavior for automation. Consider an analogy with setting DEBIAN_FRONTEND=noninteractive when running apt in dockerfiles. You don\u0026rsquo;t want prompts (even benign ones) to suddenly get in the way of your tests and end up interrupting their execution flow. Completely agnostic to the concept of \u0026ldquo;Stable\u0026rdquo; / \u0026ldquo;Beta\u0026rdquo; / \u0026ldquo;Dev\u0026rdquo;. If you have pinned versions, you don\u0026rsquo;t need to care about any of that. CfT releases are made available alongside a subset of corresponding Google Chrome releases Something important to note:\nWarning: Chrome for Testing has been created purely for browser automation and testing purposes, and is not suitable for daily browsing.\nThe main reason for that is the fact that it does not auto-update. You could argue that it doesn\u0026rsquo;t matter: Chrome for most linux distributions also does not auto-update by itself. The updates are normally deferred to the distribution\u0026rsquo;s package manager (e.g. apt, dnf, pacman, etc). Why should it be different for Chrome for Testing?\nAn additional point to consider here is that Chrome for Testing could have new features in the future that would be optimized for developers, not for end users. You don\u0026rsquo;t want end users to shoot themselves on the foot, therefore it\u0026rsquo;s easier, better and safer to do a blanket anti-recommendation of CfT for non-developers3.\nBecause of that, CfT cannot be made the default system browser.\nThe easiest way to obtain CfT is via its public API, which is documented here: https://googlechromelabs.github.io/chrome-for-testing/, or through the official CLI utility that is part of Puppeteer.\nToday, for all the reasons above (and more to come!), CfT is the de-facto recommended solution for browser automation for all things web applications and web platform testing. If you\u0026rsquo;re currently using either Chromium or Google Chrome for these purposes, you should switch to it.\nBonus: How to run Chrome for Testing in CI? The chromium-bidi repository is an excellent (and simple-ish) example on how to do so4.\nGiven a .github/workflows/e2e.yml file:\nname: E2E tests jobs: e2e: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: actions/setup-node@v3 with: node-version: 16 - run: npm ci # This is the exciting part wherein we fetch CfT. # # Despite the \u0026#34;chromium\u0026#34; name, this is actually CfT. # # We set a explicit shell to force \u0026#34;set -eo pipefail\u0026#34; so that, # if the command fails, then the entire step fails. # We do not want \u0026#34;cut\u0026#34; to run if the download fails for some reason. # # The syntactic sugar of the parsing could be improved in a future # version of the CLI tool, but that\u0026#39;s how it should be done for now. # # We store the location of the CfT binary in an environment variable. - name: Install Google Chrome for Testing shell: bash run: | cft_binary=\u0026#34;$(npx @puppeteer/browsers install chromium@latest | cut -f 2- -d\u0026#39; \u0026#39;)\u0026#34; echo \u0026#34;cft_binary=$cft_binary\u0026#34; \u0026gt;\u0026gt; $GITHUB_ENV - uses: actions/setup-python@v4 with: python-version: \u0026#39;3.10\u0026#39; - run: pip install -r tests/requirements.txt # This is an example on how to run a test suite by explicitly pointing # out to CfT, using the environment variable set earlier. - name: Run E2E tests run: npm run e2e env: BROWSER_BIN: ${{ env.cft_binary }} You can find the complete version of this example in an older commit within that repository. The reason I link to an older commit is due to its direct usage of the @puppeteer/browsers CLI tool, which makes it easier to illustrate how to fetch CfT. Recent commits of the repository use a JS wrapper to do so, which is more flexible / robust for the purposes of that particular repository at the expense of decreased readability for a newcomer. Software Engineering is all about trade-offs after all.\nTo fully realize the benefits of reproducibility, you should not use latest. Instead, pin the browser to a specific version.\nIf using an environment variable (or a command-line flag) is not an option for some reason, then an alternative would be to create a symlink (ln -s) to $cft_binary from a place in the front of your $PATH. Or, alternatively, temporarily update your $PATH with the dirname of $cft_binary.\nAlso, if you cannot or do not want to install npm (npx) just for the sake of fetching CfT5, then just fetch it directly (use curl or wget) from its API endpoint, for example:\n% wget https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/121.0.6167.85/linux64/chrome-linux64.zip Although note that this is not a future-proof way of fetching CfT. It\u0026rsquo;s a simple shortcut. The better way is to query the JSON metadata file for a specific platform and browser version:\n% curl https://googlechromelabs.github.io/chrome-for-testing/latest-patch-versions-per-build-with-downloads.json | jq -r \u0026#39;.builds.\u0026#34;121.0.6167\u0026#34;.downloads.chrome[] | select(.platform == \u0026#34;linux64\u0026#34;).url\u0026#39; \u0026hellip;so that the download works even if the URL changes in the future for some reason.\nReferences Chrome for Testing Design Document: https://goo.gle/chrome-for-testing How Chrome DevTools helps to defend against self-XSS attacks For simplicity, referred to as just Chrome hereafter.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nYou can find lots of such reports here.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe same way you wouldn\u0026rsquo;t recommend Arch Linux for linux newbies.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDisclaimer: I used to work on that repository, thus my self-assessment is clearly biased :-)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI know, I know, JS bloat.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/01/google-chrome-for-testing-reliable-downloads-for-browser-automation/","summary":"\u003cp\u003eLet\u0026rsquo;s discuss the \u003cem\u003eraison d\u0026rsquo;etre\u003c/em\u003e of \u003ca href=\"https://developer.chrome.com/blog/chrome-for-testing\"\u003eGoogle Chrome for\nTesting\u003c/a\u003e, a project I was\nthe Tech Lead of during my tenure on the Chrome Tooling / Browser Automation team\nat Google.\u003c/p\u003e","title":"★ (Google) Chrome for Testing: reliable downloads for browser automation"},{"content":"I host my own Miniflux instance, which happens to be my favorite RSS reader. Currently it is hosted on Linode (Akamai Cloud) running Alpine Linux.\nMy current setup was performed manually. I was thinking that, for fun, it would be cool to fully automate it under the principles of IaC.\nThe current setup does not use any containers. I had proudly made it as KISS as possible at the time:\nLinode is a very beginner-friendly (and cheap) VPS Alpine Linux is a first-class citizen on Linode There\u0026rsquo;s an apk package for miniflux There\u0026rsquo;s an OpenRC1 script for miniflux (so that it can be controlled via service) For the first part of this automation we will look into provisioning a Linode with an Alpine Linux installation. In order to do so we will use HashiCorp Terraform.\nRequirements Provision a new Linode Deploy it in Europe Use the smallest shape (a so-called Nanode) Run Alpine Linux Set it up with my public ssh key, which is hosted on Github Terraform setup Install a provider for Linode: https://registry.terraform.io/providers/linode/linode/latest/docs Scaffold it like this, in a main.tf file:\nterraform { required_providers { linode = { source = \u0026#34;linode/linode\u0026#34; } } } Then run:\n% terraform init Generate a Linode API token Go to https://cloud.linode.com/profile/tokens, create a new token called terraform. with the \u0026ldquo;Linodes\u0026rdquo; scope set to \u0026ldquo;Read/Write\u0026rdquo;.\nAppend this API token to main.tf: provider \u0026#34;linode\u0026#34; { token = \u0026#34;\u0026lt;your token here\u0026gt;\u0026#34; } Add a linode_instance with the appropriate fields set according to the documentation: resource \u0026#34;linode_instance\u0026#34; \u0026#34;coruscant\u0026#34; { label = \u0026#34;coruscant\u0026#34; image = \u0026#34;linode/alpine3.19\u0026#34; region = \u0026#34;eu-central\u0026#34; type = \u0026#34;g6-nanode-1\u0026#34; authorized_keys = [\u0026#34;\u0026lt;your ssh public key here\u0026gt;\u0026#34;] backups_enabled = \u0026#34;false\u0026#34; watchdog_enabled = \u0026#34;true\u0026#34; booted = \u0026#34;true\u0026#34; } Then run:\n% terraform plan \u0026ldquo;Plan\u0026rdquo; is basically a dry-run. Terraform will output what it intends to do, but nothing will be done yet.\nAnalyze the output and double check that it looks correct. To actually perform the provisioning, run:\n% terraform apply Then confirm the prompt.\nWithin a few seconds (or maybe minutes), you should see your new Linode in the Linode Console.\nWe can test our deployment by ssh\u0026rsquo;ing to our new machine:\n% ssh root@\u0026lt;public IP address\u0026gt; -i ~/.ssh/my_ssh_key Welcome to Alpine! The Alpine Wiki contains a large amount of how-to guides and general information about administrating Alpine systems. See \u0026lt;https://wiki.alpinelinux.org/\u0026gt;. You can setup the system with the command: setup-alpine You may change this message by editing /etc/motd. Let\u0026rsquo;s take a pause to appreciate how lightweight it is:\nlocalhost:~# df -h Filesystem Size Used Available Use% Mounted on devtmpfs 10.0M 0 10.0M 0% /dev shm 487.8M 0 487.8M 0% /dev/shm /dev/sda 24.1G 238.1M 22.6G 1% / tmpfs 195.1M 268.0K 194.8M 0% /run Only 238 MiB!\nTo deprovision it, run:\n% terraform plan -destroy If everything looks correct, run:\n% terraform destroy Warning: It turns out the \u0026ldquo;Linodes\u0026rdquo; scope was not enough to do the deprovisioning. I needed to create a new scope, with more permissions, in order to do so.\nAs you can see, terraform makes it very trivial to deprovision systems.\nBonus points: run terraform fmt to format your file. Never go out of style.\nTip: At any point you can run terraform validate to verify your main.tf file is syntatically correct.\nTwo things could be improved in the previous setup:\nWe could use authorized_users to pass in our linode username. If we add an SSH key to our linode account, then that key would be automatically deployed to the system, thereby removing the need to specify authorized_keys. Alternatively, we could fetch our key from an URL endpoint with the use of the hashicorp/http provider, like so: terraform { required_providers { http = { source = \u0026#34;hashicorp/http\u0026#34; } } } data \u0026#34;http\u0026#34; \u0026#34;thiagowfx_ssh_keys\u0026#34; { url = \u0026#34;https://github.com/thiagowfx.keys\u0026#34; } resource \u0026#34;linode_instance\u0026#34; \u0026#34;coruscant\u0026#34; { # ... authorized_keys = compact([for line in split(\u0026#34;\\n\u0026#34;, data.http.thiagowfx_ssh_keys.response_body) : chomp(line)]) # ... } The \u0026ldquo;list comprehension\u0026rdquo; above does line splitting magic to convert them to a list of string, and the compact removes the empty new line at the end.\nWe could improve the example above even further.\nFor starters, let\u0026rsquo;s parameterize out the username to a variable:\nvariable \u0026#34;github_username\u0026#34; { type = string default = \u0026#34;thiagowfx\u0026#34; } data \u0026#34;http\u0026#34; \u0026#34;user_ssh_keys\u0026#34; { url = \u0026#34;https://github.com/${var.github_username}.keys\u0026#34; } resource \u0026#34;linode_instance\u0026#34; \u0026#34;coruscant\u0026#34; { # ... authorized_keys = compact([for line in split(\u0026#34;\\n\u0026#34;, data.http.user_ssh_keys.response_body) : chomp(line)]) # ... } We could then easily supply another username with -var:\n% terraform plan -var github_username=torvalds Note that the above example leverages string interpolation.\nWe could also extract the SSH keys list to its own \u0026ldquo;variable\u0026rdquo; (locals):\nlocals { ssh_keys = compact([for line in split(\u0026#34;\\n\u0026#34;, data.http.user_ssh_keys.response_body) : chomp(line)]) } resource \u0026#34;linode_instance\u0026#34; \u0026#34;coruscant\u0026#34; { # ... authorized_keys = local.ssh_keys # ... } A more robust (and stable) way to query the key though is through the Github API:\ndata \u0026#34;http\u0026#34; \u0026#34;github_keys\u0026#34; { url = \u0026#34;https://api.github.com/users/${var.github_username}/keys\u0026#34; } locals { ssh_keys = jsondecode(data.http.github_keys.response_body)[*].key } Note that a typical response body looks like the following:\n[ { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;id\u0026gt;\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;\u0026lt;ssh key\u0026gt;\u0026#34; } ] API endpoint documentation: https://docs.github.com/en/rest/users/keys?apiVersion=2022-11-28#list-public-keys-for-a-user\nIf we use output instead of locals, then we can debug (inspect) it with terraform output.\nAnd that\u0026rsquo;s it for today! In a future post, we will continue from here by using Ansible to install and set up Miniflux in our new Linode.\nAlpine Linux does not use systemd.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/01/terraforming-a-linode-hello-world/","summary":"\u003cp\u003eI host my own \u003ca href=\"https://miniflux.app/\"\u003eMiniflux\u003c/a\u003e instance, which happens to be\nmy favorite RSS reader. Currently it is hosted on Linode (Akamai Cloud)\nrunning \u003ca href=\"https://www.alpinelinux.org/\"\u003eAlpine Linux\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eMy current setup was performed manually. I was thinking that, for fun, it would\nbe cool to fully automate it under the principles of\n\u003ca href=\"https://en.wikipedia.org/wiki/Infrastructure_as_code\"\u003eIaC\u003c/a\u003e.\u003c/p\u003e","title":"★ Terraforming a Linode: hello world"},{"content":"Some websites attempt to prevent users from pasting text (i.e. Ctrl+V / Cmd+V) in web browsers.\nIt serves no purpose in most cases other than hindering accessibility and increasing annoyance for users.\nIt is relatively easy to bypass most trivial blocks with plain JavaScript. Open a DevTools console (Option + Cmd + I in Google Chrome on macOS), then paste in the following snippet:\nfunction () { const forceEnableCopyPaste = (e) =\u0026gt; { e.stopImmediatePropagation(); return true; }; [\u0026#39;paste\u0026#39;, \u0026#39;copy\u0026#39;].forEach(event =\u0026gt; { document.addEventListener(event, forceEnableCopyPaste, true); }); })(); Bonus points: it also works for copying text.\nI can see the point of trying to attempt to block copying (e.g. copyright, online exams), but there\u0026rsquo;s little reason to prevent pasting.\nThe only arguably valid use case I\u0026rsquo;ve seen to date for blocking pasting is in some sign-up forms wherein you need to type in some piece of user ID (e.g. your email, or your telephone number) twice. The second text field is sometimes blocked, as to encourage you to double check it is absolutely correct™, by the means of carefully typing it out.\nThe snippet above could also be easily converted to a bookmarklet.\nSearch for \u0026ldquo;bookmarklet builder\u0026rdquo; in your favorite search engine, go to a website such as https://caiorss.github.io/bookmarklet-maker/, paste the above snippet therein, then get a compressed version such as:\njavascript:(function()%7Bjavascript%3A%20(function%20()%20%7B%0A%20%20const%20forceEnableCopyPaste%20%3D%20(e)%20%3D%3E%20%7B%0A%20%20%20%20e.stopImmediatePropagation()%3B%0A%20%20%20%20return%20true%3B%0A%20%20%7D%3B%0A%0A%20%20%5B\u0026#39;paste\u0026#39;%2C%20\u0026#39;copy\u0026#39;%5D.forEach(event%20%3D%3E%20%7B%0A%20%20%20%20document.addEventListener(event%2C%20forceEnableCopyPaste%2C%20true)%3B%0A%20%20%7D)%3B%0A%7D)()%3B%7D)()%3B Now just create a web browser favorite with that resource. Clicking the bookmark will yield the same effect as pasting the snippet into devtools.\n","permalink":"https://www.perrotta.dev/2024/01/the-fundamental-right-to-paste/","summary":"\u003cp\u003eSome websites attempt to prevent users from pasting text (i.e. \u003ccode\u003eCtrl+V\u003c/code\u003e /\n\u003ccode\u003eCmd+V\u003c/code\u003e) in web browsers.\u003c/p\u003e\n\u003cp\u003eIt serves no purpose in most cases other than hindering accessibility and\nincreasing annoyance for users.\u003c/p\u003e","title":"The fundamental right to paste"},{"content":"Whenever disk space gets almost full, I like to use the following software to clean up (unnecessary) big files from my computers:\nWindows WinDirStat:\nWinDirStat is a disk usage statistics viewer and cleanup tool for various versions of Microsoft Windows.\nIt\u0026rsquo;s user-friendly and open source.\nLinux / macOS ncdu:\nNcdu is a disk usage analyzer with an ncurses interface. It is designed to find space hogs on a remote server where you don’t have an entire graphical setup available, but it is a useful tool even on regular desktop systems. Ncdu aims to be fast, simple and easy to use, and should be able to run in any minimal POSIX-like environment with ncurses installed.\nNaturally, the command line way™ prevails on Unix systems. It\u0026rsquo;s one installation command away from your favorite package manager. Usage:\n% ncdu / ","permalink":"https://www.perrotta.dev/2024/01/remove-large-files-from-your-computer/","summary":"\u003cp\u003eWhenever disk space gets almost full, I like to use the following software to\nclean up (unnecessary) big files from my computers:\u003c/p\u003e","title":"Remove large files from your computer"},{"content":"Graphviz\u0026hellip;\nis open source graph visualization software. Graph visualization is a way of representing structural information as diagrams of abstract graphs and networks.\nWe can also use it to craft family trees!\nI came up with the following template:\ndigraph G { {Greatgrandfather Greatgrandmother} -\u0026gt; Grandfather; {Grandfather Grandmother} -\u0026gt; Father; {Father Mother} -\u0026gt; Me; Greatgrandfather [shape = box;label = \u0026#34;Greatgrandfather Lastname\u0026#34;;]; Greatgrandmother [shape = box;label = \u0026#34;Greatgrandmother Lastname\u0026#34;;]; Grandfather [shape = box;label = \u0026#34;Grandfather Lastname\u0026#34;;]; Grandmother [shape = box;label = \u0026#34;Grandmother Lastname\u0026#34;;]; Father [shape = box;label = \u0026#34;Father Lastname\u0026#34;;]; Mother [shape = box;label = \u0026#34;Mother Lastname\u0026#34;;]; Me [shape = box;label = \u0026#34;Me Lastname\u0026#34;;]; Greatgrandfather [color = \u0026#34;lightblue\u0026#34;;style = filled;]; Greatgrandmother [color = \u0026#34;pink\u0026#34;;style = filled;]; Grandfather [color = \u0026#34;lightblue\u0026#34;;style = filled;]; Grandmother [color = \u0026#34;pink\u0026#34;;style = filled;]; Father [color = \u0026#34;lightblue\u0026#34;;style = filled;]; Mother [color = \u0026#34;pink\u0026#34;;style = filled;]; Me [style = filled;color = lightblue;]; } \u0026hellip;save this to a tree.dot file.\nA .png representation of the graph can then be generated with the following command:\ndot -Tpng tree.dot \u0026gt; tree.png Note that dot is part of the graphviz distribution.\nIn case it\u0026rsquo;s not installed on your system, it\u0026rsquo;s widely available, just do it. For example, on macOS:\nbrew install graphviz The final result:\n","permalink":"https://www.perrotta.dev/2024/01/create-a-family-tree-with-graphviz/","summary":"\u003cp\u003e\u003ca href=\"https://graphviz.org/\"\u003eGraphviz\u003c/a\u003e\u0026hellip;\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eis open source graph visualization software. Graph visualization is a way of\nrepresenting structural information as diagrams of abstract graphs and\nnetworks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWe can also use it to craft family trees!\u003c/p\u003e","title":"Create a family tree with graphviz"},{"content":"Sometimes I need to start a local HTTP server for a quick one-off task, often just to serve static content. It is not important which one it is, so long as I can do it quickly.\nOption #1: use python % python3 -m http.server Serving HTTP on :: port 8000 (http://[::]:8000/) ... This is often the most universal and convenient option, as python is widely available out-of-the-box.\nOption #2: use darkhttpd darkhttpd is available almost everywhere.\n% darkhttpd . darkhttpd/1.14, copyright (c) 2003-2022 Emil Mikulic. listening on: http://0.0.0.0:8080/ Their own README states:\nWhen you need a web server in a hurry.\nThis is the most convenient option when you are in control of a package manager, as it is one installation command away from your system. In particular, it\u0026rsquo;s available in both homebrew and nixpkgs.\nOption #3: use nodejs % npx http-server -p 8000 Starting up http-server, serving ./ http-server version: 14.1.1 http-server settings: CORS: disabled Cache: 3600 seconds Connection Timeout: 120 seconds Directory Listings: visible AutoIndex: visible Serve GZIP Files: false Serve Brotli Files: false Default File Extension: none Available on: http://127.0.0.1:8000 Hit CTRL-C to stop the server If you\u0026rsquo;re already within the node ecosystem, this is also just one installation away. I would typically not recommend this setup though if you don\u0026rsquo;t already have npm installed on your system.\nOption #4: use busybox This option seemed very attractive for use on Linux systems:\n% busybox httpd -f -p 8080 However in an up-to-date Alpine Linux system (3.20) it does not work:\n% busybox httpd -f -p 8080 httpd: applet not found Therefore I don\u0026rsquo;t consider it universal enough.\nReference: https://gist.github.com/willurd/5720255\n","permalink":"https://www.perrotta.dev/2024/01/start-an-http-server-asap/","summary":"\u003cp\u003eSometimes I need to start a local HTTP server for a quick one-off task, often\njust to serve static content. It is not important which one it is, so long as I\ncan do it quickly.\u003c/p\u003e","title":"Start an HTTP server ASAP"},{"content":"Some (annoying) websites and/or mobile apps will refuse to let you proceed past their registration / login screen unless you provide a last name. Isn\u0026rsquo;t my first name enough1?\nThere are a couple of ways to work around this:\nProvide a fake last name Provide a gibberish last name (aklsjslkja) Provide only the first or second letter of your real last name (e.g. \u0026ldquo;Thiago P\u0026rdquo;) Provide some non-latin-alphabetic character (e.g. \u0026ldquo;1\u0026rdquo;, \u0026ldquo;.\u0026rdquo;, \u0026ldquo;Э̇\u0026rdquo;) Recently it occurred to me there\u0026rsquo;s an even cleverer idea: provide an empty (whitespace) character.\nSome services have validation in place to prevent you from inserting a mere ASCII whitespace (\u0026rsquo; \u0026lsquo;).\nHowever, most will not bother to check \u0026ldquo;invisible\u0026rdquo; unicode whitespace:\n‎ There\u0026rsquo;s a single whitespace character above you can easily copy. vim displays it as \u0026lt;200e\u0026gt;.\nReference: https://emptycharacter.com/\nActually, why do you even need to know my first name in the first place? I am just some random database primary key ID.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2024/01/why-do-you-need-to-know-my-last-name/","summary":"\u003cp\u003eSome (annoying) websites and/or mobile apps will refuse to let you proceed past\ntheir registration / login screen unless you provide a last name. Isn\u0026rsquo;t my\nfirst name enough\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e?\u003c/p\u003e","title":"Why do you need to know my last name?"},{"content":" In Java, you can make a variable thread safe by just adding the synchronized keyword. Is there anything that can achieve the same results in Python?\nWithout having prior knowledge of any python libraries to do so, the primitive interface I would expect resembles the following:\nclass Foo(object): def __init__(self): self.lock = Lock() def perform_mutation(self, bytes): print(bytes) def write(self, bytes): self.lock.acquire() self.perform_mutation(bytes) self.lock.release() This isn\u0026rsquo;t robust: if an exception happens in perform_mutation the lock would never be released. A small improvement we can make is to wrap it with try/finally:\nclass Foo(object): def __init__(self): self.lock = Lock() def perform_mutation(self, bytes): print(bytes) def write(self, bytes): self.lock.acquire() try: self.perform_mutation(bytes) finally: self.lock.release() However it turns out there\u0026rsquo;s a more pythonic way to do so:\nfrom threading import Lock class Foo(object): def __init__(self): self.lock = Lock() def perform_mutation(self, bytes): print(bytes) def write(self, bytes): with self.lock: self.perform_mutation(bytes) How can we test this? First, let\u0026rsquo;s use a single thread.\nif __name__ == \u0026#34;__main__\u0026#34;: foo = Foo() foo.write(\u0026#34;hello from the main thread\u0026#34;) Now let\u0026rsquo;s use multiple threads:\nif __name__ == \u0026#34;__main__\u0026#34;: foo = Foo() threads = [] for i in range(10): thread = Thread(target=foo.write, args=(f\u0026#34;hello from thread {i}\u0026#34;,)) threads.append(thread) # Start all threads for thread in threads: thread.start() # Wait for all threads to finish for thread in threads: thread.join() Without the lock this is one of the results I get locally:\n% python3 lock.py hello from thread 0 hello from thread 1 hello from thread 2 hello from thread 3 hello from thread 4 hello from thread 6 hello from thread 8 hello from thread 7 hello from thread 5 hello from thread 9 With the lock I always get the following, as you would predict:\n% python3 lock.py hello from thread 0 hello from thread 1 hello from thread 2 hello from thread 3 hello from thread 4 hello from thread 5 hello from thread 6 hello from thread 7 hello from thread 8 hello from thread 9 We could go one level deeper in the abstraction by using a @synchronized decorator:\nclass Foo(object): def perform_mutation(self, bytes): print(bytes) @synchronized def write(self, bytes): self.perform_mutation(bytes) How do we implement it?\ndef synchronized(member): @wraps(member) def wrapper(*args, **kwargs): lock = vars(member).get(\u0026#34;_synchronized_lock\u0026#34;, None) if lock is None: lock = vars(member).setdefault(\u0026#34;_synchronized_lock\u0026#34;, Lock()) with lock: return member(*args, **kwargs) return wrapper One last concept to learn: RLock a.k.a. reentrant lock.\nConsider the following program:\nfrom threading import Lock, Thread class Foo: def __init__(self): self.lock = Lock() def changeA(self, bytes): with self.lock: print(bytes) def changeB(self, bytes): with self.lock: print(bytes) def changeAandB(self, bytes): with self.lock: print(bytes) self.changeA(bytes) # a usual lock would block here self.changeB(bytes) Invoked as follows:\nfoo = Foo() threads = [] for i in range(5): thread = Thread(target=foo.changeA, args=(f\u0026#34;hello from thread {i} A\u0026#34;,)) threads.append(thread) thread = Thread(target=foo.changeB, args=(f\u0026#34;hello from thread {i} B\u0026#34;,)) threads.append(thread) thread = Thread(target=foo.changeAandB, args=(f\u0026#34;hello from thread {i} AB\u0026#34;,)) threads.append(thread) # Start all threads for thread in threads: thread.start() # Wait for all threads to finish for thread in threads: thread.join() It will not work as expected. As soon as the first changeAandB gets called, its inner self.changeA call will block. This is because the lock can only be acquired once.\nIn this specific example, the straightforward way to fix the issue is to use an RLock: self.lock = RLock(). The reentrant lock can be locked multiple times.\nReferences https://theorangeduck.com/page/synchronized-python https://stackoverflow.com/questions/29158282/how-to-create-a-synchronized-function-across-all-instances https://stackoverflow.com/questions/53026622/python-equivalent-of-java-synchronized https://stackoverflow.com/questions/16567958/when-and-how-to-use-pythons-rlock ","permalink":"https://www.perrotta.dev/2024/01/synchronized-in-python/","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u003ca href=\"https://stackoverflow.com/questions/53026622/python-equivalent-of-java-synchronized\"\u003eIn Java, you can make a variable thread safe by just adding the \u003ccode\u003esynchronized\u003c/code\u003e\nkeyword. Is there anything that can achieve the same results in\nPython?\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e","title":"★ Synchronized in Python"},{"content":"In a typical dynamic programming (DP) problem, you\u0026rsquo;ll usually instantiate a variable to hold previously computed data (cache).\nFor example, let\u0026rsquo;s consider a naive implementation of the factorial function:\ndef factorial(n: int) -\u0026gt; int: if n == 0: return 1 return n * factorial(n - 1) Now let\u0026rsquo;s add a cache to improve it, upon a use case wherein it would be called multiple times in a row:\ncache: dict[int, int] = {} def factorial(n: int) -\u0026gt; int: if n == 0: return 1 if n in cache: return cache[n] cache[n] = n * factorial(n - 1) return cache[n] This is straightforward, the only caveat to watch out for is the scope of the cache. In general you wouldn\u0026rsquo;t want to store it globally.\nOne elegant way to address this is with lru_cache:\nfrom functools import lru_cache @lru_cache(maxsize=None) def factorial(n: int) -\u0026gt; int: if n == 0: return 1 return n * factorial(n - 1) The snippet above creates and maintains a cache under the hood. The main caveat of that snippet is that it\u0026rsquo;s not easy to remember:\nis it max_size or maxsize? is it maxsize=0 or maxsize=None? This week I found out that there\u0026rsquo;s an even more ergonomic decorator, which it is super easy to remember!\nfrom functools import cache @cache def factorial(n: int) -\u0026gt; int: if n == 0: return 1 return n * factorial(n - 1) @cache is equivalent to lru_cache(maxsize=None).\nWith this trick, you won\u0026rsquo;t ever need to manually memoize any function in python anymore!\nThis works for any number of arguments so long as they can be used as dictionary keys, i.e. the arguments must be hashable. Practically speaking, this means lists are not cacheable, but tuples are.\nHappy dynamic programming!\nReference: https://docs.python.org/3/library/functools.html\n","permalink":"https://www.perrotta.dev/2024/01/python-all-hail-to-cache-memoization/","summary":"\u003cp\u003eIn a typical dynamic programming (DP) problem, you\u0026rsquo;ll usually instantiate a\nvariable to hold previously computed data (cache).\u003c/p\u003e","title":"Python: all hail to cache memoization"},{"content":"It\u0026rsquo;s 2024, the year of the linux desktop, and the best™ way to debug computer programs is still the good ol\u0026rsquo; print statement.\nSince Python 3.6 it is possible to use f-strings.\nOne of my favorite ways to use them for debugging is with the equal sign (=):\nTo display both the expression text and its value after evaluation, (useful in debugging), an equal sign '=' may be added after the expression.\nHere is one example:\ndef is_full_word_match(token, words): print(f\u0026#39; is_full_word_match: {token=} {words=}\u0026#39;) return token in words If you call it like so:\nis_full_word_match(\u0026#34;hello\u0026#34;, \u0026#34;hello world\u0026#34;) Then it will print the following:\nis_full_word_match: token=\u0026#39;hello\u0026#39; words=\u0026#39;hello world\u0026#39; This is a more ergonomic (and quicker) way to write than the classic:\nprint(\u0026#39; is_full_word_match: token=\u0026#39; + token + \u0026#39; words=\u0026#39; + words) Or even:\nprint(\u0026#39; is_full_word_match: token={} words={}\u0026#39;.format(token, words)) ","permalink":"https://www.perrotta.dev/2024/01/python-debugging-tip-with-print-and-f-strings/","summary":"\u003cp\u003eIt\u0026rsquo;s 2024, \u003ca href=\"https://yotld.com/\"\u003ethe year of the linux desktop\u003c/a\u003e, and the best™\nway to debug computer programs is still the good ol\u0026rsquo; \u003ccode\u003eprint\u003c/code\u003e statement.\u003c/p\u003e","title":"Python: debugging tip with print and f-strings"},{"content":"For some odd reason my Calibre backup to cloud storage had a bunch of empty directories. I\u0026rsquo;ve been meaning to remove them, but it\u0026rsquo;s cumbersome to do so from the web client.\nInstead, let\u0026rsquo;s do it from a local client.\nUpon installing the cloud storage software, a local directory is exposed under /Users/$USER/Library/CloudStorage (macOS).\nMy first instinct is to use find(1):\n$ find -empty -type d -delete However that does not work on macOS:\nfind: illegal option -- e usage: find [-H | -L | -P] [-EXdsx] [-f path] path ... [expression] find [-H | -L | -P] [-EXdsx] -f path [path ...] [expression] My second go to choice is fd(1):\n$ fd -t e -x rmdir \u0026hellip;this lists all empty directories and invokes rmdir on each of them.\n","permalink":"https://www.perrotta.dev/2023/12/macos-remove-all-empty-directories/","summary":"\u003cp\u003eFor some odd reason my \u003ca href=\"https://calibre-ebook.com\"\u003eCalibre\u003c/a\u003e backup to cloud\nstorage had a bunch of empty directories. I\u0026rsquo;ve been meaning to remove them, but\nit\u0026rsquo;s cumbersome to do so from the web client.\u003c/p\u003e","title":"macOS: remove all empty directories"},{"content":"Gerrit 3.9 has been released recently. This is a dear release to me because I was responsible for some of its changes1:\n$ PAGER=\u0026#34;cat\u0026#34; git shortlog --author=\u0026#34;Thiago Perrotta\u0026#34; v3.8.0..v3.9.0 Thiago Perrotta (3): Add \u0026#39;description\u0026#39;, \u0026#39;d\u0026#39; aliases for the \u0026#39;message\u0026#39; search operator Add \u0026#39;m\u0026#39; alias for the \u0026#39;message\u0026#39; search operator UX: \u0026#34;Your Turn\u0026#34; -\u0026gt; \u0026#34;Your turn\u0026#34; This is not the first Google open source project I contributed to, however I wanted to note these contributions here nonetheless.\nOne of my favorite aspects of Google culture is the ability to contribute to any project in the company at any time in any capacity (the so called \u0026ldquo;20% contributions\u0026rdquo;).\nThe output was edited to remove duplicate entries and merge commits because the Gerrit project does not maintain a clean history of their commits i.e. they do not adopt a rebase workflow.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2023/12/gerrit-v3.9-is-out/","summary":"\u003cp\u003eGerrit \u003ca href=\"https://www.gerritcodereview.com/3.9.html\"\u003e3.9\u003c/a\u003e has been released recently. This is a dear release to me because I was responsible for some of its changes\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e:\u003c/p\u003e","title":"Gerrit v3.9 is out"},{"content":"Whenever I need to write out a simple document wherein content is more important than form, LaTeX is my preferred choice. It beats Google Docs, Microsoft Word, LibreOffice, or any other text processor for that matter.\nOverleaf is a solid cloud editor choice these days but I tend to prefer to have full control over my programming environment, thus for a local solution TeXShop is my favorite on macOS:\n$ brew install texshop The following template worked well for me to answer interview questions:\n% The next lines tell TeXShop to typeset with xelatex, and to open and save the source with Unicode encoding. %!TEX TS-program = xelatex %!TEX encoding = UTF-8 Unicode \\documentclass[12pt, oneside]{article} \\usepackage{geometry} \\geometry{letterpaper} \\usepackage{amssymb} \\usepackage{fontspec,xltxtra,xunicode} \\defaultfontfeatures{Mapping=tex-text} \\setromanfont[Mapping=tex-text]{Hoefler Text} \\setsansfont[Scale=MatchLowercase,Mapping=tex-text]{Gill Sans} \\setmonofont[Scale=MatchLowercase]{Andale Mono} \\title{My document title} % \\author{Thiago Perrotta} % uncomment if applicable \\date{\\today} \\begin{document} \\maketitle \\tableofcontents \\pagebreak \\section{Preliminaries} \\paragraph{Foo bar?} Lorem ipsum. \\paragraph{What is the answer for life, universe and everything?} 42. ","permalink":"https://www.perrotta.dev/2023/12/latex-template-for-simple-interviews/","summary":"\u003cp\u003eWhenever I need to write out a simple document wherein content is more important than form, \u003cstrong\u003eLaTeX\u003c/strong\u003e is my preferred choice. It beats Google Docs, Microsoft Word, LibreOffice, or any other text processor for that matter.\u003c/p\u003e","title":"LaTeX template for simple interviews"},{"content":"uBlock-Origin-dev-filter:\nFilters to block and remove copycat-websites from DuckDuckGo, Google and other search engines. Used to be specific to dev websites like StackOverflow or GitHub, but it currently supports others like Wikipedia.\n1-click to subscribe to a blocking list via uBlock origin, customizable on a per search engine basis (e.g. Google, DuckDuckGo, etc).\nCredits: https://www.lkhrs.com/blog/2022/04/block-domains-from-search/\n","permalink":"https://www.perrotta.dev/2023/12/enhance-adblock-lists-with-ublock-origin-dev-filter/","summary":"\u003cp\u003e\u003ca href=\"https://github.com/quenhus/uBlock-Origin-dev-filter\"\u003euBlock-Origin-dev-filter\u003c/a\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eFilters to block and remove copycat-websites from DuckDuckGo, Google and\nother search engines. Used to be specific to dev websites like StackOverflow\nor GitHub, but it currently supports others like Wikipedia.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Enhance adblock lists with uBlock origin dev filter"},{"content":"Recent macbooks have a fingerprint reader, which is typically used to unlock the computer and log in.\nIt is also possible to use it for sudo authentication via PAM.\nThis was previously covered here.\nNow, with macOS Sonoma, it\u0026rsquo;s also possible to make this setting survive OS upgrades.\n% sudo cp /etc/pam.d/sudo_local{.template,} % sudo $EDITOR /etc/pam.d/sudo_local Then uncomment (or add, if not existing) the following line:\nauth sufficient pam_tid.so You can test it out by opening a new terminal and executing sudo echo.\nCredits: https://sixcolors.com/post/2023/08/in-macos-sonoma-touch-id-for-sudo-can-survive-updates/\n","permalink":"https://www.perrotta.dev/2023/12/macos-sudo-with-touch-id-survive-upgrades/","summary":"\u003cp\u003eRecent macbooks have a fingerprint reader, which is typically used to unlock the computer and log in.\u003c/p\u003e\n\u003cp\u003eIt is also possible to use it for \u003ccode\u003esudo\u003c/code\u003e authentication via \u003ca href=\"https://en.wikipedia.org/wiki/Pluggable_authentication_module\"\u003ePAM\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThis was previously covered \u003ca href=\"https://www.perrotta.dev/2022/03/macos-sudo-with-touch-id/\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eNow, with macOS Sonoma, it\u0026rsquo;s also possible to make this setting survive OS upgrades.\u003c/p\u003e","title":"macOS: sudo with touch ID: survive upgrades"},{"content":"Strongly recommended to all software developers who use macOS: Maccy.\nClipboard manager for macOS which does one job - keep your copy history at hand. Period.\nLightweight. Open source. No fluff.\nShortcut: Cmd + Shift + C to open a clipboard menu with all your recently copied items, plus a search bar for quick grepping.\nHands down this is the best piece of software I added to my workflow in 2023, competing with Obsidian and Things in terms of productivity.\n","permalink":"https://www.perrotta.dev/2023/12/maccy-macos-clipboard-manager/","summary":"\u003cp\u003e\u003cstrong\u003eStrongly recommended\u003c/strong\u003e to all software developers who use macOS: \u003ca href=\"https://maccy.app/\"\u003eMaccy\u003c/a\u003e.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eClipboard manager for macOS which does one job - keep your copy history at hand. Period.\u003c/p\u003e\n\u003cp\u003eLightweight. Open source. No fluff.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Maccy macOS clipboard manager"},{"content":"2022 was an amazing year for AIs.\nChatGPT ChatGPT can effectively replace Stack Overflow to a large extent, if you learn how to ask questions to it. Example queries that work amazingly, returning exactly what you would expect, with detailed explanation and/or context:\nPKGBUILD how to specify git version python how to sort list reverse systemd create unit file that forks xorg start i3wm with startx git update all submodules to latest tip typescript make simple post request chromium difference between args and declare_args sekiro versus elden ring generate random number between 10 and 20 I cannot run the code. Please generate a random number for me between 10 and 20 though add pasta before or after boiling water? what is the difference between auszahlen and Bargeld abheben? write a trip packing checklist for me Someone even wrote a Chrome extension to query ChatGPT alongside a Google search: https://github.com/wong2/chat-gpt-google-extension (chatgpt4google.com). I couldn\u0026rsquo;t get it to work for some reason though.\nStable Diffusion Given a query (text), it generates an image for you. It pairs up quite nicely with Slides (when looking for inspiration) and can effectively replace Google Images to some extent.\n","permalink":"https://www.perrotta.dev/2022/12/ais-galore/","summary":"\u003cp\u003e2022 was an amazing year for AIs.\u003c/p\u003e","title":"AIs galore"},{"content":"I haven\u0026rsquo;t written a blog post for a couple of months now, which is a good indicator I should probably document my workflow before I forget how to do it\u0026hellip;\nFirst, git clone --recurse https://github.com/thiagowfx/thiagowfx.github.io/. I like to store it in ~/Projects/thiagowfx.github.io.\nTo create a new post, hugo new content/posts/2022-10-09-title-comes-here.md.\nUse either vim or textmate to draft the post. Choose one or more tags, trying to reuse existing ones whenever possible. When using vim, use Q to reformat paragraphs.\nTo preview the post locally, run make run and then open http://localhost:1313/.\nIf everything looks good, git commit and git push. GitHub CI will then publish the post to GitHub Pages in a couple of seconds.\nTo blog on the go, use https://github.dev/. I documented this setup earlier, here.\n","permalink":"https://www.perrotta.dev/2022/10/do-i-still-remember-how-to-blog/","summary":"\u003cp\u003eI haven\u0026rsquo;t written a blog post for a couple of months now, which is a good indicator I should probably document my workflow before I forget how to do it\u0026hellip;\u003c/p\u003e","title":"Do I still remember how to blog?"},{"content":"moreutils has previously been covered elsewhere, multiple times. It\u0026rsquo;s a collection of small unix tools that follow the unix philosophy1 very strongly.\nHere are some of my favorites with example usages. Obviously this post isn\u0026rsquo;t a manual which would have been a disservice to the community; refer to the upstream man pages for detailed instructions.\nsponge sponge(1) - soak up standard input and write to a file\nHere\u0026rsquo;s a typical workflow sponge(1) is great at:\n# Given a file $ cat myfile a b c # Imagine that for whatever reason we want to replace \u0026#39;a\u0026#39; with \u0026#39;b\u0026#39; # Naively, we could try this: $ cat myfile | tr \u0026#39;a\u0026#39; \u0026#39;b\u0026#39; \u0026gt; myfile $ cat myfile # However the file becomes empty! # It got clobbered when we tried to simultenaously read from and write to it # sponge comes to the rescue! $ cat myfile | tr \u0026#39;a\u0026#39; \u0026#39;b\u0026#39; | sponge myfile $ cat myfile b b c It is great to use sponge in lieu of \u0026gt; (shell output redirection) in shell pipelines when trying to both read from and write to the same file.\nvidir vidir(1) - edit directories and filenames\nvidir(1) is great to bulk rename files/directories within a given directory, one level at a time. For example, if I open vidir at the top-level directory of this blog repository, it opens up vim (although it doesn\u0026rsquo;t need to be vim, your $EDITOR is honoured) with the following content:\n1\t./.git 2\t./.github 3\t./.gitignore 4\t./.gitmodules 5\t./.hugo_build.lock 6\t./LICENSE 7\t./Makefile 8\t./README.md 9\t./archetypes 10\t./config.yml 11\t./content 12\t./layouts 13\t./public 14\t./resources 15\t./static 16\t./themes If I make, say, the following modifications (lines 7 and 8):\n1\t./.git 2\t./.github 3\t./.gitignore 4\t./.gitmodules 5\t./.hugo_build.lock 6\t./LICENSE 7\t./GNUMakefile 8\t./README.rst 9\t./archetypes 10\t./config.yml 11\t./content 12\t./layouts 13\t./public 14\t./resources 15\t./static 16\t./themes And then save and quit vim (:wq), then the effect would have been the same as:\n$ mv Makefile GNUMakefile $ mv README.md README.rst If I changed my mind and decided not to save the modifications, I could just do :cq.\nIt\u0026rsquo;s possible to leverage vim features such as . (repeat command) and :%s/ (find and replace) to perform those mass file renames quickly and effectively. vidir is a breeze to use!\nifne ifne(1) - Run command if the standard input is not empty\nifne(1) is effective when used with find or fd to keep shell pipes \u0026ldquo;happy\u0026rdquo;. Here\u0026rsquo;s one simple example:\n$ find . -name \u0026#39;*.cpp\u0026#39; | xargs clang-format This should work as expected, but it\u0026rsquo;s cleaner to do:\n$ find . -name \u0026#39;*.cpp\u0026#39; | ifne xargs clang-format The added ifne ensures the xargs command is only executed if and only if find yields at least one result in its output.\nThis wasn\u0026rsquo;t a very practical example though: a more realistic way to use ifne is with prototypical on-the-fly manipulation of shell pipes wherein initially you just do whatever, but then whenever you notice some command in the middle of the pipe has failed because its input was empty (=the previous pipe command output was empty) you just prepend ifne to it:\n$ this | is | a | complicated | pipe # assume \u0026#34;complicated\u0026#34; fails because it has no input # so we iterate and do: $ this | is | a | ifne complicated | pipe combine combine(1) - combine sets of lines from two files using boolean operations\ncombine(1) is pretty much comm(1), but much more user-friendly. Given two files file1 and file2 it makes it easy to query which lines are {unique, common} to {each, both} files, using boolean operations (or, and, not, xor). Here\u0026rsquo;s one example to find the common lines in both files, compare combine and comm:\n$ combine file1 and file2 $ comm -12 file1 file2 # flags are harder to remember The Unix philosophy emphasizes building simple, short, clear, modular, and extensible code that can be easily maintained and repurposed by developers other than its creators. The Unix philosophy favors composability as opposed to monolithic design.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/05/tools-you-should-know-about-moreutils/","summary":"\u003cp\u003e\u003ca href=\"https://joeyh.name/code/moreutils/\"\u003e\u003ccode\u003emoreutils\u003c/code\u003e\u003c/a\u003e has previously been covered \u003ca href=\"https://news.ycombinator.com/item?id=31043655\"\u003eelsewhere\u003c/a\u003e, multiple times. It\u0026rsquo;s a collection of small unix tools that follow the \u003ca href=\"https://en.wikipedia.org/wiki/Unix_philosophy\"\u003eunix philosophy\u003c/a\u003e\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e very strongly.\u003c/p\u003e\n\u003cp\u003eHere are some of my favorites with example usages. Obviously this post isn\u0026rsquo;t a manual which would have been a disservice to the community; refer to the upstream man pages for detailed instructions.\u003c/p\u003e","title":"★ Tools you should know about: moreutils"},{"content":"It occurred to me the Large Prints section of our local public library is a decent way to sample popular books.\nWikipedia:\nLarge-print (also large-type or large-font) refers to the formatting of a book or other text document in which the typeface (or font) are considerably larger than usual to accommodate people who have low vision. Frequently the medium is also increased in size to accommodate the larger text. Special-needs libraries and many public libraries will stock large-print versions of books, along with versions written in Braille.\nInstead of browsing the entire library catalogue to try to find an interesting book to read1, it’s often more effective to browse the Large Print ones.\na.k.a. serendipity.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/04/large-prints/","summary":"\u003cp\u003eIt occurred to me the \u003cstrong\u003eLarge Prints\u003c/strong\u003e section of our local public library is a decent way to sample popular books.\u003c/p\u003e","title":"Large prints"},{"content":"I was looking for a way to track packages and parcels (mail) for the most popular post and courier services e.g. DHL, UPS, Fedex, Canada Post, USPS, Correios, etc.\nThere were only two requeriments:\none place to rule them all: whether an app, chatbot, self-hosted software, or website, all services should be available from a single UI endpoint, for ease of management automatic / periodic updates: whether by polling, subscription or webhook, the service should autonomously retrieve parcel statuses; accessing each individual provider website should be a no-go Ultimately I found two solutions that pleased me, both of which free:\nShopify\u0026rsquo;s Shop app: Pro: Great and KISS design, Con: Tracking; Fortune 500 company gathering analytics and data from my purchases. Although I do have a great amount of respect for Shopify generally, the less amount of tracking by Big Tech the better.\nTelegram\u0026rsquo;s @Trackbot:\nTrackBot is a Telegram bot for tracking all your shipments. Free, forever. Automatic courier recognition. TrackBot automatically detects the courier of the shipment by using machine learning techniques, with an accuracy higher than 97%.\nAfter using both of them for a while, my preferred solution nowadays is the Telegram bot. Its basic operations are (i) List all shipments and (ii) Add a new shipment. It is smart enough to detect the carrier by itself in most cases from the tracking code alone, however whenever it doesn\u0026rsquo;t one can simply specify it manually. Whenever new updates to your existing shipments are detected, it sends you a message (notification) on Telegram.\n","permalink":"https://www.perrotta.dev/2022/04/tracking-packages-automatically-in-a-single-place/","summary":"\u003cp\u003eI was looking for a way to \u003cstrong\u003etrack packages and parcels\u003c/strong\u003e (mail) for the most popular post and courier services e.g. DHL, UPS, Fedex, Canada Post, USPS, Correios, etc.\u003c/p\u003e","title":"Tracking packages automatically in a single place"},{"content":"Issue: For whatever reason, the Home and End keys on my Keychron K2 do not work as intended on macOS.\nExpectations vs Reality For example, when using a text editor such as TextMate or a web browser like Chrome, I\u0026rsquo;d expect:\nHome to position the text cursor in the beginning of the line (à la C-a in emacs) End to position the text cursor in the end of the line (à la C-e in emacs) The only way to provoke these effects out-of-the-box is by pressing, respectively, the Cmd + Left and Cmd + Right shortcuts, as you would normally do in a Macbook laptop native keyboard.\nThis is very annoying because it only happens in macOS1: the Home and End keys work just fine in both Linux and Windows. A reddit user reported the same issue in /r/keychron, but the existing thread has no proposed solutions.\nEnter Karabiner Elements I\u0026rsquo;ve always heard good things about Karabiner Elements as a praised one-size-fits-all application for keyboards and macros in macOS, thus decided to give it a try. Bonus points: it is open source, released into the public domain.\nUpon installing it with Homebrew Cask (brew install karabiner-elements), I executed it. Then I needed to give a bunch of permissions to the application via macOS Settings \u0026gt; Security \u0026amp; Privacy \u0026gt; Privacy \u0026gt; Input Monitoring. The following apps were whitelisted accordingly:\nkarabiner_grabber karabiner_observer Karabiner-EventViewer.app: this one is optional, but useful for debugging The app is straightforward to use. It allows you to do all sorts of reactions to key codes input events.\nI had a simple idea: I wanted to map Home to Cmd + Left, and End to Cmd + Right.\nUnfortunately these are considered \u0026ldquo;Complex modifications\u0026rdquo; because they map one origin key to two destination keys. \u0026ldquo;Simple modifications\u0026rdquo; are one-to-one key mappings. Why is it unfortunate? Because it doesn\u0026rsquo;t seem to be possible to do such mappings via the app UI. Apparently one needs to express those mappings in a .json file instead.\nAh, communities Sure, no problem, I was about to do it but then I realized there\u0026rsquo;s an official website for community-maintained mappings. The website is well organized and curated. Why create something fully from scratch when I could just reuse an existing one?\nI found a \u0026ldquo;Keychron K2\u0026rdquo; category which made me instantly happy but it turned out not to be useful, as there were only two defined mappings therein:\nChange Keychron K2 keyboard layout to more closely resemble an Apple keyboard Remap some Keychrom K2(US) keys to make it less painful to switch from Macbook(RU) keyboard None of these mattered to me. Then I searched for home to cmd which led me to this entry, which had exactly the mappings I wanted:\nHome and End\nHome to Command Left End to Command Right Its resulting JSON looks roughly like this (irrelevant bits stripped for the sake of brevity):\n{ \u0026#34;title\u0026#34;: \u0026#34;Home and End\u0026#34;, \u0026#34;rules\u0026#34;: [ { \u0026#34;description\u0026#34;: \u0026#34;Home to Command Left\u0026#34;, \u0026#34;manipulators\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;basic\u0026#34;, \u0026#34;from\u0026#34;: { \u0026#34;key_code\u0026#34;: \u0026#34;home\u0026#34; }, \u0026#34;to\u0026#34;: [ { \u0026#34;key_code\u0026#34;: \u0026#34;left_arrow\u0026#34;, \u0026#34;modifiers\u0026#34;: \u0026#34;command\u0026#34; } ] } ] }, { \u0026#34;description\u0026#34;: \u0026#34;End to Command Right\u0026#34;, \u0026#34;manipulators\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;basic\u0026#34;, \u0026#34;from\u0026#34;: { \u0026#34;key_code\u0026#34;: \u0026#34;end\u0026#34; }, \u0026#34;to\u0026#34;: [ { \u0026#34;key_code\u0026#34;: \u0026#34;right_arrow\u0026#34;, \u0026#34;modifiers\u0026#34;: \u0026#34;command\u0026#34; } ] } ] } ] } There\u0026rsquo;s conveniently an Import button in the website though, which automatically opens the mappings in Karabiner Elements, so I didn\u0026rsquo;t even need to copy and paste the JSON.\nVerdict End Result: It worked flawlessly! The only caveat is that from now on I need to keep the Karabiner Elements application running as a daemon, but it is well justified. Plus, if I ever need2 to map additional keys in the future, now I already have a workflow in place to do so.\nKarabiner is like having QMK purely at the software layer, which works for any keyboard whatsoever.\nAs of this writing: macOS Monterey: 12.3.1.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://en.wikipedia.org/wiki/You_aren%27t_gonna_need_it\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/04/keychron-k2-on-macos-fix-home-and-end-keys/","summary":"\u003cp\u003e\u003cstrong\u003eIssue\u003c/strong\u003e: For whatever reason, the \u003ccode\u003eHome\u003c/code\u003e and \u003ccode\u003eEnd\u003c/code\u003e keys on my \u003ca href=\"https://www.perrotta.dev/2022/01/keychron-k2-review/\"\u003eKeychron K2\u003c/a\u003e do not work as intended on macOS.\u003c/p\u003e","title":"★ Keychron K2 on macOS: fix Home and End keys"},{"content":"I\u0026rsquo;ve been trying to learn German on my own, without taking formal classes. I should write a post about this sometime.\nFor now, in this post I will briefly list the resources I use to translate German to English and/or to look up the definition of German words in German.\nGoogle Translate Everyone knows Google Translate, right? Direct link for DE-\u0026gt;EN translations: https://translate.google.com/?sl=de\u0026amp;tl=en/\nGoogle Translate is my to go swiss-army knife one-size-fits-all application whenever I need to translate something without thinking too much. It is decent for words and for phrases, sentences and expressions, giving you: (i) translation, (ii) synonyms / alternate translations, (iii) dictionary definition and (iv) pronounciation.\nA quicker shortcut to use on the go is Google itself: A query like translate strasse from german works as you would expect, and it even displays an Open in Google Translate button for further tweaking.\ndict.cc Dict.cc is great to look up detailed definitions of German words and idioms. It\u0026rsquo;s a superb dictionary (Wörterbuch). Example query: strasse. It has apps for Android and iOS, both of which support offline lookups.\nDeepL DeepL Translate is the new kid in town (released on 2017), \u0026ldquo;The world\u0026rsquo;s most accurate translator\u0026rdquo; as they describe themselves. It\u0026rsquo;s free, but with some limitations. Apparently they use some heavy machine learning machinery different than Google\u0026rsquo;s that may yield better (\u0026ldquo;more natural\u0026rdquo;) results in some situations. I like to keep it around as an alternative to Google Translate when its translations aren\u0026rsquo;t satisfactory, but I don\u0026rsquo;t use it much.\nGoogle Dictionary When using Google Chrome the Google Dictionary extension is handy to quickly look up word definitions without leaving the current page. A double click in a word triggers a pop-up balloon with a concise definition.\nAs a bonus it doubles down as an English dictionary.\nApple Dictionary On Apple operating systems, including iOS and macOS, one can out-of-the-box force touch a word which yields a behavior very similar to Google Dictionary (\u0026ldquo;Look Up\u0026rdquo;).\n","permalink":"https://www.perrotta.dev/2022/04/translating-german-to-english/","summary":"\u003cp\u003eI\u0026rsquo;ve been trying to learn German on my own, without taking formal classes. I should write a post about this sometime.\u003c/p\u003e\n\u003cp\u003eFor now, in this post I will briefly list the resources I use to translate German to English and/or to look up the definition of German words in German.\u003c/p\u003e","title":"Translating German to English"},{"content":"Terminal.app is a pretty decent terminal emulator for macOS, with sensible defaults. That said, I have my own gripes about it, but the list is surprisingly small.\n1. No OSC-52 support https://github.com/roy2220/osc52pty:\nOSC 52 is one of Xterm Control Sequences, which is designated for clipboard setting. Once a terminal supporting OSC 52 catches a text in the form of OSC 52 from the output, instead of printing the text onto the screen, it decodes the text first and then sends the content to the system clipboard.\nAlthough Terminal.app does NOT support OSC 52, here [osc52pty] is the workaround for it.\nI dislike this workaround because it requires an external binary. Even though it is a single binary because it is a Golang executable, I still dislike the external dependency.\n2. No true color (256 colors) What is true color? See stack overflow for context.\nRun the following to print a color band, a smooth (gradient) output indicates true color support:\nawk \u0026#39;BEGIN{ s=\u0026#34;/\\\\/\\\\/\\\\/\\\\/\\\\\u0026#34;; s=s s s s s s s s; for (colnum = 0; colnum\u0026lt;77; colnum++) { r = 255-(colnum*255/76); g = (colnum*510/76); b = (colnum*255/76); if (g\u0026gt;255) g = 510-g; printf \u0026#34;\\033[48;2;%d;%d;%dm\u0026#34;, r,g,b; printf \u0026#34;\\033[38;2;%d;%d;%dm\u0026#34;, 255-r,255-g,255-b; printf \u0026#34;%s\\033[0m\u0026#34;, substr(s,colnum+1,1); } printf \u0026#34;\\n\u0026#34;; }\u0026#39; Terminal.app will not print a gradient.\n3. No GPU acceleration https://unix.stackexchange.com/q/658709:\nQ: What are the advantages of hardware-accelerated terminal emulators?\nA: They can potentially be faster at outputting and refreshing vast amounts of information. It could also allow for smooth(er) scrolling. Human beings however are quite slow at reading this information, [\u0026hellip;] the average person is unlikely to be able to comprehend it anyways. CPU usage could be lower but it needs to be tested.\nTerminal.app isn\u0026rsquo;t GPU accelerated.\nRecommendations Both alacritty and kitty are decent replacements (or complements) for Terminal.app that work out-of-the-box, with sensible defaults including all the aforementioned points.\n","permalink":"https://www.perrotta.dev/2022/03/macos-terminal-app-gripes/","summary":"\u003cp\u003e\u003ccode\u003eTerminal.app\u003c/code\u003e is a pretty decent terminal emulator for macOS, with sensible\ndefaults. That said, I have my own gripes about it, but the list is\nsurprisingly small.\u003c/p\u003e","title":"macOS terminal app gripes"},{"content":"Just tested straight to spam today:\nLove emails but hate people? Don’t want someone 🤡 at your party 🥳 but have to invite them 🤢 cause your mom 💁‍♀️ made you? Trust Straight 2 Spam to send your v important email 📧 straight to their spam 🗑\nClick the button below👇 to copy a nasty ❌ ooey ❌ gooey ❌ spam-keyword filled invisible message 🔤 for your email that you totally sent on time ⏰ but the 🐦 dodo-brain 🧠 won\u0026rsquo;t see it because they didn’t check their spam folder 📂 (Just make sure you\u0026rsquo;re not in the recipient\u0026rsquo;s address book 📇, or all bets are off 🙅‍♀️)\nIt works exactly as advertised:\nSend an email to someone whose address book contains your email address, and it will not go to spam. Send an email to someone whose address book does not contain your email address, and it goes straight to the spam folder. The email body is indeed invisible, at least in the Gmail web UI. Even Ctrl+A won\u0026rsquo;t reveal it. If you click \u0026ldquo;Show original\u0026rdquo; to inspect the full message body and headers though, you\u0026rsquo;ll see some junk like the following:\nHello#1 $$$ 100% Act now Action Additional income Affordable All natural/new Amazed Apply now Avoid Be amazed/your own Bitcoin boss Beneficiary Billing Billion Bonus Boss Buy Call!!!!!! free/now Cancel Crypto Cash Casino There\u0026rsquo;s actually more, but I don\u0026rsquo;t want to make this post too spammy for search engines.\nThe aforementioned text is wrapped in this HTML:\n\u0026lt;div dir=3D\u0026#34;ltr\u0026#34;\u0026gt;Hello\u0026lt;span style=3D\u0026#34;color:rgb(255,255,255);font-family:\u0026amp;qu= ot;Comic Sans MS\u0026amp;quot;;font-size:1px\u0026#34;\u0026gt; Which explains why it is \u0026lsquo;invisible\u0026rsquo; (note the white color).\n","permalink":"https://www.perrotta.dev/2022/03/send-emails-straight-to-spam/","summary":"Just tested straight to spam today:\nLove emails but hate people? Don’t want someone 🤡 at your party 🥳 but have to invite them 🤢 cause your mom 💁‍♀️ made you? Trust Straight 2 Spam to send your v important email 📧 straight to their spam 🗑\nClick the button below👇 to copy a nasty ❌ ooey ❌ gooey ❌ spam-keyword filled invisible message 🔤 for your email that you totally sent on time ⏰ but the 🐦 dodo-brain 🧠 won\u0026rsquo;t see it because they didn’t check their spam folder 📂 (Just make sure you\u0026rsquo;re not in the recipient\u0026rsquo;s address book 📇, or all bets are off 🙅‍♀️)","title":"Send emails straight to spam"},{"content":"EFF\u0026rsquo;s1 HTTPS Everywhere is a browser extension available for all major browsers that automatically upgrades HTTP to HTTPS on supported websites.\nOther than adblocking, it\u0026rsquo;s one of the first extensions I add to a fresh browser installation.\nToday I learned it is apparently not needed anymore. Both Google Chrome and Firefox have settings these days to perform exactly the same functionality of the extension.\nIn Google Chrome do: chrome://settings -\u0026gt; Security and Privacy -\u0026gt; Advanced -\u0026gt; Toggle \u0026lsquo;Always use secure connections\u0026rsquo; on.\nIn Firefox the option is located on Settings -\u0026gt; Privacy \u0026amp; Security -\u0026gt; HTTPs only mode.\nEFF is a big proponent and advocate for a secure web, being one of the core responsible actors for certbot and Let\u0026rsquo;s Encrypt.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/03/https-everywhere-so-long-and-thanks-for-all-the-fish/","summary":"\u003cp\u003eEFF\u0026rsquo;s\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e \u003ca href=\"https://chrome.google.com/webstore/detail/https-everywhere/gcbommkclmclpchllfjekcdonpmejbdp\"\u003eHTTPS Everywhere\u003c/a\u003e is a browser extension available for all major browsers that automatically upgrades HTTP to HTTPS on supported websites.\u003c/p\u003e","title":"HTTPS Everywhere: So long and thanks for all the fish"},{"content":"Recent macbooks have a fingerprint reader, which is typically used to unlock the computer and log in.\nIt is also possible to use it for sudo authentication via PAM:\n% $EDITOR /etc/pam.d/sudo # sudo: auth account password session auth sufficient pam_tid.so # \u0026lt;== add this line auth sufficient pam_smartcard.so auth required pam_opendirectory.so account required pam_permit.so password required pam_deny.so session required pam_permit.so Once the file is saved with the added line, a command with sudo will spawn the touch ID prompt. I confirmed it works on both Terminal.app and Kitty.\nThis solution does not work within tmux (confirmed), and apparently within iTerm2 as well (not confirmed). A separate PAM module is needed to do so (pam_reattach.so). I\u0026rsquo;d rather keep my core dependencies surface small though and not include a third party, so for now I am satisfied with the native touch ID module.\nReferences https://sixcolors.com/post/2020/11/quick-tip-enable-touch-id-for-sudo/ https://apple.stackexchange.com/a/306324 ","permalink":"https://www.perrotta.dev/2022/03/macos-sudo-with-touch-id/","summary":"\u003cp\u003eRecent macbooks have a fingerprint reader, which is typically used to unlock the computer and log in.\u003c/p\u003e\n\u003cp\u003eIt is also possible to use it for \u003ccode\u003esudo\u003c/code\u003e authentication via \u003ca href=\"https://en.wikipedia.org/wiki/Pluggable_authentication_module\"\u003ePAM\u003c/a\u003e:\u003c/p\u003e","title":"macOS: sudo with touch ID"},{"content":"Computers are fast.\nLet\u0026rsquo;s find out how well you know computers! All of these programs have a variable NUMBER in them. Your mission: guess how big NUMBER needs to get before the program takes 1 second to run.\nYou don\u0026rsquo;t need to guess exactly: they\u0026rsquo;re all between 1 and a billion. Just try to guess the right order of magnitude!\nThis is basically an interactive version of Latency Numbers Every Programmer Should Know, originally coined by Jeff Dean.\n","permalink":"https://www.perrotta.dev/2022/03/computers-are-fast/","summary":"\u003cp\u003e\u003ca href=\"https://computers-are-fast.github.io\"\u003eComputers are fast\u003c/a\u003e.\u003c/p\u003e","title":"Computers are fast"},{"content":"Whenever I need to fully reinstall a Windows Desktop system, there are certain applications that need to be bootstrapped: image viewer, image editor, office suite, PDF viewer, video player, web browsers, etc.\nIn my opinion, Ninite is the best way to do so.\nAll you need to do is to select a few checkboxes. The Ninite installer will then automatically download and install all selected applications and software, one-by-one, with sensible defaults1 and a decent progress report:\nApp1\tOK App2\tInstalling App3\tWaiting to install App4\tDownloading App5\tWaiting to download I dunno why they do it one-by-one, but it\u0026rsquo;s in principle reasonable, probably intended to avoid potential conflicts of multiple installers trying to fiddle with each other at the same time.\nThe Ninite installer also has an interesting reuse2 property: You could save it to run it again in the future: it will end up updating the existing applications – and maybe reinstalling them, in case some of them were uninstalled in the meantime.\nIf you bookmark the URL generated by the webapp, which looks like https://ninite.com/7zip-chrome-irfanview-steam/ 3, the same set of applications could be bootstrapped once again in the future, which is useful to do batch installations in multiple computers, or to reinstall everything after a factory reset.\nThis is the URL I used to install sensible applications for my parents:\nhttps://ninite.com/7zip-chrome-classicstart-dropbox-firefox-gimp-googledrivefordesktop-inkscape-irfanview-klitecodecs-libreoffice-qbittorrent-steam-sumatrapdf-teamviewer15-thunderbird-vlc/\nFor example, by saying \u0026lsquo;No\u0026rsquo; to junk like browser toolbars, add-ons and \u0026ldquo;extras\u0026rdquo;.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI wanted to say \u0026lsquo;reproducibility\u0026rsquo;, but it\u0026rsquo;s not quite what it means.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nYou can add more pieces of software as needed.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/03/ninite-bootstrap-a-windows-installation/","summary":"\u003cp\u003eWhenever I need to fully reinstall a Windows Desktop system, there are certain applications that need to be bootstrapped: image viewer, image editor, office suite, PDF viewer, video player, web browsers, etc.\u003c/p\u003e\n\u003cp\u003eIn my opinion, \u003ca href=\"https://ninite.com\"\u003eNinite\u003c/a\u003e is the best way to do so.\u003c/p\u003e","title":"Ninite: Bootstrap a Windows installation"},{"content":"Miniflux 2.0.36 has been released this week. This is a dear release to me because I was responsible for many of its changes:\n$ PAGER=\u0026#34;cat\u0026#34; git shortlog --author=\u0026#34;Thiago Perrotta\u0026#34; 2.0.35..2.0.36 Thiago Perrotta (8): Add pagination on top of all entries. Closes #1305. Add links to scraper/rewrite/filtering docs when editing feeds Add several icons to menus according to their roles Add new keyboard shortcut: \u0026#39;M\u0026#39; - toggle read/unread, go to prev item refactor handleEntryStatus / goToNextListItem / goToPrevListItem Add (+) action next to Feeds to quickly add new feeds Add \u0026#39;+\u0026#39; shortcut for new subscription page Gray out pagination buttons when they are not applicable This is the first open source project I self-host in a serious manner that I am contributing back to. It is very exciting and fulfilling, and it sparks a lot of joy in my heart.\nI have a few other ideas for improving the miniflux user experience and functionality, while still keeping it simple and elegant. As everything else in life, time is the only constraint\u0026hellip;\n","permalink":"https://www.perrotta.dev/2022/03/miniflux-v2.0.36-is-out/","summary":"\u003cp\u003eMiniflux \u003ca href=\"https://miniflux.app/releases/2.0.36.html\"\u003e2.0.36\u003c/a\u003e has been released this week. This is a dear release to me because I was responsible for many of its changes:\u003c/p\u003e","title":"Miniflux v2.0.36 is out"},{"content":"I try to avoid websites with paywalls. If I really like the website and it deserves my attention, I will throw in a monthly subscription for it. High-quality content deserves to be supported. The fragmentation isn\u0026rsquo;t always great and it\u0026rsquo;s often hard to keep track of multiple distinct news sources and portals / subscriptions, but that\u0026rsquo;s a topic for another day.\nSometimes people will link to news articles or websites with paywalls from various sources (blogs, social media, other news articles, etc). I\u0026rsquo;d rather know in advance that those articles are paywalled, but that\u0026rsquo;s not always possible. After clicking them, curiosity already killed the cat.\nThere are several ways to access those as one-offs. I will add a disclaimer that I do not publicly endorse any of those methods, they are just mentioned for educational purposes.\nThe most typical way is to open an incognito tab or window in your browser with the desired URL. This works because many paywalls are often implemented with browser cookies.\nThe second most typical way is to use a VPN to appear that you\u0026rsquo;re accessing the URL from another IP address. This works for websites that add rolling article limits per IP address.\nOccasionally some large news websites will implement paywalls poorly:\nThe idea is pretty simple, news sites want Google to index their content so it shows up in search results. So they don\u0026rsquo;t show a paywall to the Google crawler. We benefit from this because the Google crawler will cache a copy of the site every time it crawls it.\nAll we do is show you that cached, unpaywalled version of the page.\n12ft automatically uses this mechanism to display cached versions of news articles. If you\u0026rsquo;re in \u0026lt;url\u0026gt;, just prepend 12ft.io to it: https://12ft.io/\u0026lt;url\u0026gt;.\nAlternatively, Outline used to be another website/service to do so, but apparently it is unavailable since last week. Outline displays a pretty printed version of text from an article, looking a lot like a markdown-rendered version of a web page.\nSomeone on Hacker News suggested txtify.it as a replacement to it. Indeed, Txtify is very similar to Outline, however it displays plain text instead (i.e. no formatting at all).\nApparently some people even go further by installing browser extensions to do so.\nUltimately, whenever possible, prefer to access news sources from news portals that aren\u0026rsquo;t paywalled and/or that you are a subscriber of.\n","permalink":"https://www.perrotta.dev/2022/03/bypass-news-article-paywalls/","summary":"\u003cp\u003eI try to avoid websites with paywalls. If I really like the website and it\ndeserves my attention, I will throw in a monthly subscription for it.\nHigh-quality content deserves to be supported. The fragmentation isn\u0026rsquo;t always\ngreat and it\u0026rsquo;s often hard to keep track of multiple distinct news sources and\nportals / subscriptions, but that\u0026rsquo;s a topic for another day.\u003c/p\u003e\n\u003cp\u003eSometimes people will link to news articles or websites with paywalls from\nvarious sources (blogs, social media, other news articles, etc). I\u0026rsquo;d rather\nknow in advance that those articles are paywalled, but that\u0026rsquo;s not always\npossible. After clicking them, curiosity already killed the cat.\u003c/p\u003e\n\u003cp\u003eThere are several ways to access those as one-offs. I will add a disclaimer\nthat I do not publicly endorse any of those methods, they are just mentioned\nfor educational purposes.\u003c/p\u003e","title":"Bypass news article paywalls"},{"content":"I use QWERTY keyboards with a US layout. Sometimes I need to type accents or cedillas, and I keep forgetting how to do so, this post summarizes how to do it.\nIntro There are basically two layouts:\nUS (\u0026lsquo;vanilla\u0026rsquo;): type accents like '^`~ and they will be emitted immediately US International (INTL): accents are the so called \u0026lsquo;dead keys\u0026rsquo;: A dead key is a special kind of a modifier key on a mechanical typewriter, or computer keyboard, that is typically used to attach a specific diacritic to a base letter.\nWe can switch between keyboard layouts with setxkbmap. It\u0026rsquo;s also possible to use localectl in systemd-based distros, but its syntax is harder to remember so I won\u0026rsquo;t even include it here.\nSet US \u0026lsquo;vanilla\u0026rsquo; keyboard layout $ setxkbmap us This is what a standard QWERTY keyboard should use to type in English.\nSet US International (INTL) keyboard layout $ setxkbmap -layout us -variant intl This is what a standard QWERTY keyboard1 should use to type, for example, in Portuguese or in German.\nPortuguese - á é í ó ú : \u0026#39; + \u0026lt;vowel\u0026gt; - â ê î ô û : ^ + \u0026lt;vowel\u0026gt; - ã õ : ~ + \u0026lt;vowel\u0026gt; - à : ` + \u0026lt;vowel\u0026gt; - ç (cedilla) : Alt Gr + , (Option + c on macOS) German - ß (ss) : Alt Gr + s (Option + s on macOS) - ä ö ü : \u0026#34; + \u0026lt;vowel\u0026gt; Alt Gr is typically the Right Alt key.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/02/linux-us-international-keyboard-layout/","summary":"\u003cp\u003eI use QWERTY keyboards with a US layout. Sometimes I need to type accents or\ncedillas, and I keep forgetting how to do so, this post summarizes how to do it.\u003c/p\u003e","title":"Linux: US International keyboard layout"},{"content":"I keep forgetting these, so I wrote a small summary for my own reference.\nHSTS Wikipedia — HSTS:\nHTTP Strict Transport Security (HSTS) is a policy mechanism that helps to protect websites against man-in-the-middle attacks such as protocol downgrade attacks and cookie hijacking. It allows web servers to declare that web browsers (or other complying user agents) should automatically interact with it using only HTTPS connections.\nIn layman\u0026rsquo;s terms: Force HTTPS on a given domain.\nHSTS Preload List:\nThis form is used to submit domains for inclusion in Chrome\u0026rsquo;s HTTP Strict Transport Security (HSTS) preload list. This is a list of sites that are hardcoded into Chrome as being HTTPS only.\nMost major browsers (Chrome, Firefox, Opera, Safari, IE 11 and Edge) also have HSTS preload lists based on the Chrome list. (See the HSTS compatibility matrix.)\nIf you add your website to that list, major browsers will honor it and only use HTTPS for your domain.\nSome TLDs enforce HTTPS through HSTS, some popular ones are .app and .dev. Here is a more comprehensive list.\nHSTS is fire-and-forget, you\u0026rsquo;ll usually only need to worry about it once, when configuring a SSL certificate (HTTPS) for your domain or subdomains.\nCSP Wikipedia — CSP:\nContent Security Policy (CSP) is a computer security standard introduced to prevent cross-site scripting (XSS), clickjacking and other code injection attacks resulting from execution of malicious content in the trusted web page context.\nMDN — CSP:\nContent Security Policy (CSP) is an added layer of security that helps to detect and mitigate certain types of attacks, including Cross-Site Scripting (XSS) and data injection attacks. These attacks are used for everything from data theft, to site defacement, to malware distribution.\nCSP can be configured in at least two distinct ways:\nWeb server: return the Content-Security-Policy HTTP header: Content-Security-Policy: default-src \u0026#39;self\u0026#39;; img-src https://*; child-src \u0026#39;none\u0026#39;; HTML \u0026lt;meta\u0026gt; tag: \u0026lt;meta http-equiv=\u0026#34;Content-Security-Policy\u0026#34; content=\u0026#34;default-src \u0026#39;self\u0026#39;; img-src https://*; child-src \u0026#39;none\u0026#39;;\u0026#34;\u0026gt; CSP is something to worry about at the application level. For example, miniflux to fetch resources (fonts) from another domain (Google Fonts).\nCORS Wikipedia — CORS:\nCross-origin resource sharing (CORS) is a mechanism that allows restricted resources on a web page to be requested from another domain outside the domain from which the first resource was served.\nCORS can be configured via web server: return the Access-Control-Allow-Origin HTTP header:\nAccess-Control-Allow-Origin: * Access-Control-Allow-Origin: http://example.com:8080 CORS is something to worry about at the application level. For example, https://keep.google.com/ ⟷ https://google.com/ cookies.\nCSRF Wikipedia — CSRF:\nCross-site request forgery, also known as one-click attack or session riding and abbreviated as CSRF (sometimes pronounced sea-surf) or XSRF, is a type of malicious exploit of a website where unauthorized commands are submitted from a user that the web application trusts.\nCSRF is something to be aware of and to watch out for. OWASP has some additional resources on it.\n","permalink":"https://www.perrotta.dev/2022/02/http-a-few-acronyms/","summary":"\u003cp\u003eI keep forgetting these, so I wrote a small summary for my own reference.\u003c/p\u003e","title":"HTTP: a few acronyms"},{"content":"This post contains a small handful of distinct services to query your machine external IP address.\nGoogle URL: https://www.google.com/search?q=what+is+my+ip\nAs of this writing, this doesn\u0026rsquo;t work on duckduckgo: https://duckduckgo.com/?q=what+is+my+ip. I suppose this is related to their philosophy of not tracking their users.\nThis is the easiest method when you have a web browser as you do not need to memorize any URL.\nI can haz ip URL: https://icanhazip.com/\n$ curl icanhazip.com NNN.NNN.NNN.NNN I love the simplicity of I can haz ip. It just returns your IP address in plain text, nothing else. It also works from the web browser. You can find details about it here. TL;DR: It was an open source pet project of a single person (Major Hayden), then it was eventually bought by Cloudflare as it immensely grew.\nIt\u0026rsquo;s also possible to query your IPv6 address in case you have one:\n$ curl -6 icanhazip.com IPInfo URL: https://ipinfo.io/\nIPInfo returns structured data beyond just your IP address. There are several similar services that do this, for example, What is my IP? and https://ifconfig.co/, however IPInfo is the cleanest one I have seen.\nping.eu URL: https://ping.eu/\nI\u0026rsquo;ll also give an honourable mention to ping.eu as it contains a small handful of utilities to check for things like Traceroute, DNS, whois, port check, etc.\n","permalink":"https://www.perrotta.dev/2022/02/what-is-my-ip/","summary":"\u003cp\u003eThis post contains a small handful of distinct services to query your machine\nexternal IP address.\u003c/p\u003e","title":"What is my IP?"},{"content":"When I created this blog, I pondered a lot about which typography to use. I kept experimenting with several fonts available in Google Fonts, and settled on a few favorites for websites:\nHeader fonts (sans-serif): Inter, Fira Sans, Lato\nBody fonts (serif): Crimson Pro, Vollkorn, Alegreya\nCode fonts (mono): Fira Code, PT Mono, IBM Plex Mono\nUltimately though, none of them mattered. I was motivated and influenced by Kev Quirk\u0026rsquo;s Trying To Go Green With Local Fonts and Steve\u0026rsquo;s This website is killing the planet, which basically boils down to the same spirit of https://motherfuckingwebsite.com/: The web is too bloated nowadays, most websites have a ton of unnecessary CSS and JavaScript junk to fetch over and over again.\nThis is not a big deal if you have access to fast internet and powerful computers, but that\u0026rsquo;s not the case for many people in the planet.\nWith the intent of not unnecessarily fetching fonts from the web, that\u0026rsquo;s why my current font stack just uses the existing fonts in your system, with a few opinionated bits in case you have some of my favorite fonts already installed:\nbody { font-family: Crimson Pro, Vollkorn, Alegreya, Iowan Old Style, Apple Garamond, Baskerville, Times New Roman, Noto Serif, Droid Serif, Times, Source Serif Pro, serif, Apple Color Emoji, Segoe UI Emoji, Segoe UI Symbol, Noto Color Emoji; } h1, h2, h3, h4, h5, h6, footer, nav, .toc, .post-meta { font-family: Inter, Fira Sans, Lato, system-ui, -apple-system, BlinkMacSystemFont, Avenir Next, Avenir, Segoe UI, Helvetica Neue, Helvetica, Ubuntu, Roboto, Noto, Cantarell, Arial, sans-serif; } code, pre { font-family: Fira Code, PT Mono, IBM Plex Mono, Menlo, Consolas, Monaco, Liberation Mono, Ubuntu Mono, Lucida Console, monospace; } The system font stack reference comes from https://systemfontstack.com/ and CSS Tricks.\n","permalink":"https://www.perrotta.dev/2022/02/website-fonts-just-use-the-system-font-stack/","summary":"\u003cp\u003eWhen I created this blog, I pondered a lot about which typography to use. I\nkept experimenting with several fonts available in \u003ca href=\"https://fonts.google.com/\"\u003eGoogle\nFonts\u003c/a\u003e, and settled on a few favorites for websites:\u003c/p\u003e","title":"Website fonts: just use the system font stack"},{"content":"Suppose you want to take a screenshot from a web page, maybe to add to a pull request of a project you\u0026rsquo;re working on.\n(i) The straightforward way to do so is to use your OS tool:\nWindows: Snipping tool or hit the PrintScreen key Linux: scrot or something from your Desktop Environment (DE) such as xfce4-screenshooter. For a full-featured one, I\u0026rsquo;d recommend flameshot (first) or shutter (second). For Wayland people seem to like grim. macOS: Hit Cmd + Shift + 4 or one of its variations. Even Chromebooks have a way to do so these days.\nHowever, maybe you didn\u0026rsquo;t know you can also use Google Chrome to take screenshots! Here\u0026rsquo;s how:\nGo to the page you want to screenshot. Open DevTools (Ctrl + Shift + J on Linux). Hit Ctrl + Shift + P à la VSCode to pop up a command bar, type \u0026lsquo;screenshot\u0026rsquo;. Choose one option. I like the \u0026lsquo;Capture area screenshot\u0026rsquo; one which allows me to drag a square for the area I want to capture. Hit Enter. You will then be prompted where you want to save your screenshot. Profit!\n","permalink":"https://www.perrotta.dev/2022/02/screenshot-a-web-page-from-within-chrome-devtools/","summary":"\u003cp\u003eSuppose you want to take a screenshot from a web page, maybe to add to a \u003ca href=\"https://github.com/miniflux/v2/pull/1341\"\u003epull\nrequest\u003c/a\u003e of a project you\u0026rsquo;re working\non.\u003c/p\u003e\n\u003cp\u003e(i) The straightforward way to do so is to use your OS tool:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eWindows\u003c/strong\u003e: \u003ca href=\"https://support.microsoft.com/en-us/windows/use-snipping-tool-to-capture-screenshots-00246869-1843-655f-f220-97299b865f6b\"\u003eSnipping\ntool\u003c/a\u003e\nor hit the \u003cem\u003ePrintScreen\u003c/em\u003e key\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLinux\u003c/strong\u003e: \u003ca href=\"\"\u003e\u003ccode\u003escrot\u003c/code\u003e\u003c/a\u003e or something from your Desktop Environment (DE) such as\n\u003ccode\u003exfce4-screenshooter\u003c/code\u003e. For a full-featured one, I\u0026rsquo;d recommend\n\u003ca href=\"https://flameshot.org/\"\u003e\u003ccode\u003eflameshot\u003c/code\u003e\u003c/a\u003e (first) or\n\u003ca href=\"https://shutter-project.org/\"\u003e\u003ccode\u003eshutter\u003c/code\u003e\u003c/a\u003e (second). For Wayland people seem\nto like \u003ca href=\"https://wayland.emersion.fr/grim/\"\u003e\u003ccode\u003egrim\u003c/code\u003e\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003emacOS\u003c/strong\u003e: Hit \u003ccode\u003eCmd + Shift + 4\u003c/code\u003e or one of its\n\u003ca href=\"https://support.apple.com/en-ca/HT201361\"\u003evariations\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEven \u003ca href=\"https://support.google.com/chromebook/answer/10474268?hl=en\"\u003eChromebooks\u003c/a\u003e\nhave a way to do so these days.\u003c/p\u003e\n\u003cp\u003eHowever, maybe you didn\u0026rsquo;t know you can also use \u003ca href=\"https://www.google.com/intl/en_ca/chrome/\"\u003eGoogle Chrome\u003c/a\u003e to take screenshots! Here\u0026rsquo;s how:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGo to the page you want to screenshot.\u003c/li\u003e\n\u003cli\u003eOpen DevTools (\u003ccode\u003eCtrl + Shift + J\u003c/code\u003e on Linux).\u003c/li\u003e\n\u003cli\u003eHit \u003ccode\u003eCtrl + Shift + P\u003c/code\u003e à la VSCode to pop up a command bar, type \u0026lsquo;screenshot\u0026rsquo;.\u003c/li\u003e\n\u003cli\u003eChoose one option. I like the \u0026lsquo;Capture area screenshot\u0026rsquo; one which allows me to drag a square for the area I want to capture.\u003c/li\u003e\n\u003cli\u003eHit \u003ccode\u003eEnter\u003c/code\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eYou will then be prompted where you want to save your screenshot. Profit!\u003c/p\u003e","title":"Screenshot a web page from within chrome devtools"},{"content":"I\u0026rsquo;ve been giving Wayland a try. My window manager of choice in X11/Xorg is i3, so the natural choice in Wayland is sway.\nIntro sway works well with the i3 config out-of-the-box. A few adjustments were necessary for full compatibility. To maximize code reuse, I went with the following structure:\n$ tree ~/.config/{i3,sway} /home/typhoon/.config/i3 ├── conf.d │ └── i3.conf └── config /home/typhoon/.config/sway ├── conf.d │ └── sway.conf └── config -\u0026gt; ../../../i3/.config/i3/config The master config is ~/.config/i3/config. It is pretty standard, generated by i3-config-wizard with a few tweaks on top for my own workflow. It works for both i3 and sway. The config contains this snippet: # Load user configs if existing. Order is important. include conf.d/*.conf The snippet allows drop-in customizations to live in conf.d. The include directive is a relatively new addition to the i3config syntax and it\u0026rsquo;s the main reason this setup is elegant and minimalist.\ni3-only config options live in ~/.config/i3/conf.d/i3.conf. To give you an idea of what it looks like and which options aren\u0026rsquo;t compatible with sway, here\u0026rsquo;s a snapshot of my config in early 2022: # i3(1) only config file # Commands herein are not compatible or interoperable with sway(1) # Reference: https://i3wm.org/docs/userguide.html # Autostart XDG applications (.desktop files). # https://wiki.archlinux.org/title/XDG_Autostart # # Troubleshooting: # dex -ade i3 exec dex --autostart --environment i3 # lock screen, Ctrl+Alt+l (systemd) exec --no-startup-id xss-lock -l -- i3lock -c 222222 bindsym Ctrl+Mod1+l exec loginctl lock-session # XF86AudioPlayPause is not recognized by sway, add it only to i3 # https://github.com/swaywm/sway/issues/4783 bindsym XF86AudioPlayPause exec playerctl play-pause # show window title icon for_window [all] title_window_icon on set $bgcolor #526532 set_from_resource $black i3.color0 set_from_resource $red i3.color1 set_from_resource $green i3.color2 set_from_resource $white i3.color7 set_from_resource $gray i3.color8 # Theme colors client.focused $bgcolor $bgcolor $white $green client.focused_inactive $gray $gray $black $gray client.unfocused $black $black $gray $black client.urgent $red $red $white $red # Start i3bar to display a workspace and status bar bar { status_command i3status position top workspace_min_width 25 colors { background $black statusline $white focused_workspace $bgcolor $bgcolor $white $black active_workspace $gray $gray $black $gray inactive_workspace $black $black $gray $gray urgent_workspace $red $red $white $green } } # restart i3 inplace (preserves layout/session, can be used to upgrade i3) bindsym $mod+Shift+r restart # vim: ft=i3config It\u0026rsquo;s possible some of these configs will become compatible with sway over time, but at the time of this writing they are not.\nsway-only config options live in ~/.config/sway/conf.d/sway.conf. To give you an idea of what it looks like and which options aren\u0026rsquo;t compatible with i3, here\u0026rsquo;s a snapshot of my config in early 2022: # sway(1) only config file # Commands herein are not compatible or interoperable with i3(1) # References: # sway(5) # https://github.com/swaywm/sway/wiki # https://github.com/swaywm/sway/wiki/Useful-add-ons-for-sway # HiDPI output \u0026#34;*\u0026#34; scale 1.5 # Wallpaper output \u0026#34;*\u0026#34; bg ~/.wallpaper fill # Gaps a la i3-gaps gaps inner 10 # XF86AudioPlayPause is not recognized by sway: xmodmap -pke | grep XF86AudioPlay # https://github.com/swaywm/sway/issues/4783 bindcode 172 exec playerctl play-pause # Start i3bar to display a workspace and status bar bar { status_command i3status position top workspace_min_width 25 } # restart i3 inplace (preserves layout/session, can be used to upgrade i3) bindsym $mod+Shift+r exec sway reload # vim: ft=i3config Most of those are wayland-specific options.\nQuirks gaps is available in i3 as well but only if you use i3-gaps, which generally I refuse to in order to stay closer to vanilla/upstream i3.\nThe play-pause multimedia key is a bug I found on sway. It\u0026rsquo;s quite annoying, the workaround as you can see above is to use bindcode instead of bindsym. For more details see the bug.\nIn general sway works very well out-of-the-box so long as you install XWayland (xorg-xwayland on Arch Linux). XWayland transparently proxies X11 apps to a X11 server that runs inside wayland.\nIt\u0026rsquo;s possible to detect those apps by running xprop and trying to click a window: If you cannot do it, then the window is not a X11 app. Alternatively xeyes is another way to detect them.\nTo achieve a 100% Xorg/X11-free experience with pure wayland, just add xwayland disable to the sway config. I wouldn\u0026rsquo;t recommend that though, most Linux GUI apps aren\u0026rsquo;t Wayland ready and will probably never be. To put it another way, X11/Xorg will take a long time (if ever) to disappear the same way that IPv4 will take a long time (if ever) to let IPv6 completely replace it. That\u0026rsquo;s life.\nX11 apps look a bit blurry in a 4K monitor with scaled DPI (\u0026gt;96) when they run inside Wayland with XWayland. I am not particularly bothered by that, but it\u0026rsquo;s noticeable.\nThere\u0026rsquo;s no need to replace all of your small i3 Xorg utilities with wayland ones. For example, rofi (application launcher) works just fine (no need for wofi). The stock i3 bar (sway bar?) works just fine, there\u0026rsquo;s no need for polybar or waybar.\nSome utilities need to be replaced though. For example, dunst (notification daemon) does not seem to work with sway out-of-the-box, mako seems to be a recommended replacement. i3lock (lock screen) also does not work, sway comes with its own screen lock directives. Screenshotters (e.g. scrot) will also need to be replaced.\nThe system tray does not seem to work fine out-of-the-box. I haven\u0026rsquo;t investigated much to figure out what\u0026rsquo;s wrong with it.\nI was looking for a display manager that works well with both X11 and Xorg and ended up trying greetd, emptty and ly, in that order. ly is in my opinion the best one in terms of balancing simplicity and usefulness.\nsway / XWayland doesn\u0026rsquo;t source ~/.Xresources. This is an issue if you rely on customizations therein. It does source ~/.Xdefaults though! Leveraging this, I did the following changes:\n(i) ~/.Xresources sources ~/.Xdefaults: $ cat ~/.Xresources ! These settings apply to X11 only. ! Use ~/.Xdefaults for settings that apply to both X11 and Wayland (xorg-xwayland). #include \u0026#34;.Xdefaults\u0026#34; ! Source: ! xrdb -merge ~/.Xresources ! ! Dump all properties: ! xrdb -q ! ! Check if DPI is set: ! xrdb -q | grep -i dpi ! HiDPI ! Common values: ! 96 (x1.0, baseline) ! 144 (x1.5) ! 192 (x2.0, HiDPI) *.dpi: 144 (ii) ~/.Xdefaults holds my customizations that originally lived in ~/.Xresources: $ cat ~/.Xdefaults ! These settings apply to both X11 and Wayland (xorg-xwayland). ! Use ~/.Xresources for X11-only settings. Xft.antialias: true Xft.hinting: true ... In principle I could just have symlinked them:\n$ ln -s ~/.Xresources ~/.Xdefaults The reason why I didn\u0026rsquo;t do it is to avoid double scaling (DPI). You see, my sway config already sets DPI / scaling to 1.5x. If we do that in ~/.Xdefaults as well then Xorg applications would have been scaled twice.\nClosing remarks In general Wayland / sway works reasonably well out-of-the-box in 2022, but tiny adjustments are still necessary, and it isn\u0026rsquo;t as polished as it could have been. Furthermore, my workflow is very simple. Try sharing your screen in a video call in Wayland and you\u0026rsquo;ll run into other quirks. I have mixed feelings and wouldn\u0026rsquo;t necessarily recommend it. I wouldn\u0026rsquo;t give an anti recommendation either. It\u0026rsquo;s complicated\u0026hellip;even though Wayland is supposed to overcome some X11 / Xorg limitations, as a client and without knowing its internals I fail to see its advantages.\n","permalink":"https://www.perrotta.dev/2022/02/wayland-from-i3-to-sway/","summary":"\u003cp\u003eI\u0026rsquo;ve been giving Wayland a try. My window manager of choice in X11/Xorg is \u003ca href=\"https://i3wm.org/\"\u003e\u003ccode\u003ei3\u003c/code\u003e\u003c/a\u003e, so the natural choice in Wayland is \u003ca href=\"https://swaywm.org/\"\u003e\u003ccode\u003esway\u003c/code\u003e\u003c/a\u003e.\u003c/p\u003e","title":"★ Wayland: from i3 to sway"},{"content":"This blog is managed and generated with Hugo, which is a SSG (static site generator), which basically means I write all my blog posts in static markdown files, off-line, and then do git commit + git push to publish them.\nThe advantage of this workflow is that it\u0026rsquo;s really minimalist, efficient and portable: I can use whatever text editor I want to1, from pretty much any operating system I want to, even from my phone if I am really inclined2.\nThat said, sometimes I am on the go with a Chromebook and don\u0026rsquo;t have easy access to a machine to ssh to. Sure, I could just write a post in a note-taking app like Google Keep, Standard Notes or Simple Note and publish it later. But what if I wanted to publish it right away?\nIt would be really nice if I could just pop up an editor in a web browser just like the cool kids do with WordPress, Medium and Substack\u0026hellip;\nMike Stone describes one way to do so, where he edits it directly from GitHub. That works fine, but then you need to write your post all at once, there\u0026rsquo;s no \u0026ldquo;save and continue later\u0026rdquo;.\nI think a better approach is to use GitHub codespaces: I go to https://github.dev/thiagowfx/thiagowfx.github.io where there\u0026rsquo;s a Visual Studio Code instance running on the web, make my edits or compose a new post therein, and then git push. Even if I don\u0026rsquo;t want to git push right away, I could just come back later and continue it from where I stopped. It\u0026rsquo;s brilliant! It even has terminal access if needed (e.g. to play with hugo on the go).\nI mostly use vim to compose these blog posts, and occasionally Visual Studio Code (vscode).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThere is a ton of markdown note-taking apps these days. Bear ($$) seems to be a popular one for iOS, but even the stock Notes app is decent for drafting blog posts.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/02/hugo-compose-or-edit-blog-posts-from-the-web/","summary":"\u003cp\u003eThis blog is managed and generated with \u003ca href=\"https://gohugo.io\"\u003eHugo\u003c/a\u003e, which is\na \u003ca href=\"https://jamstack.org/generators/\"\u003eSSG\u003c/a\u003e (static site generator), which\nbasically means I write all my blog posts in static markdown files, off-line,\nand then do \u003ccode\u003egit commit\u003c/code\u003e + \u003ccode\u003egit push\u003c/code\u003e to publish them.\u003c/p\u003e\n\u003cp\u003eThe advantage of this workflow is that it\u0026rsquo;s really minimalist, efficient and\nportable: I can use whatever text editor I want to\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e, from pretty much any\noperating system I want to, even from my phone if I am really inclined\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e\n\u003cp\u003eThat said, sometimes I am on the go with a Chromebook and don\u0026rsquo;t have easy\naccess to a machine to \u003ccode\u003essh\u003c/code\u003e to. Sure, I could just write a post in\na note-taking app like \u003ca href=\"https://keep.google.com/\"\u003eGoogle Keep\u003c/a\u003e, \u003ca href=\"https://standardnotes.com\"\u003eStandard\nNotes\u003c/a\u003e or \u003ca href=\"https://simplenote.com\"\u003eSimple Note\u003c/a\u003e and\npublish it later. But what if I wanted to publish it right away?\u003c/p\u003e\n\u003cp\u003eIt would be really nice if I could just pop up an editor in a web browser just like the cool kids do with WordPress, Medium and Substack\u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://mikestone.me/creating-on-github/\"\u003eMike Stone\u003c/a\u003e describes one way to do so, where he edits it directly from GitHub. That works fine, but then you need to write your post all at once, there\u0026rsquo;s no \u0026ldquo;save and continue later\u0026rdquo;.\u003c/p\u003e\n\u003cp\u003eI think a better approach is to use \u003ca href=\"https://www.perrotta.dev/2022/01/ephemeral-linux-shell-access-in-the-cloud/\"\u003eGitHub codespaces\u003c/a\u003e: I go to \u003ca href=\"https://github.dev/thiagowfx/thiagowfx.github.io\"\u003ehttps://github.dev/thiagowfx/thiagowfx.github.io\u003c/a\u003e where there\u0026rsquo;s a Visual Studio Code instance running on the web, make my edits or compose a new post therein, and then \u003ccode\u003egit push\u003c/code\u003e. Even if I don\u0026rsquo;t want to \u003ccode\u003egit push\u003c/code\u003e right away, I could just come back later and continue it from where I stopped. It\u0026rsquo;s brilliant! It even has terminal access if needed (e.g. to play with \u003ccode\u003ehugo\u003c/code\u003e on the go).\u003c/p\u003e","title":"Hugo: compose or edit blog posts from the web"},{"content":"As much as I love my Miniflux setup, I\u0026rsquo;ve also come to appreciate some indie blog aggregators. They are especially handy when I feel like breaking out of my own bubble and/or getting the occasional serendipity dose to discover new blogs to follow.\nHere\u0026rsquo;s a non-exhaustive list in alphabetical order:\n(i) Aggregation of blogs, acts as poor man\u0026rsquo;s RSS feed readers that someone else manages:\nBlog Surf, by DKB: See about. Diff Blog: See FAQ. (ii) Article submission websites, where their users can submit articles which are then upvoted (or not) by other users (wisdom of the crowds):\nHacker News. You probably know this one already. The /best page should yield a higher SNR1. Alternatively, to consume the best of hacker news directly in your RSS feed, you can use something like HNRSS to filter out all submissions below a certain threshold. Lobsters. See about. Lobsters is basically a more niche version of Hacker News. One distinctive feature is that new users can only join if they are invited by another user2. And for completeness I\u0026rsquo;ll also mention Reddit but everyone already knows it at this point.\nSignal-to-noise ratio.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPlease invite me oh, dear reader. I do not currently have an account there.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/02/praise-for-blog-aggregators/","summary":"\u003cp\u003eAs much as I love my \u003ca href=\"https://miniflux.app/\"\u003eMiniflux\u003c/a\u003e setup, I\u0026rsquo;ve also come to\nappreciate some indie blog aggregators. They are especially handy when I feel\nlike breaking out of my own bubble and/or getting the occasional serendipity dose\nto discover new blogs to follow.\u003c/p\u003e","title":"Praise for blog aggregators"},{"content":"I am currently evaluating Nix as a replacement for Homebrew CLI apps in macOS1. Others have previously written about this.\nMy goal is to keep a sane learning curve and learn things on-the-fly, only as needed. Nix is a massive ecosystem and has so many batteries included and components (NixOS, NixPkgs, NixOps, Nix programming language, nix-shell, nix-env, nix-darwin, home-manager, \u0026hellip;). The good news is that those components are for the most part modular, there\u0026rsquo;s no need to adopt them all in order to reap the benefits that Nix provides.\nFor now, I am only adopting nix-env and nix-shell, with no *.nix config files. This post covers nix-env.\nFor simplicity, think of nix-env as a package manager, akin to apk, pacman, brew, apt, pkg, etc.\nInstall a package $ nix-env -i moreutils installing \u0026#39;moreutils-0.67\u0026#39; building \u0026#39;/nix/store/jsp0l5ny3kx8p9lx9w9r0x159i9jjnn6-user-environment.drv\u0026#39;... I see some guides using nix-env -iA but -i seems to suffice. We could optionally specify the nixpkgs. prefix:\n$ nix-env -i nixpkgs.moreutils error: selector \u0026#39;nixpkgs.moreutils\u0026#39; matches no derivations Oh no! Maybe that\u0026rsquo;s what the -A is for?\n$ nix-env -iA nixpkgs.moreutils replacing old \u0026#39;moreutils-0.67\u0026#39; installing \u0026#39;moreutils-0.67\u0026#39; Indeed! Apparently that -A thing stands for attribute. The only thing I know is that there are both nixpkgs.* and nixos.*. But I don\u0026rsquo;t care about NixOS at this point. I\u0026rsquo;ll just ignore -A from now on, for the time being.\nList installed packages $ nix-env -q moreutils-0.67 Easy! This actually gets displayed in my less pager.\nUpgrade installed packages $ nix-env -u Easy! At this point, I am not super confident whether that works as intended though. We will find out in a few days when there\u0026rsquo;s some update to one of my installed CLI applications. I\u0026rsquo;ve heard there\u0026rsquo;s something called nix channel to control that. Leaving it for another day though.\nUpdate(2022-02-18): I learned that nix-env -u is akin to apt upgrade or apk upgrade. It upgrades installed packages to newer versions but only if it is aware there are newer versions. To actually refresh the repositories à la apt update or apk update, use nix-channel --update.\nNote: On macOS this needs to be sudo -i nix-channel --update. See issue.\nUninstall a package $ nix-env --uninstall moreutils uninstalling \u0026#39;moreutils-0.67\u0026#39; building \u0026#39;/nix/store/5k8rsf4cxg4iz7cqnqirpww6r97bwnqr-user-environment.drv\u0026#39;... Easy!\nSearch for packages $ nix-env -qaP \u0026#39;.*moreutils.*\u0026#39; The .* seems to be needed. It works if I omit them, but only if I write the exact package name (apparently called \u0026lsquo;derivation\u0026rsquo; in Nix):\n$ nix-env -qaP moreutils nixpkgs.moreutils moreutils-0.67 If I write the wrong package name, the following happens:\n$ nix-env -qaP moreutil error: selector \u0026#39;moreutil\u0026#39; matches no derivations, maybe you meant: moreutils It was helpful in this case, but I wouldn\u0026rsquo;t always count on that. It is a bit annoying that there\u0026rsquo;s no nix search moreutils command, but it seems that nix-env is very heavily tailored to use short flags, just like pacman in Arch Linux. I got used to pacman, hopefully I can get used to the nix-env short flags at some point.\nActually I tried it out and there is a nix search command!\n$ nix search moreutils error: experimental Nix feature \u0026#39;nix-command\u0026#39; is disabled; use \u0026#39;--extra-experimental-features nix-command\u0026#39; to override This isn\u0026rsquo;t very promising though. How come searching is experimental?! Anyway, I can live with the nix-env form for now.\nThese are the 5 basic package management operations that I needed to bootstrap my dev environment. Without putting much effort on it, my initial list of package looks like this:\n$ nix-env -q atool-0.39.0 bash-interactive-5.1-p12 coreutils-9.0 exa-0.10.1 fpp-0.9.2 fzf-0.29.0 git-2.34.1 htop-3.1.2 hugo-0.92.0 jq-1.6 less-600 moreutils-0.67 ncdu-1.16 perl5.34.0-ack-3.5.0 ranger-1.9.3 stow-2.3.1 tmux-3.2a tree-1.8.0 vim-8.2.4186 watch-procps-3.3.16 wget-1.21.2 zoxide-0.8.0 Those were very intuitive to find, with the exception of ack and bash-interactive:\nbash is a bit odd because Nix splits it into two packages: a non-interactive version and an interactive version. I have no idea why. My ~/.bashrc wrecked havoc with the non-interactive version. ack is very oddly named. Really. Also: nix-env -i ack doesn\u0026rsquo;t work, but nix-env -iA nixpkgs.ack does. I suspect it will be hard to ignore -A in the future. Strictly speaking there\u0026rsquo;s nothing special about macOS in this context. The same setup can also be used in Linux distributions, for example, Debian or Ubuntu. In fact, this is what I did at $DAYJOB, because relying solely on Debian for package management is a very big limitation. I find that Nix complements the Debian repositories very well, the same way that it does for macOS.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/02/nix-env-in-a-nutshell-for-basic-usage-in-macos/","summary":"\u003cp\u003eI am currently evaluating \u003ca href=\"https://nixos.org/download.html\"\u003eNix\u003c/a\u003e as a\nreplacement for \u003ca href=\"https://brew.sh\"\u003eHomebrew\u003c/a\u003e CLI apps in macOS\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e.\n\u003ca href=\"https://wickedchicken.github.io/post/macos-nix-setup/\"\u003eOthers\u003c/a\u003e\n\u003ca href=\"https://ianthehenry.com/posts/how-to-learn-nix/switching-from-homebrew-to-nix/\"\u003ehave\u003c/a\u003e\n\u003ca href=\"https://ghedam.at/15490/so-tell-me-about-nix\"\u003epreviously\u003c/a\u003e written about this.\u003c/p\u003e\n\u003cp\u003eMy goal is to keep a sane learning curve and learn things on-the-fly, only as\nneeded. Nix is a massive ecosystem and has so many batteries included and\ncomponents (NixOS, NixPkgs, NixOps, Nix programming language, nix-shell,\nnix-env, nix-darwin, home-manager, \u0026hellip;). The good news is that those components\nare for the most part modular, there\u0026rsquo;s no need to adopt them all in order to\nreap the benefits that Nix provides.\u003c/p\u003e\n\u003cp\u003eFor now, I am only adopting \u003ccode\u003enix-env\u003c/code\u003e and \u003ccode\u003enix-shell\u003c/code\u003e, with no \u003ccode\u003e*.nix\u003c/code\u003e config\nfiles. This post covers \u003ccode\u003enix-env\u003c/code\u003e.\u003c/p\u003e","title":"★ nix-env in a nutshell for basic usage in macOS"},{"content":"One of the most classic sysadmin/DevOps tasks is to use secure shell to connect to remote machines.\nTo persist those connections, a terminal multiplexer is often used, tmux and screen being the two most popular ones.\nIn this post I will cover a few different client-side and server-side ways to have ssh automatically spawn tmux upon connection.\nOption #1: Use command-line ssh flags (client-side, recommended) Start tmux, forcing unicode, attaching to and/or creating a session named main:\n$ ssh user@host -t -- tmux -u new -A -s main -u is not strictly necessary, however I experienced occasional weirdness when connecting to some machines and omitting it. Some unicode characters wouldn\u0026rsquo;t be properly rendered, like the horizontal and vertical lines used to render tmux pane splits. Even though most machines should work just fine these days by supporting UTF-8 out-of-the-box, it\u0026rsquo;s safer to always include -u just in case.\nTip: If it\u0026rsquo;s annoying to remember to type the full command above, consider adding an alias in your shell config. Alternatively, use a ssh client that remembers your flags preferences such as the chrome secure shell extension.\nOption #2: Use ~/.ssh/config (client-side) This option is very similar to the previous one, but the flags live in the ssh config rather then being specified at the command line:\n$ cat ~/.ssh/config Host * RequestTTY yes RemoteCommand tmux -u new -A -s main You don\u0026rsquo;t need to match all hosts (Host *), if you\u0026rsquo;d rather match one or more specific hosts, refer to the ssh config syntax ssh_config(5) to add them. A simple example would be Host mymachine.example.org.\nCaveat: I\u0026rsquo;ve found this method interferes with git + ssh authentication. More specifically:\n$ git remote -v origin\tgit@github.com:thiagowfx/.dotfiles.git (fetch) origin\tgit@github.com:thiagowfx/.dotfiles.git (push) $ git push Cannot execute command-line and remote command. fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists. Therefore I discourage it, unless you only use it with specific hosts i.e. don\u0026rsquo;t use it with Host *.\nOption #3: Use ~/.bash_profile or similar (server-side, recommended) This method leverages your login shell startup config file (~/.bash_profile, ~/.zprofile, etc) to automatically spawn tmux.\n# This file is invoked as part of my ~/.bash_profile. $ cat ~/.profile.d/tmux_auto_ssh.sh. # Automatically spawn tmux within ssh sessions for interactive terminals. # https://stackoverflow.com/a/43819740/1745064 # # The session is called `main`. # Create a session with PREFIX :new, rename with PREFIX $, toggle with PREFIX s. # # Escape hatch: # ssh \u0026lt;host\u0026gt; -t -- NOTMUX=1 \u0026lt;shell\u0026gt; if [ -z \u0026#34;$NOTMUX\u0026#34; ] \u0026amp;\u0026amp; [ -z \u0026#34;$TMUX\u0026#34; ] \u0026amp;\u0026amp; [ -n \u0026#34;$SSH_TTY\u0026#34; ] \u0026amp;\u0026amp; [[ $- =~ i ]]; then tmux -u new -A -s main exit fi The if basically checks:\nwhether we\u0026rsquo;re not already inside a tmux session (we shouldn\u0026rsquo;t be), so that we don\u0026rsquo;t nest tmux whether we\u0026rsquo;re accessing the shell via ssh (we should be) whether we\u0026rsquo;re accessing an interactive shell (we should be), so that it doesn\u0026rsquo;t interefere with oneshot ssh commands There\u0026rsquo;s also a escape hatch. If you want to get an interactive shell but bypass tmux for some reason1, just set NOTMUX=1:\n$ ssh user@host -t -- NOTMUX=1 bash Final remarks My favorite methods are #1 and #3, and whether I use one or the other depends whether I want to unconditionally spawn tmux server-side, or selectively spawn tmux client-side.\nWhen using chrome secure shell (hterm) I find #1 convenient because hterm remembers your ssh host settings. That said, in scenarios where I fully control a host and it\u0026rsquo;s not solely used for production, #3 is my favorite as it works unconditionally regardless of the client terminal emulator I am using.\nFor example, maybe if tmux broke due to a recent upgrade, or if the ~/.tmux.conf is invalid.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/02/ssh-plus-tmux-automatically/","summary":"\u003cp\u003eOne of the most classic sysadmin/DevOps tasks is to use secure shell to connect to remote machines.\u003c/p\u003e\n\u003cp\u003eTo persist those connections, a terminal multiplexer is often used, \u003ccode\u003etmux\u003c/code\u003e and \u003ccode\u003escreen\u003c/code\u003e being the two most popular ones.\u003c/p\u003e\n\u003cp\u003eIn this post I will cover a few different client-side and server-side ways to have \u003ccode\u003essh\u003c/code\u003e automatically spawn \u003ccode\u003etmux\u003c/code\u003e upon connection.\u003c/p\u003e","title":"★ SSH plus tmux automatically"},{"content":"Sometimes I fire up a python interpreter in my terminal for quick prototyping, but often forget what the standard library method signatures are.\nFor example, how should I invoke subprocess.call?\nThe most straightforward action at this point is to simply google it, no shame. The first result helpfully redirects me to the official python documentation, as one would expect.\nFrom the documentation, I\u0026rsquo;d run something like this:\nsubprocess.call([\u0026#34;ls\u0026#34;, \u0026#34;-al\u0026#34;], cwd=\u0026#39;/tmp\u0026#39;) What if I wanted to figure out the correct way to do so from the command line though?\nbpython Enter bpython:\nbpython is a fancy interface to the Python interpreter for Linux, BSD, OS X and Windows (with some work). bpython is released under the MIT License. It has the following (special) features:\nIt should be available in your favorite linux distribution. Once it\u0026rsquo;s installed, a typical session would look like this:\n% bpython bpython version 0.22.1 on top of Python 3.10.2 /usr/bin/python \u0026gt;\u0026gt;\u0026gt; import subprocess \u0026gt;\u0026gt;\u0026gt; subprocess.call([\u0026#34;ls\u0026#34;, \u0026#34;-la\u0026#34;], cwd=\u0026#39;/tmp\u0026#39;) ┌──────────────────────────────────────────────────────────────────────────────────────┐ │ subprocess.call: (*popenargs, timeout=None, **kwargs) │ │ call │ │ Run command with arguments. Wait for command to complete or │ │ timeout, then return the returncode attribute. │ │ │ │ The arguments are the same as for the Popen constructor. Example: │ │ │ │ retcode = call([\u0026#34;ls\u0026#34;, \u0026#34;-l\u0026#34;]) │ └──────────────────────────────────────────────────────────────────────────────────────┘ To see all Popen arguments:\n\u0026gt;\u0026gt;\u0026gt; subprocess.Popen( ┌──────────────────────────────────────────────────────────────────────────────────────┐ │ subprocess.Popen: (args, bufsize=-1, executable=None, stdin=None, stdout=None, │ │ stderr=None, preexec_fn=None, close_fds=True, shell=False, cwd=None, env=None, │ │ universal_newlines=None, startupinfo=None, creationflags=0, restore_signals=True, │ │ start_new_session=False, pass_fds=(), *, user=None, group=None, extra_groups=None, │ │ encoding=None, errors=None, text=None, umask=-1, pipesize=-1) │ │ Popen │ │ Execute a child program in a new process. │ │ │ │ For a complete description of the arguments see the Python documentation. │ │ │ │ Arguments: │ │ args: A string, or a sequence of program arguments. │ # output truncated for brevity; bpython displays it all As you can see, it wouldn\u0026rsquo;t be difficult to have a rough idea of which arguments are available and what they do.\nI could keep going:\n\u0026gt;\u0026gt;\u0026gt; p = subprocess.run([\u0026#34;ls\u0026#34;, \u0026#34;-la\u0026#34;], cwd=\u0026#39;/tmp\u0026#39;) \u0026gt;\u0026gt;\u0026gt; p. ┌──────────────────────────────────────────────────────────────────────────────────────┐ │ args check_returncode returncode stderr │ │ stdout │ └──────────────────────────────────────────────────────────────────────────────────────┘ \u0026gt;\u0026gt;\u0026gt; p.args. ┌──────────────────────────────────────────────────────────────────────────────────────┐ │ append clear copy count extend │ │ index insert pop remove reverse │ │ sort │ └──────────────────────────────────────────────────────────────────────────────────────┘ Out-of-the-box it also displays autosuggestions based on the history of my previous commands1. It also supports python 3. For the full list of features, refer to https://bpython-interpreter.org/.\nipython Alternatively ipython2 is comparable to bpython, however I find it a bit less user-friendly out-of-the-box:\n% ipython iPython 3.10.2 (main, Jan 15 2022, 19:56:27) [GCC 11.1.0] Type \u0026#39;copyright\u0026#39;, \u0026#39;credits\u0026#39; or \u0026#39;license\u0026#39; for more information IPython 8.0.1 -- An enhanced Interactive Python. Type \u0026#39;?\u0026#39; for help. In [1]: import subprocess In [2]: subprocess. builtins contextlib io select threading call() DEVNULL list2cmdline() selectors time CalledProcessError errno os signal TimeoutExpired check_call() fcntl PIPE STDOUT types check_output() getoutput() Popen SubprocessError warnings In [2]: subprocess.call( abs() False ModuleNotFoundError SystemError all() FileExistsError NameError SystemExit any() FileNotFoundError next() TabError ArithmeticError filter() None timeout= ascii() float NotADirectoryError TimeoutError The tab completion after call( doesn\u0026rsquo;t display the documentation for it. However, appending a ? works:\n% ipython Python 3.10.2 (main, Jan 15 2022, 19:56:27) [GCC 11.1.0] Type \u0026#39;copyright\u0026#39;, \u0026#39;credits\u0026#39; or \u0026#39;license\u0026#39; for more information IPython 8.0.1 -- An enhanced Interactive Python. Type \u0026#39;?\u0026#39; for help. In [1]: import subprocess In [2]: subprocess.call? Signature: subprocess.call(*popenargs, timeout=None, **kwargs) Docstring: Run command with arguments. Wait for command to complete or timeout, then return the returncode attribute. The arguments are the same as for the Popen constructor. Example: retcode = call([\u0026#34;ls\u0026#34;, \u0026#34;-l\u0026#34;]) File: /usr/lib/python3.10/subprocess.py Type: function Furthermore, subprocess.Popen? opens a pager with the documentation for the method.\nConclusion Both bpython and ipython are excellent tools to enhance the user experience within the python interpreter, being great for quick prototyping, experimentation or exploration. bpython seems a bit more user-friendly and intuitive upon first usage, ipython takes a bit getting used to.\nfish shell and zsh-autosuggestions users should know what I\u0026rsquo;m talking about.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nipython has been around for longer and these days there\u0026rsquo;s the whole Jupyter Notebook ecosystem around it.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/02/python-interactive-completion/","summary":"\u003cp\u003eSometimes I fire up a \u003ccode\u003epython\u003c/code\u003e interpreter in my terminal for quick\nprototyping, but often forget what the standard library method signatures are.\u003c/p\u003e\n\u003cp\u003eFor example, how should I invoke \u003ccode\u003esubprocess.call\u003c/code\u003e?\u003c/p\u003e","title":"Python: interactive completion"},{"content":"As soon as we finish installing Nix on Darwin, we\u0026rsquo;re greeted with a call to action:\nAlright! We\u0026#39;re done! Try it! Open a new terminal, and type: $ nix-shell -p nix-info --run \u0026#34;nix-info -m\u0026#34; Thank you for using this installer. If you have any feedback or need help, don\u0026#39;t hesitate: You can open an issue at https://github.com/nixos/nix/issues Hello world (bloated) All right then, let\u0026rsquo;s do it!\n$ nix-shell -p nix-info --run \u0026#34;nix-info -m\u0026#34; - system: `\u0026#34;aarch64-darwin\u0026#34;` - host os: `Darwin 21.3.0, macOS 12.2` - multi-user?: `yes` - sandbox: `no` - version: `nix-env (Nix) 2.6.0` - channels(root): `\u0026#34;nixpkgs\u0026#34;` - nixpkgs: `/nix/var/nix/profiles/per-user/root/channels/nixpkgs` Cool, it works. Let\u0026rsquo;s break it down a bit.\nHello world (classic) Nix shell creates an ephemeral shell environment with the customizations you want. The most basic customization is to make a given set of packages available. There\u0026rsquo;s a hello package:\n$ nix-shell -p hello $ hello Hello, world! In case you\u0026rsquo;re curious, this is a GNU binary:\n$ hello --version hello (GNU Hello) 2.10 Copyright (C) 2014 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later \u0026lt;http://gnu.org/licenses/gpl.html\u0026gt; This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. I have no idea why they are in version 2.10 and what their changelog is. It\u0026rsquo;s such a simple binary\u0026hellip;\nIf you exit the shell, hello seemingly vanishes:\n$ exit exit $ hello zsh: command not found: hello An easy way to think of nix-shell is like an ephemeral sandbox where all your desired packages are made available when you enter it. It\u0026rsquo;s possible to provide more than one package, naturally. It\u0026rsquo;s also possible to provide a shell.nix file with the package declarations, so that when you can nix-shell without any arguments.\n$ cat shell.nix { pkgs ? import \u0026lt;nixpkgs\u0026gt; {} }: pkgs.mkShell { # nativeBuildInputs is usually what you want -- tools you need to run nativeBuildInputs = [ pkgs.buildPackages.hello ]; } $ nix-shell $ hello Hello, world! Hello world (oneshot) $ nix-shell -p hello --run hello Hello, world! This oneshot style doesn\u0026rsquo;t enter the shell, it just runs the given --run command and then exits.\nThis post just scratched the surface of what nix-shell can do. See the references below for more in-depth guides about it.\nReferences Tools You Should Know About: nix-shell An introduction to nix-shell NixOS manual: nix-shell ","permalink":"https://www.perrotta.dev/2022/02/nix-shell-in-a-nutshell/","summary":"\u003cp\u003eAs soon as we finish installing \u003ca href=\"https://nixos.org/download.html\"\u003e\u003ccode\u003eNix\u003c/code\u003e\u003c/a\u003e on\nDarwin, we\u0026rsquo;re greeted with a call to action:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eAlright! We\u0026#39;re done!\nTry it! Open a new terminal, and type:\n\n  $ nix-shell -p nix-info --run \u0026#34;nix-info -m\u0026#34;\n\nThank you for using this installer. If you have any feedback or need\nhelp, don\u0026#39;t hesitate:\n\nYou can open an issue at https://github.com/nixos/nix/issues\n\u003c/code\u003e\u003c/pre\u003e","title":"nix-shell in a nutshell"},{"content":"Not everything is available via RSS. However, there are some decent workarounds in a few situations.\nNewsletters Some blogs and authors refuse to provide RSS feeds to their websites. Instead, they will only provide newsletters. This is very hostile to the open web, and the main reason why it\u0026rsquo;s done is so that these authors can own a direct channel to reach out to their audience directly, which is better for (their) business, making it easier for them to push sponsored and promoted content and measure engagement metrics and analytics.\nKill the Newsletter is a service that proxies those newsletters, publishing them as RSS feeds.\nYou can either self-host it or use its official hosted version at https://kill-the-newsletter.com/.\nTwitter Use Nitter:\nNitter is a free and open source alternative Twitter front-end focused on privacy. The source is available on GitHub at https://github.com/zedeus/nitter\nFurthermore, it has built-in RSS support!\nFor example, you can see @taylorswift13\u0026rsquo;s profile on Nitter at https://nitter.net/taylorswift13 and follow her via RSS with https://nitter.net/taylorswift13/rss — by merely appending /rss to it.\nYou can either self-host it or use one of its public instances. At the time of this writing the official instance is https://nitter.net.\nReddit Reddit famously includes RSS support for every subreddit, for example1: https://www.reddit.com/r/archlinux/.rss. It has a lot of noise though as it includes all recent posts including the ones with a few number of votes.\nTo experience a higher quality, filtered version of the latest given subreddit posts with more than a certain threshold (of your choosing) of upvotes, check out the reddit-top-rss project:\nReddit Top RSS is a set of scripts for Reddit\u0026rsquo;s API that generates RSS feeds for specified subreddits with score thresholds. To preview your outputted feed items there is a front end that utilizes the Bootstrap v4 framework.\nYou\u0026rsquo;re supposed to self-host it, but there\u0026rsquo;s a demo version available at https://reddit-top-rss.herokuapp.com/.\nAppendix For more RSS bridges and resources, see:\nhttps://github.com/RSS-Bridge/rss-bridge https://github.com/AboutRSS/ALL-about-RSS Disclaimer: I do not endorse these lists of resources. Use them at your own risk.\nThe last slash isn\u0026rsquo;t strictly necessary: https://www.reddit.com/r/archlinux.rss is also valid.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/02/rss-bridging-the-gap/","summary":"\u003cp\u003eNot everything is available via RSS. However, there are some decent workarounds in a few situations.\u003c/p\u003e","title":"RSS: bridging the gap"},{"content":"From the homepage:\nGemini is a new internet protocol which:\nIs heavier than gopher Is lighter than the web Will not replace either Strives for maximum power to weight ratio Takes user privacy very seriously That\u0026rsquo;s too abstract though. I prefer the way Kev Quirk puts it:\nTo put that into human-digestible form; Gemini is basically a very light, text-only alternative to HTML.\nGemini aims to replace \u0026ldquo;lightweight HTML\u0026rdquo;, but it already starts with a big barrier for entry and adoption: It\u0026rsquo;s not obvious what it is by just reading its project homepage alone. This in my opinion comes off as elitist.\nFurthermore, you need a custom piece of software in order to consume the so-called gemini capsules (a fancy name for what\u0026rsquo;s the equivalent of a plain-text SSG website).\nI tried out amfora which is a popular CLI one1. Amfora is pretty decent and lightweight. The experience is very similar to a hybrid of using a CLI RSS reader like newsboat to keep track of your favorite capsules, and a CLI Web browser like elinks or w3m to navigate them.\nAnd that\u0026rsquo;s part of the adoption problem: Why would you subject yourself to purposely using a text-only browser in the 2020s? It is a painful experience, and there\u0026rsquo;s not any extra value compared to just using a minimalist RSS reader like miniflux to keep track of your favorite blogs / news portals via RSS.\nNowadays there are plenty of SSGs, for every programming language you can think of, even in plain shell scripting (POSIX sh). There\u0026rsquo;s little reason to learn a new niche protocol given that it\u0026rsquo;s relatively easy to publish simple blogs.\nConclusion: As Kev puts it:\nI’m not sure if you heard, but The Web Is F*cked and techies everywhere are touting the Gemini protocol as its saviour. I disagree. A lot.\nI will end this article with a praise for Gemini, courtesy of Drew DeVault. Drew argues that:\nMy disdain for web browsers is well documented. Web browsers are extraordinarily complex, and any attempt to build a new one would be a Sisyphean task. Successfully completing that implementation, if even possible, would necessarily produce a Lovecraftian mess: unmaintainable, full of security vulnerabilities, with gigabytes in RAM use and hours in compile times. And given that all of the contemporary web browsers that implement a sufficiently useful subset of web standards are ass and getting assier, what should we do?\nFine, but the beloved plain duo of HTML + CSS still works just fine. There\u0026rsquo;s no need to create a new, difficult-to-use protocol to force people to keep things simple. Unless you just wanna have fun and treat it like a toy or learning project; then go for it. Nothing wrong with that.\nMy interest for Gemini ends as soon as this post is published. Q.E.D.\nPackaged for every relevant platform out there nowadays.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/02/thoughts-on-gemini/","summary":"\u003cp\u003eFrom the \u003ca href=\"https://gemini.circumlunar.space\"\u003ehomepage\u003c/a\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eGemini is a new internet protocol which:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIs heavier than gopher\u003c/li\u003e\n\u003cli\u003eIs lighter than the web\u003c/li\u003e\n\u003cli\u003eWill not replace either\u003c/li\u003e\n\u003cli\u003eStrives for maximum power to weight ratio\u003c/li\u003e\n\u003cli\u003eTakes user privacy very seriously\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThat\u0026rsquo;s too abstract though. I prefer the way \u003ca href=\"https://kevq.uk/gemini-isnt-the-solution-to-the-broken-web/\"\u003eKev Quirk\u003c/a\u003e puts it:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eTo put that into human-digestible form; Gemini is basically a very light, text-only alternative to HTML.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Thoughts on Gemini"},{"content":"doas is a lightweight and safer replacement for sudo. In most occasions you invoke it exactly like sudo:\n$ sudo apt install \u0026lt;foo\u0026gt; $ doas apt install \u0026lt;foo\u0026gt; doas has gained popularity recently. Besides being the default in OpenBSD, Alpine Linux 3.15 (released last year) has also switched to it:\ndoas is the default temporary privilege escalation tool. You are advised to migrate from sudo to doas as 3.15 will be the last release to support sudo throughout its full lifecycle, in 3.16 sudo will be moved from main to community.\nIt\u0026rsquo;s not very difficult to get used to it, however you may still find yourself writing sudo occasionally. This post highlights a few ways to bridge that gap.\nUse a shell alias In your ~/.bashrc or ~/.zshrc or in your favorite shell, do:\nalias sudo=doas Caveat: Besides being an user-dependent workaround1, doas isn\u0026rsquo;t really a full drop-in replacement to sudo. This workaround will work in most day-to-day situations but it will obviously not support most sudo specific flags.\nUse a shim/wrapper (recommended) Alpine Linux provides a doas-sudo-shim package:\n$ doas apk add doas-sudo-shim This is a shim for the sudo command that utilizes doas. It supports only a subset of the sudo options (both short and long variants) that have an equivalent in doas, plus option -i (--login).\nThis is a slightly better solution, as this thin wrapper is aware of some sudo flags, translating them to the equivalent doas ones; furthermore, it works out-of-the-box and it\u0026rsquo;s system-wide. As an added bonus, it\u0026rsquo;s implemented entirely in shell script, being as much portable as possible.\nFinal remarks Last but not least, you could choose to install sudo and configure it, keeping both doas and sudo, but what\u0026rsquo;s the point? If your system favours doas, stick to doas. There\u0026rsquo;s no need to unnecessarily increase complexity by keeping around two programs that serve exactly the same purpose.\nIf you don\u0026rsquo;t like or want doas for some reason, you could look into the other way around: find a doas shim that bridges to sudo, or define an alias: $ alias doas=sudo.\nThe best long-term solution though would be to just use doas without any alias or shim, but our muscle memory may have trouble adapting to that, especially when sudo is still the de facto standard in most Linux distributions out there these days.\nTo make it system-wide, change the relevant file in /etc: for example, /etc/bashrc for bash. I would advise against it though.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/02/doas-bridging-the-sudo-gap/","summary":"\u003cp\u003e\u003ca href=\"https://man.openbsd.org/doas\"\u003e\u003ccode\u003edoas\u003c/code\u003e\u003c/a\u003e is a lightweight and safer replacement for \u003ccode\u003esudo\u003c/code\u003e. In most occasions you invoke it exactly like \u003ccode\u003esudo\u003c/code\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e$ sudo apt install \u0026lt;foo\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e$ doas apt install \u0026lt;foo\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003ccode\u003edoas\u003c/code\u003e has gained popularity recently. Besides being the default in OpenBSD, Alpine Linux 3.15 (released last year) has also \u003ca href=\"https://wiki.alpinelinux.org/wiki/Release_Notes_for_Alpine_3.15.0#Move_from_sudo_to_doas\"\u003eswitched to it\u003c/a\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003ccode\u003edoas\u003c/code\u003e is the default temporary privilege escalation tool. You are advised to migrate from \u003ccode\u003esudo\u003c/code\u003e to \u003ccode\u003edoas\u003c/code\u003e as 3.15 will be the last release to support \u003ccode\u003esudo\u003c/code\u003e throughout its full lifecycle, in 3.16 \u003ccode\u003esudo\u003c/code\u003e will be moved from main to community.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIt\u0026rsquo;s not very difficult to get used to it, however you may still find yourself writing \u003ccode\u003esudo\u003c/code\u003e occasionally. This post highlights a few ways to bridge that gap.\u003c/p\u003e","title":"Doas: bridging the sudo gap"},{"content":"After years of using bash as my default interactive shell at $DAYJOB, I decided to switch to zsh. I didn\u0026rsquo;t want to start from scratch and lose all my history though:\n$ wc -l ~/.bash_history | cut -f1 -d\u0026#39; \u0026#39; 64002 Thus my goal was to first migrate all my history from bash to zsh.\nThe bash-to-zsh-hist.py python script in this gist did most of the job:\n#!/usr/bin/env python # -*- coding: utf-8 -*- # # This is how I used it: # $ cat ~/.bash_history | python bash-to-zsh-hist.py \u0026gt;\u0026gt; ~/.zsh_history import sys import time def main(): timestamp = None for line in sys.stdin.readlines(): line = line.rstrip(\u0026#39;\\n\u0026#39;) if line.startswith(\u0026#39;#\u0026#39;) and timestamp is None: t = line[1:] if t.isdigit(): timestamp = t continue else: sys.stdout.write(\u0026#39;: %s:0;%s\\n\u0026#39; % (timestamp or time.time(), line)) timestamp = None if __name__ == \u0026#39;__main__\u0026#39;: main() To use it:\n$ wget https://gist.githubusercontent.com/muendelezaji/c14722ab66b505a49861b8a74e52b274/raw/49f0fb7f661bdf794742257f58950d209dd6cb62/bash-to-zsh-hist.py $ chmod +x ./bash-to-zsh-hist.py $ cat .bash_history | ./bash-to-zsh-hist.py \u0026gt;\u0026gt; ~/.zsh_history However, that didn\u0026rsquo;t fully work. Upon running zsh, there was an error:\n$ zsh zsh: corrupt history file /usr/local/google/home/tperrotta/.zsh_history A quick google search led me to a blog post. I adapted the command suggest therein1:\n$ strings -eS .zsh_history | sponge .zsh_history And that fixed the issue!\nsponge comes from the moreutils package.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/02/transfer-bash-history-to-zsh/","summary":"\u003cp\u003eAfter years of using \u003ccode\u003ebash\u003c/code\u003e as my default interactive shell at $DAYJOB,\nI decided to switch to \u003ccode\u003ezsh\u003c/code\u003e. I didn\u0026rsquo;t want to start from scratch and lose all\nmy history though:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e$ wc -l ~/.bash_history | cut -f1 -d\u003cspan style=\"color:#e6db74\"\u003e\u0026#39; \u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e64002\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThus my goal was to first migrate all my history from \u003ccode\u003ebash\u003c/code\u003e to \u003ccode\u003ezsh\u003c/code\u003e.\u003c/p\u003e","title":"Transfer bash history to zsh"},{"content":"My search engine of choice is Google, nonetheless I still enjoy DuckDuckGo occasionally.\nThe main reason to stick with Google is its superior quality, by the means of better search results. 20 years later, it\u0026rsquo;s still arguably the best search engine. Of course, part of the reason for that is contingent upon how much data it collects, but that\u0026rsquo;s a topic for another day\u0026hellip;\nThere are at least three reasons to use DuckDuckGo as an alternative:\nit\u0026rsquo;s privacy-focused, there\u0026rsquo;s no tracking and no bubbling1 its search results stem from sources other than Google; these days, mostly from Bing it has !bangs, lots of them The quest(ion) then becomes: How can I use mostly Google, but still have quick access to DuckDuckGo bangs?\nThere are several ways to do so.\nDuckDuckGoog DuckDuckGoog is a search engine which does exactly that:\nSearches Google and !bangs DuckDuckGo. Tell your browser!\nIf I search for i3, it will open https://www.google.com/search?q=i3 and probably think I am interested in Intel i3 CPUs. If I search for !aw i3, using the ArchWiki bang, it opens https://wiki.archlinux.org/title/I3 and goes straight to the i3 window manager page in the ArchWiki, exactly what I wanted. If I search for !ddg i3, it opens https://duckduckgo.com/?q=i3, on DuckDuckGo. Caveat: You cannot add custom search engines to Safari, therefore this method only works in other browsers (Firefox, Chrome, etc).\nSelf-Hosted DuckDuckGoog claims to collect no data:\nIt\u0026rsquo;s quite simple. DuckDuckGoog doesn\u0026rsquo;t track any queries submitted whatsoever, It simply redirects you to DuckDuckGo or Google depending on whether your search contains a !bang or not.\nIf you still don\u0026rsquo;t trust it for some reason, you could also self-host it in your own server, as it\u0026rsquo;s open source.\nOne advantage of doing so is using (and owning) your own infrastructure, which is probably more reliable in terms of bandwidth and latency than a random guy\u0026rsquo;s server in the wild.\n!Bang Quick Search !Bang Quick Search is a Chrome extension:\nThis extension adds DuckDuckGo !bang search to chrome. You can use it from the URL bar as long as your default search engine is either google or bing (for now). You can also use it directly on google\u0026rsquo;s and bing\u0026rsquo;s websites.\nSo long as your search engine is set to either Google or Bing, it will intercept !bangs from your query to redirect them to DuckDuckGo.\nTip: Use !ddg to search on DuckDuckGo.\nCaveat: As a Chrome extension, it obviously only works in Chrome (or any of its derivatives like Edge or Vivaldi).\nDuckDuckGo Another simple way is to just use DuckDuckGo directly. Whenever you want to go to Google, just add !g to your query.\nFinal words I used all three methods in the past. My favorite one these days is the Chrome Extension because Chrome is my current browser.\nAs a fallback I find that using DuckDuckGo directly is acceptable as well, however it quickly becomes quite annoying to constantly add !g to every query. Defaults matter.\nRelated Switching to DuckDuckGo Biased search results based on your past searches.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/02/google-and-duckduckgo/","summary":"\u003cp\u003eMy search engine of choice is \u003ca href=\"https://google.com/\"\u003eGoogle\u003c/a\u003e, nonetheless I still enjoy \u003ca href=\"https://duckduckgo.com/\"\u003eDuckDuckGo\u003c/a\u003e occasionally.\u003c/p\u003e\n\u003cp\u003eThe main reason to stick with Google is its superior quality, by the means of better search results. 20 years later, it\u0026rsquo;s still arguably the best search engine. Of course, part of the reason for that is contingent upon how much data it collects, but that\u0026rsquo;s a topic for another day\u0026hellip;\u003c/p\u003e\n\u003cp\u003eThere are at least three reasons to use DuckDuckGo as an \u003cem\u003ealternative\u003c/em\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eit\u0026rsquo;s privacy-focused, there\u0026rsquo;s no tracking and no bubbling\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e\u003c/li\u003e\n\u003cli\u003eits search results stem from sources other than Google; these days, mostly from Bing\u003c/li\u003e\n\u003cli\u003eit has \u003ca href=\"https://duckduckgo.com/bang\"\u003e\u003cstrong\u003e!bangs\u003c/strong\u003e\u003c/a\u003e, lots of them\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe quest(ion) then becomes: How can I use mostly Google, but still have quick access to DuckDuckGo bangs?\u003c/p\u003e","title":"Google and Duckduckgo"},{"content":"In this post we will cover a few linux swap recipes.\nEmpty swap space Completely empty (flush) swap space:\n% swapoff --all \u0026amp;\u0026amp; swapon --all Decrease swappiness Emptying is too extreme. Why did you get so much swap in the first place? A small tweak is to decrease the sensibility of the system to swap:\n$ cat /etc/sysctl.d/90-custom.conf vm.swappiness=20 vm.vfs_cache_pressure=50 The default swappiness of the Linux kernel these days is 60%, which IMHO is quite aggressive for desktop usage. By decreasing it to 20%, our system will only start to swap once we use more than 80% of total RAM. In other words, only when there is 20% or less of free / available RAM.\nvfs_cache_pressure:\nThis percentage value controls the tendency of the kernel to reclaim the memory which is used for caching of directory and inode objects.\nAt the default value of vfs_cache_pressure=100 the kernel will attempt to reclaim dentries and inodes at a \u0026ldquo;fair\u0026rdquo; rate with respect to pagecache and swapcache reclaim. Decreasing vfs_cache_pressure causes the kernel to prefer to retain dentry and inode caches. When vfs_cache_pressure=0, the kernel will never reclaim dentries and inodes due to memory pressure and this can easily lead to out-of-memory conditions. Increasing vfs_cache_pressure beyond 100 causes the kernel to prefer to reclaim dentries and inodes.\nHowever, /etc/sysctl.d settings will only be applied after a reboot. To apply them immediately, use the sysctl(8) command:\n% sudo sysctl -p /etc/sysctl.d/90-custom.conf vm.swappiness = 20 vm.vfs_cache_pressure = 50 Use a swapfile If you find yourself with a fully partitioned disk without any dedicated swap partition, there\u0026rsquo;s a trick to adding swap anyway: Use a swap file! Everything is a file anyway!\n# https://wiki.archlinux.org/title/Swap#Swap_file # Create the swap file: 8GiB in this case, to match our total RAM % dd if=/dev/zero of=/swapfile bs=1M count=8000 status=progress # Set restricting permissions % chmod 600 /swapfile # Format the ~~partition~~ file % mkswap /swapfile # Activate the swap file % swapon /swapfile You can check it\u0026rsquo;s working correctly by inspecting /proc/swaps:\n% cat /proc/swaps Filename\tType\tSize\tUsed\tPriority /swapfile file\t8388604\t0\t-2 Then finally add it to your /etc/fstab so that it is automatically mounted in subsequent boots:\n# swap file /swapfile none swap defaults 0 0 Add ZRAM swap Explaining zram is out of scope if this post, but check out the ArchWiki or Wikipedia.\nThe recipe I use in Arch Linux is the zramswap package:\nInstall the package. Set desired zram swap percentage, I picked 20%: % cat /etc/zramswap.conf ZRAM_SIZE_PERCENT=20 Enable/Start the service: % systemctl enable --now zramswap % systemctl status zramswap ● zramswap.service - Zram-based swap (compressed RAM block devices) Loaded: loaded (/usr/lib/systemd/system/zramswap.service; enabled; vendor preset: disabled) Active: active (exited) since Tue 2022-02-01 16:13:37 EST; 7h ago Main PID: 582 (code=exited, status=0/SUCCESS) CPU: 27ms Feb 01 16:13:37 localhost.localdomain systemd[1]: Starting Zram-based swap (compressed RAM block devices)... Feb 01 16:13:37 localhost.localdomain zramctrl[627]: Setting up swapspace version 1, size = 1.5 GiB (1654009856 bytes) Feb 01 16:13:37 localhost.localdomain zramctrl[627]: LABEL=zram0, UUID=a39e0131-f102-4503-a1e7-a3e0ca330126 Feb 01 16:13:37 localhost.localdomain systemd[1]: Finished Zram-based swap (compressed RAM block devices). You can inspect /proc/swaps again to check it\u0026rsquo;s working properly1:\n% cat /proc/swaps Filename\tType\tSize\tUsed\tPriority /swapfile file\t8388604\t0\t-2 /dev/zram0 partition\t1615244\t0\t100 zswap should have more priority than the swap file.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/02/linux-swap-shenanigans/","summary":"\u003cp\u003eIn this post we will cover a few linux swap recipes.\u003c/p\u003e","title":"Linux swap shenanigans"},{"content":"In this post we will learn how to share environment variables (e.g. $GDK_SCALE) between a system user session and X11/Xorg.\nThe typical ~/.xinitrc and/or ~/.xprofile setup in 2020s involves some environment variable exports such as the following:\n# fix java application decorations, for tiling window managers export _JAVA_AWT_WM_NONREPARENTING=1 # make Chrome pick up proxy settings stored in gconf export DESKTOP_SESSION=gnome # HiDPI settings for GTK3+ export GDK_DPI_SCALE=0.5 export GDK_SCALE=2 # HiDPI settings for QT export QT_FONT_DPI=192 This particular set of customizations stems from my dotfiles but there isn\u0026rsquo;t anything special about it. I\u0026rsquo;ll include an explanation anyway for completeness:\nThe java setting is meant for launching certain java-based applications from within a tiling window manager.\nAll the other settings are meant for 4K HiDPI displays. The baseline DPI is 96, which is too small for 4K monitors, the fonts and icons all look tiny. In order to make them scale it\u0026rsquo;s necessary to use a higher DPI. Typical setups use either 144 (x1.5) or 192 (x2.0), the bigger the DPI the bigger fonts and icons will appear in the screen.\nThose exports work well for graphical applications launched from your favorite window manager after it has already started, however if you decide to launch an application from systemd, those settings will not be picked up by it.\nFor example, if you decide to manage redshift1 (more specifically, redshift-gtk which has a system tray app) from a systemd user session2, its fonts will look small.\nThere are several ways to address this issue.\nOne of them is to edit the service file directly:\n$ systemctl --user edit redshift-gtk And then add:\n[Unit] Environment=GDK_SCALE=2 GDK_DPI_SCALE=0.5 Which results in:\n$ cat ~/.config/systemd/user/redshift-gtk.service.d/override.conf [Unit] Environment=GDK_SCALE=2 GDK_DPI_SCALE=0.5 Which you can make effective by:\n$ systemctl --user daemon-reload $ systemctl --user restart redshift-gtk I am not a fan of this approach though, because this step would need to be repeated to all service files you want to manage this way. There\u0026rsquo;s a better, DRY way to do so.\nsystemd supports environment files (environment.d(5)). User-defined ones live in ~/.config/environment.d/*.conf by default.\nThis means we could produce the following file:\n$ cat ~/.config/environment.d/user.conf # systemd environment.d(5) EnvironmentFile # https://www.freedesktop.org/software/systemd/man/environment.d.html # # Do not use export here. # # Alternatively # systemctl --user import-environment [var1] [var2] [...] # # Troubleshooting # systemctl --user show-environment # fix java application decorations, for tiling window managers _JAVA_AWT_WM_NONREPARENTING=1 # make Chrome pick up proxy settings stored in gconf DESKTOP_SESSION=gnome # HiDPI settings for GTK3+ GDK_DPI_SCALE=0.5 GDK_SCALE=2 # HiDPI settings for QT QT_FONT_DPI=192 Which is applied to all systemd user service files automatically, no need to set Environment= manually everywhere.\nHowever, now we need to maintain two different files: the systemd .conf one and the xorg ~/.xinitrc one.\nOne elegant way to reduce maintenance burden is, in my opinion, the follownig:\n$ cat ~/.xinitrc ... # Parse user session environment variables. # This file is shared with the systemd user instance. # Export all variables: https://stackoverflow.com/a/30969768/1745064 set -a [ -r ~/.config/environment.d/user.conf ] \u0026amp;\u0026amp; . ~/.config/environment.d/user.conf set +a It does what you expect: the underlying shell sources the *.conf file as if you were exporting each variable therein.\nOne caveat of this setup is that you cannot define the variables dynamically; for example, with subshells, with external programs, or with simple mathematical operations derived from other variables3.\nUltimately though you end up with only one file to manage, which is the systemd one. KISS™.\nXKCD Courtesy of Randall Munroe\nRedshift adjusts the color temperature of your screen according to your surroundings. This may help your eyes hurt less if you are working in front of the screen at night. Redshift is similar to f.lux.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nsystemctl --user start redshift.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFor example, QT_FONT_DPI=$(($GDK_SCALE * 96)) or similar.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/01/systemd-share-environment-variables-with-xorg/","summary":"\u003cp\u003eIn this post we will learn how to share environment variables (e.g.\n\u003ccode\u003e$GDK_SCALE\u003c/code\u003e) between a system user session and X11/Xorg.\u003c/p\u003e","title":"★ systemd: share environment variables with xorg"},{"content":"This post describes some tooling usages to watch for file changes and run or reload a command whenever they happen.\nContext I am contributing to miniflux, a minimalist and opinionated RSS reader. Miniflux\u0026rsquo;s stack is as minimalist as the app itself: It\u0026rsquo;s a Golang application that connects to a local PostgreSQL database. It has a well-documented and comprehensive Makefile.\nIn order to achieve an edit-and-preview workflow for quick prototyping and local iteration, all that it\u0026rsquo;s needed is to execute make run whenever any1 file in the repository is changed.\nMy goal was to achieve that workflow with the least amount of friction, and with an application that is widely available in most package managers / linux distributions out there.\nOption #1: entr (recommended) entr(1):\nRun arbitrary commands when files change\nThe following invocation does the job:\n$ fd | entr -r -- make run However, we could do better. From the upstream docs:\n» ag and ack offer many advantages over utilities such as find(1) or ls(1) in that they recognize files by their contents and are smart enough to skip directories such as .git\nI am happy with fd for this use case though. To limit entr to .go files only, we could do:\n$ fd -e go | entr -r -- make run It took me less than 5 minutes to install and figure out how to use entr.\nThis blog post covers it in more detail.\nOption #2: watchman watchman from Facebook Open Source:\nWatchman exists to watch files and record when they change. It can also trigger actions (such as rebuilding assets) when matching files change.\nWatchman\u0026rsquo;s workflow doesn\u0026rsquo;t seem to be very suited for this job though. It\u0026rsquo;s much more centered on subscribing to inotify events:\ncd \u0026lt;repository root\u0026gt; watchman watch . \u0026hellip;and then adding predefined actions to recompile parts of the application as they change. The official docs give an example with CSS minification:\n# set up a trigger named \u0026#39;buildme\u0026#39; # will run \u0026#39;minify-css\u0026#39; whenever a CSS file is changed watchman -- trigger . buildme \u0026#39;*.css\u0026#39; -- minify-css In this regard it seems to be more modular, and I could easily see a scenario where I would kick off several specialized triggers in a webdev project: for example, one for CSS minification, one for JS minification, another one for TypeScript compilation, etc.\nThat said, for the simple use case of triggering (and reloading) make run, it seems overkill. I also found its official docs too verbose and lacking sample usages for simple Makefile-based projects like miniflux.\nOne caveat of watchman is that it\u0026rsquo;s less widely available than entr. Another caveat is that recently official distributions of watchman seem to be binary only, even though watchman itself is open source.\nIt took me several minutes to figure out what\u0026rsquo;s the gist of watchman, only to realize it is more bloated than warranted.\nConclusion For simple projects, entr is the way to go, hands down. For complex webdev projects, I would look into watchman more deeply.\nTo be truly strict, only changes to .go files matter.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/01/watch-files-and-react-to-changes-during-development/","summary":"\u003cp\u003eThis post describes some tooling usages to watch for file changes and run or reload a command whenever they happen.\u003c/p\u003e","title":"★ Watch files and react to changes during development"},{"content":"A few recipes for remounting linux devices / disks. It mostly boils down to running mount -o remount as root.\nRemount as read-write If /dev/sdb1 is mounted on /mnt/data as read-only (ro), it could be remounted as rw:\n% mount -o remount,rw /mnt/data or\n% mount -o remount,rw /dev/sdb1 Increase RAM disk size /dev/shm (shared memory) is typically allocated half of the available amount of RAM in the system. For example, in my 8GB Arch Linux system:\n$ df -h | grep /dev/shm tmpfs 3.9G 127M 3.8G 4% /dev/shm To increase the amount of space allocated to it:\n% mount -o remount,size=8G /dev/shm The result:\n$ df -h | grep /dev/shm tmpfs 8.0G 72M 8.0G 1% /dev/shm ","permalink":"https://www.perrotta.dev/2022/01/linux-remount-device-with-different-options/","summary":"\u003cp\u003eA few recipes for remounting linux devices / disks. It mostly boils down to running \u003ccode\u003emount -o remount\u003c/code\u003e as root.\u003c/p\u003e","title":"Linux: Remount device with different options"},{"content":"Last year I found out about Advent of Code.\nYou said what? Advent of Code by Eric Wastl happens every year since 2015, every December from the 1st to the 25th. Each day there\u0026rsquo;s a new programming challenge1 split into two parts. The first part tends to be easier than the second one. The second part usually builds upon the first one, being a follow-up task that requires more steps and/or with a higher degree of complexity. You can\u0026rsquo;t always reuse the bits from the first part to solve the second one though.\nFor those familiar with programming contests like ACM ICPC or OBI, or online judges like SPOJ or UVa, advent of code feels like home. The main difference is that there is no time pressure and no need to write spaghetti and unreadable code; in fact, writing readable and elegant solutions is encouraged (citation needed\u0026hellip;).\nFor those familiar with FAANG/Tech whiteboard interviews, advent of code feels a lot like a typical interview. I would even go further and say it\u0026rsquo;s a great way to practice for interviews.\nIt is a great moment to either (i) learn a new exciting programming language or (ii) improve your mastery on programming languages that you already know. I know several people (see below) that used AoC2 to learn Rust or Kotlin or whatever else was exciting for them at the time. It\u0026rsquo;s surprising that the official Kotlin Docs even contain a section called Advent of Code puzzles in idiomatic Kotlin.\nSome folks go even further and use it to practice their code golfing3 or even Google Sheets skills. I have a deep amount of respect for them as it\u0026rsquo;s quite a challenge. If you think it stops there, I\u0026rsquo;ve also seen solutions in awk and sed.\nAnother positive aspect of AoC is that it has an integrated dashboard that tracks your progress as you go. It\u0026rsquo;s a simple element of gamification that immensely improves motivation and fun. You really feel a big desire to collect all those 50 stars\u0026hellip;\nWhat about me? My goal for 2021 was relatively less ambitious than that, I just wanted to improve my Python skills, more specifically Python 3. I learned Python 2 during my first year in university and used it sparingly at work and for personal endeavours, but always had a knowledge gap in Python 3.\nI set up a public git repository with my solutions and aspired to write simple and elegant python, my only constraint was to limit myself to what is available in the standard library of a vanilla python3 installation in Alpine linux, with the exception of numpy which is widespread enough to deserve an entry in my requirements.txt, and of course devtools like debuggers, linters and auto formatters as needed.\nAs an additional, non-programming challenge I also limited myself to only use the command line. This basically meant no IDEs4. My programming environment was ultimately ssh to an Alpine Linux VPS + tmux + vim. To make my life easier, one of the first tasks I accomplished was to write a generic Makefile to help me test and run my scripts. A typical invocation would look like:\n$ make DEBUG=1 DAY=3 \u0026hellip;whereas I could choose between the sample input versus the real one with DEBUG, and the puzzle day with DAY.\nWas the experience worth it? Definitely yes! Even though I only completed ~8 puzzles out of the 25 ones due to having my attention split with another project I was working on at the time, the thematic submarine puzzles were hella fun and I learned a lot of python 3 on the way.\nA few highlights of what I learned and used from my python 2to3 transition were f-strings / string interpolation (print(f'The sum is {sum}')), \u0026ldquo;everything is an iterator now\u0026rdquo; even map and range, the standard library is awesome and sometimes you stumble upon useful abstractions like Counter and defaultdict, sort is different now (key instead of comparison function), this pdb debugger thingy, among other topics I can\u0026rsquo;t remember at the moment. I realized the only concept that was previously familiar was the different syntax of the print function (you have to use parentheses now).\nIn terms of workflow, I also learned that virtual environments are now supported natively5 (python -m venv), direnv is an amazing tool to automate/manage environments in git repositories and also happens to have first-class python integration, pylint and autopep8 are good integrations with vim to help spot basic errors and/or suggest best practices, and numpy takes forever to build from source.\nWhat about the community? AoC enjoys a lot of popularity and zeitgeist, especially during times of the COVID-19 pandemic, but even before then. There\u0026rsquo;s a large /r/adventofcode subreddit community, lots of people share their solution snippets and impressions on Twitter (#AdventOfCode), there\u0026rsquo;s a ton of public git repositories on GitHub where people share their coding solutions, in pretty much any programming language you can think of, and finally there are many screencasts on YouTube. The Internet in the 2020s sparks creativity in every unimaginable corner.\nThere\u0026rsquo;s so much information that it\u0026rsquo;s impossible to stay on top of everything. Here is a small list of repositories that I followed this year, most of those are acquaintances/friends and/or stumbled upon Twitter:\nC++:\nhttps://github.com/riuri/adventofcode Python:\nhttps://github.com/sjvrijn/AdventofCode https://github.com/oomenn/AOC Rust:\nhttps://github.com/dimo414/advent-2021 https://github.com/mfs/aoc I find it\u0026rsquo;s really constructive and useful (and also fun) to peek at other people\u0026rsquo;s solutions after I coded my own. I have extensive (albeit kinda rusty these days) experience with C++ so I wanted to follow at least one repository coded with it; since I wrote my solutions in python it was also a natural choice to follow a few python repositories; and, finally, I wanted to peek at some languages I am not familiar with to get a gist of them. This year I watched Rust and a few bits of Clojure and Kotlin on Twitter.\nFinally, for some extra inspiration, there are also some 10x programmers6 out there that seem to be fans of AoC as well: Peter Norvig and Russ Cox (rsc). There are probably several others I am not aware of.\nFinal remarks I am hoping to participate in AoC this year (2022) as well, and possibly revisit the 2021 puzzles and resolve the rest of the ones I missed as time permits.\nHopefully this post encourages and motivates you to try Advent of Code as well! Happy coding.\nOr puzzle, if you will.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAcronym not to be confused with a certain annoying^W politician.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFor those unfamiliar with the concept, code golfing is all about writing a correct solution with the fewest amount of characters.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFor example: PyCharm, and also VSCode, which is getting so big these days I don\u0026rsquo;t even know if it\u0026rsquo;s possible to just call it a simple text editor anymore.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBack in the days, virtualenvwrapper was all the rage.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe 10x programmer thing is a well-known joke however in this instance the mentioned characters are indeed superb programmers that I immensely respect.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/01/advent-of-code/","summary":"\u003cp\u003eLast year I found out about \u003ca href=\"https://adventofcode.com/\"\u003eAdvent of Code\u003c/a\u003e.\u003c/p\u003e","title":"★ Advent of Code"},{"content":"ChromeOS hterm (\u0026ldquo;Secure Shell extension\u0026rdquo;) is one of my favorite chrome extensions. It is a bit dull with its out-of-the-box monospace font though. In this post we\u0026rsquo;ll learn how to customize it.\nGoogle Fonts The easiest way to customize the Secure Shell extension to use a custom font is to select one from Google Fonts. Once you select a font from there, it will give you information like this:\nUse on the web To embed a font, copy the code into the \u0026lt;head\u0026gt; of your html ( ) \u0026lt;link\u0026gt; (x) @import \u0026lt;style\u0026gt; @import url(\u0026#39;https://fonts.googleapis.com/css2?family=IBM+Plex+Sans\u0026amp;display=swap\u0026#39;); \u0026lt;/style\u0026gt; font-family: \u0026#39;IBM Plex Sans\u0026#39;, sans-serif; All we have to do is to copy the URL within the url('...') fragment above, go to the settings of the Secure Shell extension, and then paste it there:\n# Example 1: IBM Plex Sans Custom CSS (URI): https://fonts.googleapis.com/css2?family=IBM+Plex+Sans\u0026amp;display=swap # Example 2: Fira Code Custom CSS (URI): https://fonts.googleapis.com/css2?family=Fira+Code:wght@400;700\u0026amp;display=swap # Example 3: Combine both https://fonts.googleapis.com/css2?family=Fira+Code:wght@400;700\u0026amp;family=IBM+Plex+Sans\u0026amp;display=swap And then set the extension to use it:\nText font family: \u0026#34;IBM Plex Sans\u0026#34;, \u0026#34;Fira Code\u0026#34;, monospace From Secure Shell FAQ:\nBy default, we disable ligatures. Some fonts actively enable them like macOS\u0026rsquo;s Menlo (e.g. “ae” is rendered as “æ”). This messes up copying and pasting and is, arguably, not terribly legible for a terminal.\nIf your font supports ligatures, consider enabling them:\nCustom CSS (inline text): * { -webkit-font-feature-settings: \u0026#34;liga\u0026#34; on, \u0026#34;calt\u0026#34; on; -webkit-font-smoothing: antialiased; text-rendering: optimizeLegibility; } Not all fonts are available on Google Fonts though. For example, Hermit is one of my current monospace favorites: it\u0026rsquo;s not there1.\nGithub Many fonts are available on GitHub (or in other forges), checked into a git repository.\nIf you happen to find a .woff2 web font file laying therein, you could also use it in hterm:\nCustom CSS (inline text): @font-face { font-family: \u0026#34;Anonymous Pro\u0026#34;; src: url(https://cdn.rawgit.com/wernight/powerline-web-fonts/8040cf32c146c7cd4f776c1484d23dc40685c1bc/fonts/AnonymousPro.woff2); } And then set the extension to use it:\nText font family: \u0026#34;Anonymous Pro\u0026#34;, monospace Note: I couldn\u0026rsquo;t get this method to work with .ttf or .otf.\nhttps://github.com/pcaro90/hermit/issues/2\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/01/chromeos-hterm-customize-fonts/","summary":"\u003cp\u003e\u003ca href=\"https://chrome.google.com/webstore/detail/secure-shell/iodihamcpbpeioajjeobimgagajmlibd?hl=en\"\u003eChromeOS hterm\u003c/a\u003e (\u0026ldquo;Secure Shell extension\u0026rdquo;) is one of my favorite chrome extensions.\nIt is a bit dull with its out-of-the-box monospace font though.\nIn this post we\u0026rsquo;ll learn how to customize it.\u003c/p\u003e","title":"ChromeOS hterm: customize fonts"},{"content":"A brief list of disposable / throwaway account resources:\nEmail inbox https://dispostable.com/ https://10minutemail.com/ https://yadim.dismail.de/ NAT / public URLs https://ngrok.com/ Pastebin https://paste.debian.net/ https://paste.dismail.de/ https://paste.opensuse.org/ https://upaste.de/ (produces short URLs, deleted after one hour) SMS https://smsreceivefree.com/ Appendix The New Oil has some good tips on disinformation for throwaway and/or ephemeral accounts. ","permalink":"https://www.perrotta.dev/2022/01/throwaway-accounts-for-ephemeral-use-cases/","summary":"\u003cp\u003eA brief list of disposable / throwaway account resources:\u003c/p\u003e","title":"Throwaway accounts for ephemeral use cases"},{"content":"This post covers how to add DNS entries / mappings to a local network managed with pihole.\nThere are several ways to do so:\n1. The CLI way: /etc/pihole/ Edit /etc/pihole/custom.list, set one mapping per line, just as you would for /etc/hosts:\n$ cat /etc/pihole/custom.list 127.0.0.1 localhost.corp.google.com 192.168.1.75 myhostname.home.arpa This works because /etc/dnsmasq.d/01-pihole.conf contains addn-hosts=/etc/pihole/custom.list by default.\nFrom Gentoo Wiki:\nIt is possible to refer to an (additional) hosts file to use as source for DNS queries. To do so, add the -H /path/to/hostsfile (\u0026ndash;addn-hosts=/path/to/hostsfile) command line option. It is also possible to pass a directory; in that case, all files inside that directory will be treated as additional hosts files.\n2. The CLI way: /etc/dnsmasq.d/ $ cat /etc/dnsmasq.d/03-pihole-custom-dns.conf address=/localhost.corp.google.com/127.0.0.1 address=/myhostname.home.arpa/192.168.1.75 From ArchWiki:\nIn some cases, such as when operating a captive portal, it can be useful to resolve specific domains names to a hard-coded set of addresses. This is done with the address config.\n3. The Web way Navigate to http://pi.hole/admin/dns_records.php and set your DNS records there. From pihole docs:\nThe order of locally defined DNS records is:\nThe device\u0026rsquo;s host name (/etc/hostname) and pi.hole Configured in a config file in /etc/dnsmasq.d/ Read from /etc/hosts Read from the \u0026ldquo;Local (custom) DNS\u0026rdquo; list (stored in /etc/pihole/custom.list) (the aforementioned ways) Only the first record will trigger an address-to-name association.\nWrapping up Then restart pihole to apply changes:\n$ pihole restartdns ","permalink":"https://www.perrotta.dev/2022/01/pihole-add-custom-dns-mappings/","summary":"\u003cp\u003eThis post covers how to add DNS entries / mappings to a local network managed\nwith \u003ca href=\"https://pi-hole.net/\"\u003epihole\u003c/a\u003e.\u003c/p\u003e","title":"Pihole: Add custom DNS mappings"},{"content":"This document describes my workflow to manage APKBUILDs for the aports repository in Alpine Linux.\nDisclaimer First of all, this post is not a substitute to the AlpineWiki and it will likely get outdated at some point. In particular, refer to the following articles for up-to-date documentation that will outlive this blog:\nhttps://wiki.alpinelinux.org/wiki/APKBUILD_Reference https://wiki.alpinelinux.org/wiki/Abuild_and_Helpers https://wiki.alpinelinux.org/wiki/Aports_tree https://wiki.alpinelinux.org/wiki/Creating_an_Alpine_package This article is not a tutorial, as such it assumes you already know what an APKBUILD is and how to use abuild. In particular, you should have the alpine-sdk, atools and spdx-licenses-list packages installed in your system.\nStructure I manage my packages with git. Create a GitLab account on https://gitlab.alpinelinux.org/, fork the aports tree, and git clone your fork.\nThe structure follows Alpine Linux repositories:\n$ git clone https://gitlab.alpinelinux.org/alpine/aports.git \u0026amp;\u0026amp; tree -L 1 aports aports ├── CODINGSTYLE.md ├── COMMITSTYLE.md ├── README.md ├── community ├── main ├── non-free ├── scripts ├── testing └── unmaintained Bootstrapping I am going to illustrate with a package I added recently, sensible-utils:\nBefore you even begin, check if the package already exists, do a quick search in the Alpine Repositories1.\nStart by scaffolding a new APKBUILD from the base template:\n$ cd aports/testing # Always add new packages in testing/ first. $ newapkbuild sensible-utils $ cd sensible-utils $ $EDITOR APKBUILD Note: If you have a language-specific package (e.g. perl, python, rust), use the language-specific template instead of the base one. Run newapkbuild -h to list available templates. There are also some apkbuild-* helpers such as apkbuild-pypi and apkbuild-cpan.\nFill in APKBUILD metadata like pkgname=, url=, etc. Refer to the AlpineWiki for up-to-date best practices.\nBy doing so, I produced the following APKBUILD:\npkgname=sensible-utils pkgver=0.0.14 pkgrel=0 pkgdesc=\u0026#34;Utilities for sensible alternative selection\u0026#34; url=\u0026#34;https://packages.debian.org/source/sensible-utils\u0026#34; arch=\u0026#34;all\u0026#34; license=\u0026#34;GPL-2.0-or-later\u0026#34; makedepends=\u0026#34;po4a\u0026#34; subpackages=\u0026#34;$pkgname-doc\u0026#34; source=\u0026#34;http://ftp.debian.org/debian/pool/main/s/$pkgname/${pkgname}_$pkgver.tar.xz\u0026#34; builddir=\u0026#34;$srcdir/$pkgname.git\u0026#34; build() { ./configure --prefix=/usr make } check() { make -k check } package() { make DESTDIR=\u0026#34;$pkgdir/\u0026#34; install # only works with update-alternatives, specific to debian rm \u0026#34;$pkgdir/usr/bin/select-editor\u0026#34; } sha512sums=\u0026#34; 15ba996f811ab3a9c1f5726f35766d74aafdf925c5c2392b33c6643d6c439796a742f9d0f4625c79de640e6b5e4a6a032b768eb1bc4ac31b448f9767b0ceed44 sensible-utils_0.0.14.tar.xz \u0026#34; Note: $srcdir refers to the src/ directory within sensible-utils. $pkgdir refers to the pkg/ directory within sensible-utils.\nIf you\u0026rsquo;re used to Arch Linux PKGBUILDs you\u0026rsquo;ll notice a striking similarity to APKBUILDs. I highlighted a few notable differences in a previous post, My First APKBUILD.\nAdjustments Generate the checksums with abuild checksum. It will automatically update the APKBUILD inplace.\nDownload and extract package files with abuild unpack.\nls src/ and check the directory structure. Update $builddir in your APKBUILD to match it. Usually it will be $srcdir/$pkgname-$pkgver, but sometimes tiny adjustments are necessary. In this case, it was $srcdir/$pkgname.git.\nThen run abuild -r. If everything goes well, your package (and subpackages, if any) will be successfully built2 in an isolated environment and placed in ~/packages (sensible-utils-0.0.14-r0.apk and sensible-utils-doc-0.0.14-r0.apk), however that doesn\u0026rsquo;t mean it is a decent package yet.\nRun apkbuild-lint APKBUILD and abuild sanitycheck to lint your package and catch common errors. Fix the errors, if any.\nRequest feedback if needed If the package is only relevant to you, stop here. git commit, git push, and then you\u0026rsquo;re done. Install the package with doas apk add \u0026lt;pkg\u0026gt;.\nOtherwise, if the package might be potentially useful to other Alpine users, you could consider uploading it to the aports repository.\nBefore you do so, stop for a moment and make an honest judgment whether this is a high quality package and whether you\u0026rsquo;re confident it is clean and polished enough, following the best practices documented in the Wiki. The answer doesn\u0026rsquo;t need to be positive, it\u0026rsquo;s perfectly OK to commit mistakes and everyone is a newbie at some point.\nIf the answer is negative, or if you\u0026rsquo;re new to this process and would like some help, fear no more! There are at least two decent community resources wherein to ask for help:\n#alpine-devel on OFTC IRC Drew DeVault wrote a good post about IRC etiquette.\nalpine-devel mailing list.\nIf you\u0026rsquo;re part of any other community (e.g. Reddit, Discord) feel free to ask therein as well. Avoid posting everywhere though, pick one community, draft your post and then patiently wait.\nPublish your package If all is well, it\u0026rsquo;s time to publish your APKBUILD. Follow the up-to-date steps at https://wiki.alpinelinux.org/wiki/Creating_an_Alpine_package#Code_review. There are basically two options:\nSend a gitlab merge request (MR). This follows the standard git forge workflow (GitHub / BitBucket / GitLab) wherein you fork the main repository, create a branch in your own clone, push it and then initiate a pull request3.\nAlternatively, send an email with your patch to the aports mailing list with git send-email:\n$ git config sendemail.to \u0026#34;alpine-aports@lists.alpinelinux.org\u0026#34; $ git send-email -1 # Implicitly uses --to=alpine-linux@lists.alpinelinux.org as set above Tip: The second approach has a steep learning curve, however once you figure it out it\u0026rsquo;s actually faster, simpler and more streamlined. Whenever a new email is sent to the aports mailing list, a MR is automatically created on GitLab.\nNote: If you adopt the email workflow and need to send a follow-up to your initial patch, do not use --in-reply-to. Instead, create a new email thread. This is needed because as of this post new GitLab MRs are only created when new email threads are created. Replies to existing email threads do not update the MR patch.\nAnd that\u0026rsquo;s all! Other useful tips:\nUse repology to look for preexisting packages in other Linux (or even BSD) distributions, it\u0026rsquo;s very handy as a starting point if you have no idea how to package a given package. In particular, Arch Linux PKGBUILDs are very similar to APKBUILDs. Gentoo EBUILDs and FreeBSD Makefiles are also reasonable approximations. Use abump to bump pkgver in APKBUILD files if the package gets an update to a newer upstream release. Use apkgrel to bump or reset the pkgrel value of your APKBUILD. Use urlwatch to track upstream updates. If you use https://duckduckgo.com/, query for !alpine sensible-utils.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPackage debugging is out of scope of this post.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIn GitLab it\u0026rsquo;s called Merge Request (MR). The list of all aports MRs is here.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/01/alpine-linux-new-apkbuild-workflow/","summary":"\u003cp\u003eThis document describes my workflow to manage \u003ccode\u003eAPKBUILDs\u003c/code\u003e for the\n\u003ca href=\"https://gitlab.alpinelinux.org/alpine/aports\"\u003eaports\u003c/a\u003e repository in \u003ca href=\"https://alpinelinux.org/\"\u003eAlpine Linux\u003c/a\u003e.\u003c/p\u003e","title":"★ Alpine Linux: New APKBUILD Workflow"},{"content":"Recently I needed to figure out what the IP address of my pihole instance was in my Raspberry Pi in my local network.\nFinding the Raspberry Pi nmap nmap to the rescue!\n# nmap -sS 192.168.1.1-255 | tee network.txt | less The relevant snippets to the pihole look like this:\nNmap scan report for pi.hole (192.168.1.XX) Host is up (0.0052s latency). Not shown: 997 closed tcp ports (reset) PORT STATE SERVICE 22/tcp open ssh 53/tcp open domain 80/tcp open http MAC Address: AA:AA:AA:AA:AA:AA (Raspberry Pi Foundation) Nmap scan report for pi.hole (192.168.1.YY) Host is up (0.0059s latency). Not shown: 997 closed tcp ports (reset) PORT STATE SERVICE 22/tcp open ssh 53/tcp open domain 80/tcp open http MAC Address: BB:BB:BB:BB:BB:BB (Raspberry Pi Foundation) There are two IP addresses, one for the ethernet interface (eth0) and the other for the wifi (wlan0). Later on I would disable the wifi interface.\nThe 3 open ports are for services you would expect in a pihole:\nssh (port 22) for remote access / debugging / troubleshooting DNS server (port 53) for the dnsmasq server that pihole uses underneath for adblocking HTTP server (port 80) for the http://pi.hole/admin web management UI ip Another way is to use the ip command. In particular, ip neigh lists the neighbours, one of which should be the pihole.\nTesting the pihole One effective way to test the pihole is to see if analytics.google.com is blocked. There are several ways to do so:\nping should return a local address like 127.0.0.1 or 0.0.0.0. $ ping analytics.google.com PING analytics.google.com (127.0.0.1) 56(84) bytes of data. 64 bytes from localhost (127.0.0.1): icmp_seq=1 ttl=64 time=0.023 ms 64 bytes from localhost (127.0.0.1): icmp_seq=2 ttl=64 time=0.031 ms ^C --- analytics.google.com ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1002ms rtt min/avg/max/mdev = 0.023/0.027/0.031/0.004 ms Ditto for a DNS lookup utility such as dig: $ dig +short analytics.google.com 0.0.0.0 Other ways: drill, host, nslookup, systemd-resolve.\nhttps://d3ward.github.io/toolz/adblock.html seems to be a reasonable website to test whether your adblock blocklists are properly working. Alternatively, just visit any modern and large news corporation website, it will probably be full of ads.\nTroubleshooting the pihole If DNS resolution fails from the pihole itself, run pihole restartdns. Then ping google.com. The ping should work, if it doesn\u0026rsquo;t then there\u0026rsquo;s a bigger problem, out of scope of this post. If the ping works now but stops working later on once you eventually reboot the Pi, consider triggering this command at startup via cron or a systemd timer.\nIf DNS resolution works from the pihole but fails from a neighbouring device, double-check if the device is properly configured: its DNS should be set to the IP address of the pihole. Check these:\n/etc/resolv.conf If the system uses systemd-resolved, run resolvectl. Another possibility is that the pihole might be configured to only answer queries from eth0. Use the http://pi.hole/admin interface to ensure the pihole is configured to answer DNS queries from the local network.\nSetting a static IP in the pihole There are several ways to do so, in order of recommendation:\nStatic DHCP lease from your router. If running a modem, this will likely not work. Prefer running a DHCP server from the pihole.\ndhcpcd: This is typically done as part of the standard pihole setup.\n$ cat /etc/dhcpcd.conf ... # fallback to static profile on eth0 #interface eth0 #fallback static_eth0 interface eth0 static ip_address=192.168.1.XX/24 static routers=192.168.1.1 static domain_name_servers= Note: Restart dhcpcd to apply: systemctl restart dhcpcd.\n/etc/network/interfaces if running Raspberry Pi OS (debian): $ sudoedit /etc/network/interfaces.d/pihole auto lo iface lo inet loopback auto eth0 iface eth0 inet static address 192.168.1.XX netmask 255.255.255.0 gateway 192.168.1.1 Note: Reconfigure debian networking to apply: systemctl restart networking.\nStatic DHCP lease from the pihole itself if it\u0026rsquo;s running a DHCP server. This solution is a bit redundant and should only be applied as last resort. ","permalink":"https://www.perrotta.dev/2022/01/introspect-the-local-network-for-pihole/","summary":"\u003cp\u003eRecently I needed to figure out what the IP address of my \u003ca href=\"https://pi-hole.net/\"\u003epihole\u003c/a\u003e\ninstance was in my \u003ca href=\"https://www.raspberrypi.org/\"\u003eRaspberry Pi\u003c/a\u003e in my local network.\u003c/p\u003e","title":"Introspect the local network for Pihole"},{"content":"Here\u0026rsquo;s a situation that happens often during development:\nSuppose you committed something to git. A few commits later, you realized you forgot to add something to that commit, or possibly missed a link, or even spotted a typo. How do you go about fixing it?\nTeam If you\u0026rsquo;re working on a repository with a team, you should just git commit and git push. Write an eloquent commit message to refer to the previous commit in which you forgot to include your changes.\nSelf Now, if you\u0026rsquo;re working on a standalone repository, just for yourself1, this creates an opportunity to rewrite your history in a cleaner way. The workflow is as follows:\nMake the changes or fixes you had originally forgot to. git add them. Identify the commit id in which you originally wanted to make those changes. git log or tig are simple CLI-oriented ways to do so. Hereafter assume this id is abcdef. Commit your changes while referencing the original commit and then rewrite history: $ git commit --fixup=abcdef # Then pick one of: $ git rebase -i --root $ git rebase -i abcdef~1 # And then save the file as is. Double-check everything went as expected with git log and/or git show and/or tig. If you\u0026rsquo;re happy with the current state of your repository, commit the sin: git push --force. XKCD Courtesy of Randall Munroe\nReferences tig, in case you don\u0026rsquo;t know: tig is an ncurses-based text-mode interface for git. It functions mainly as a Git repository browser, but can also assist in staging changes for commit at chunk level and act as a pager for output from various Git commands.\ngit rebase --root: c.f. Stack Overflow. This is just a lazy way to make the rebase include abcdef. You could do something like git rebase -i HEAD~10 where 10 is an arbitrary guess, but this will only work if abcdef is within the most 10 recent commits. Alternatively git rebase -i abcdef~1 also works. For example: your dotfiles, or your personal blog.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/01/git-oops-i-forgot-to-add-this-thingy/","summary":"\u003cp\u003eHere\u0026rsquo;s a situation that happens often during development:\u003c/p\u003e\n\u003cp\u003eSuppose you committed something to \u003ccode\u003egit\u003c/code\u003e. A few commits later, you realized you\nforgot to add something to that commit, or possibly missed a link, or even\nspotted a typo. How do you go about fixing it?\u003c/p\u003e","title":"Git: Oops I forgot to add this thingy"},{"content":"This document describes my workflow to manage PKGBUILDs for the AUR (Arch User Repository) in Arch Linux.\nDisclaimer First of all, this post is not a substitute to the excellent ArchWiki and it will likely get outdated at some point. In particular, refer to the following articles for up-to-date documentation that will outlive this blog:\nhttps://wiki.archlinux.org/title/Arch_User_Repository https://wiki.archlinux.org/title/Arch_package_guidelines https://wiki.archlinux.org/title/Creating_packages https://wiki.archlinux.org/title/PKGBUILD This article is not a tutorial, as such it assumes you already know what a PKGBUILD is and how to use makepkg. In particular, you should have the base-devel and devtools packages installed in your system.\nStructure I manage my packages with git plus Eli Schwartz\u0026rsquo;s excellent aurpublish. The tree structure is simple, with one PKGBUILD per directory:\n$ git clone https://github.com/thiagowfx/PKGBUILDs \u0026amp;\u0026amp; tree PKGBUILDs PKGBUILDs ├── bkt │ └── PKGBUILD ├── fpp-git │ └── PKGBUILD ├── git-crecord │ └── PKGBUILD ├── i3a │ └── PKGBUILD ├── LICENSE ├── Makefile ├── README.md ├── ttf-camingocode │ └── PKGBUILD └── urlwatch.yml aurpublish is used solely to automate certain interactions with the AUR, more about it later.\nBootstrapping I am going to illustrate with a package I added recently, bkt:\nBefore you even begin, check if the package already exists, do a quick search in the AUR and also in the official repos1.\nStart by copying over the standard PKGBUILD template:\n$ cd PKGBUILDs $ mkdir bkt \u0026amp;\u0026amp; cd bkt $ cp /usr/share/pacman/PKGBUILD.proto PKGBUILD $ $EDITOR PKGBUILD Fill in PKGBUILD metadata like pkgname=, url=, etc. Refer to the ArchWiki for up-to-date best practices.\nThe most important step is to refer to https://wiki.archlinux.org/title/Category:Arch_package_guidelines to figure out the package type.\nbkt is a Rust package. This is my first time packaging for Rust, not a problem though, as I can just refer to https://wiki.archlinux.org/title/Rust_package_guidelines.\nThe rust package guidelines page contains the blueprint for prepare(), check(), build() and package(). Packaging is mostly a matter of gluing everything together. Read the project README.md and the wiki, and then combine the needed steps in the PKGBUILD.\nBy doing so, I produced the following PKGBUILD:\npkgname=bkt pkgver=0.5.0 pkgrel=1 pkgdesc=\u0026#34;A subprocess caching utility\u0026#34; arch=(\u0026#39;x86_64\u0026#39;) url=\u0026#34;https://www.bkt.rs/\u0026#34; license=(\u0026#39;MIT\u0026#39;) makedepends=(\u0026#39;cargo\u0026#39;) source=(\u0026#34;$pkgname-$pkgver.tar.gz::https://github.com/dimo414/$pkgname/archive/refs/tags/$pkgver.tar.gz\u0026#34;) sha256sums=() prepare() { cd \u0026#34;$srcdir/$pkgname-$pkgver\u0026#34; cargo fetch --locked --target \u0026#34;$CARCH-unknown-linux-gnu\u0026#34; } build() { cd \u0026#34;$srcdir/$pkgname-$pkgver\u0026#34; export RUSTUP_TOOLCHAIN=stable export CARGO_TARGET_DIR=target cargo build --frozen --release --all-features } check() { cd \u0026#34;$srcdir/$pkgname-$pkgver\u0026#34; export RUSTUP_TOOLCHAIN=stable cargo test --frozen --all-features } package() { cd \u0026#34;$srcdir/$pkgname-$pkgver\u0026#34; install -Dm0755 -t \u0026#34;$pkgdir/usr/bin/\u0026#34; \u0026#34;target/release/$pkgname\u0026#34; install -Dm644 LICENSE -t \u0026#34;$pkgdir/usr/share/licenses/$pkgname\u0026#34; } Note: $srcdir refers to the src/ directory within bkt. $pkgdir refers to the pkg/ directory within bkt.\nAdjustments Generate the checksums with updpkgsums. It will automatically update the PKGBUILD inplace.\nDownload and extract package files with makepkg -o.\nls src/ and check the directory structure. Update cd in your PKGBUILD to match it. Usually it will be cd $srcdir/$pkgname-$pkgver, but sometimes tiny adjustments are necessary.\nThen run makepkg -s. If everything goes well, your package will be successfully built2 (bkt-0.5.0-1-x86_64.pkg.tar.zst), however that doesn\u0026rsquo;t mean it is a decent package yet.\nRun namcap PKGBUILD and namcap *.pkg.tar.zst to lint your package and catch common errors. Fix the errors, if any.\nTo ensure all dependencies have been correctly declared and none of them are missing, run makepkg within a clean chroot. I like to use Graysky\u0026rsquo;s excellent ccm (Clean Chroot Manager) to do so. Run ccm s (=\u0026ldquo;run makepkg in the clean chroot\u0026rdquo;). If it produces any errors, it likely means you missed some dependencies. Adjust depends=, checkdepends= and makedepends= accordingly.\nRequest feedback if needed If the package is only relevant to you, stop here. git commit, git push, and then you\u0026rsquo;re done. Install the package with makepkg -i.\nOtherwise, if the package might be potentially useful to other Arch users, you could consider uploading it to the AUR.\nBefore you do so, stop for a moment and make an honest judgment whether this is a high quality package and whether you\u0026rsquo;re confident it is clean and polished enough, following the best practices documented in the Wiki. The answer doesn\u0026rsquo;t need to be positive, it\u0026rsquo;s perfectly OK to commit mistakes and everyone is a newbie at some point.\nIf the answer is negative, or if you\u0026rsquo;re new to this process and would like some help, fear no more! There are at least two decent community resources wherein to ask for help:\nAUR Issues, Discussion \u0026amp; PKGBUILD Requests BBS / Forums: Open a new thread, post your PKGBUILD (use [code][/code] tags if you paste it directly!) or a link to it3. Request folks to critique your work, mention that you\u0026rsquo;re looking for feedback. This kind of thread is generally well received in the official forums if you demonstrate you did diligent research before asking for help.\nAUR General Mailing List: Send an email to the mailing list asking for help. In general, follow proper mailing list etiquette, good resources for that are https://useplaintext.email/ and https://man.sr.ht/lists.sr.ht/etiquette.md. TL;DR: Use plain-text instead of HTML in your email.\nIf you\u0026rsquo;re part of any other community (e.g. Reddit, Discord) feel free to ask therein as well. Avoid posting everywhere though, pick one community, draft your post and then patiently wait.\nPublish your package If all is well, it\u0026rsquo;s time to publish your PKGBUILD to the AUR. Follow the up-to-date steps at https://wiki.archlinux.org/title/Arch_User_Repository#Submitting_packages.\nTL;DR: If you don\u0026rsquo;t use aurpublish, do:\n$ makepkg --printsrcinfo \u0026gt; .SRCINFO Then you\u0026rsquo;ll need both the PKGBUILD and the .SRCINFO file, it\u0026rsquo;s basically a matter of committing your changes and pushing them to the right repository.\nIf you do use aurpublish this process is much easier, it\u0026rsquo;s mostly a matter of doing git commit, git push and aurpublish bkt. Aurpublish automatically generates the .SRCINFO and a commit message by the means of git hooks.\nAnd that\u0026rsquo;s all! Other useful tips:\nUse repology to look for preexisting packages in other Linux (or even BSD) distributions, it\u0026rsquo;s very handy as a starting point if you have no idea how to package a given package. In particular, Alpine Linux APKBUILDs are very similar to PKGBUILDs. Gentoo EBUILDs and FreeBSD Makefiles are also reasonable approximations. Use makepkg -src to clean up after building a package. Bonus: Track upstream Use a software like urlwatch or nvchecker to track future upstream changes so that you can update your packages in a timely fashion4. There\u0026rsquo;s also a web service called Release Monitoring, part of Fedora Infra. I use urlwatch the following way:\n$ cat PKGBUILDs/urlwatch.yml # urls for urlwatch(1) --- name: \u0026#34;bkt\u0026#34; command: \u0026#34;git ls-remote --tags https://github.com/dimo414/bkt\u0026#34; --- name: \u0026#34;fpp\u0026#34; command: \u0026#34;git ls-remote --tags https://github.com/facebook/PathPicker\u0026#34; --- name: \u0026#34;git-crecord\u0026#34; command: \u0026#34;git ls-remote --tags https://github.com/andrewshadura/git-crecord\u0026#34; --- name: \u0026#34;i3a\u0026#34; command: \u0026#34;git ls-remote --tags https://git.goral.net.pl/mgoral/i3a\u0026#34; # --- # name: \u0026#34;ttf-camingocode\u0026#34; # N/A # Run this command periodically via cron or systemd timer. # Set up notifications e.g. via sendmail. $ urlwatch --urls urlwatch.yml If you use https://duckduckgo.com/, query for !aur bkt and !archpkg bkt. Handy!\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPackage debugging is out of scope of this post.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFor example, use https://gist.github.com or http://paste.opensuse.org/ or http://ix.io/.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIn 99% of the cases this is just a matter of bumping the pkgver= and updating the checksums. If pkgver= is the same but there\u0026rsquo;s a fix to the package itself, then bump pkgrel= instead.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/01/arch-linux-new-pkgbuild-workflow/","summary":"\u003cp\u003eThis document describes my workflow to manage \u003ccode\u003ePKGBUILDs\u003c/code\u003e for the \u003ca href=\"https://aur.archlinux.org/\"\u003eAUR\u003c/a\u003e (Arch User Repository) in \u003ca href=\"https://www.archlinux.org/\"\u003eArch Linux\u003c/a\u003e.\u003c/p\u003e","title":"★ Arch Linux: New PKGBUILD Workflow"},{"content":"As packages are upgraded over time, updated configs files under /etc may arise. Different package managers treat this issue differently.\nAlpine Linux apk creates .apk-new files, which can be located and merged by running doas update-conf. Here is a recent example:\n% doas update-conf --- //etc/securetty +++ //etc/securetty.apk-new @@ -12,3 +12,6 @@ tty11 hvc0 ttyS0 +ttyS1 +ttyAMA0 +ttyAMA1 New //etc/securetty available: Quit, Next, Show diff, Edit new, Zap new, Use new (q/n/s/e/z/u) [s] u Arch Linux pacman creates .pacnew files, which can be located and merged by running sudo pacdiff. Here is a recent example:\n% sudo pacdiff ==\u0026gt; pacnew file found for /etc/sudoers :: (V)iew, (S)kip, (R)emove pacnew, (O)verwrite with pacnew, (Q)uit: [v/s/r/o/q] Tip: The pacdiff-pacman-hook-git package helpfully adds a pacman hook that automatically checks whether there are any due .pacnew files upon upgrading the system (pacman -Syu), being a simple and effective way to automate this maintenance task. It looks like this:\n: Running post-transaction hooks... (1/5) Reloading system manager configuration... (2/5) Creating temporary files... (3/5) Arming ConditionNeedsUpdate... (4/5) Registering Haskell modules... (5/5) Reviewing .pacnew files... /etc/sudoers.pacnew ⟶ /etc/sudoers ──────────────────────────────────────────────────────────────────────────────── ────┐ 76: │ ────┘ ## ## User privilege specification ## -root ALL=(ALL:ALL) ALL +root ALL=(ALL) ALL ## Uncomment to allow members of group wheel to execute any command -# %wheel ALL=(ALL:ALL) ALL +# %wheel ALL=(ALL) ALL ## Same thing without a password -# %wheel ALL=(ALL:ALL) NOPASSWD: ALL +# %wheel ALL=(ALL) NOPASSWD: ALL ## Uncomment to allow members of group sudo to execute any command -# %sudo ALL=(ALL:ALL) ALL +# %sudo ALL=(ALL) ALL ## Uncomment to allow any user to run sudo if they know the password ## of the user they are running the command as (root by default). # Defaults targetpw # Ask for the password of the target user -# ALL ALL=(ALL:ALL) ALL # WARNING: only use this together with \u0026#39;Defaults targetpw\u0026#39; +# ALL ALL=(ALL) ALL # WARNING: only use this together with \u0026#39;Defaults targetpw\u0026#39; ## Read drop-in files from /etc/sudoers.d @includedir /etc/sudoers.d :: Searching databases for updates... :: Searching AUR for updates... there is nothing to do ","permalink":"https://www.perrotta.dev/2022/01/alpine-/-arch-linux-.apk-new-and-.pacnew-files/","summary":"\u003cp\u003eAs packages are upgraded over time, updated configs files under \u003ccode\u003e/etc\u003c/code\u003e may\narise. Different package managers treat this issue differently.\u003c/p\u003e","title":"Alpine / Arch Linux: .apk-new and .pacnew files"},{"content":"Here\u0026rsquo;s how we can enable automatic (unattended) package upgrades in Debian.\nHowto Install the unattended-upgrades package with apt(8):\n% apt install unattended-upgrades The service is then enabled and started automatically:\n$ systemctl status unattended-upgrades ● unattended-upgrades.service - Unattended Upgrades Shutdown Loaded: loaded (/lib/systemd/system/unattended-upgrades.service; enabled; vendor preset: enabled) Active: active (running) since Sun 2022-01-16 02:05:42 EST; 35s ago Docs: man:unattended-upgrade(8) Main PID: 22442 (unattended-upgr) Tasks: 2 (limit: 1597) CPU: 516ms CGroup: /system.slice/unattended-upgrades.serviceGk └─22442 /usr/bin/python3 /usr/share/unattended-upgrades/unattended-upgrade-shutdown --wait-for-signal By default, only security updates are enabled. We can enable all updates by uncommenting the applicable lines:\n$ sudoedit /etc/apt/apt.conf.d/50unattended-upgrades ... Unattended-Upgrade::Origins-Pattern { // Codename based matching: // This will follow the migration of a release through different // archives (e.g. from testing to stable and later oldstable). // Software will be the latest available for the named release, // but the Debian release itself will not be automatically upgraded. // \u0026#34;origin=Debian,codename=${distro_codename}-updates\u0026#34;; // \u0026#34;origin=Debian,codename=${distro_codename}-proposed-updates\u0026#34;; // \u0026#34;origin=Debian,codename=${distro_codename},label=Debian\u0026#34;; \u0026#34;origin=Debian,codename=${distro_codename},label=Debian-Security\u0026#34;; \u0026#34;origin=Debian,codename=${distro_codename}-security,label=Debian-Security\u0026#34;; ... For debugging, one should run:\n$ sudo unattended-upgrade -d We could go beyond and add logging by the means of etckeeper, just like how we did for Alpine Linux\u0026rsquo;s apk\n% apt install etckeeper Reading package lists... Done Building dependency tree... Done Reading state information... Done The following NEW packages will be installed: etckeeper 0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded. Need to get 54.4 kB of archives. After this operation, 180 kB of additional disk space will be used. Get:1 http://raspbian.freemirror.org/raspbian bullseye/main armhf etckeeper all 1.18.16-1 [54.4 kB] Fetched 54.4 kB in 1s (84.3 kB/s) Preconfiguring packages ... Selecting previously unselected package etckeeper. (Reading database ... 44403 files and directories currently installed.) Preparing to unpack .../etckeeper_1.18.16-1_all.deb ... Unpacking etckeeper (1.18.16-1) ... Setting up etckeeper (1.18.16-1) ... Created symlink /etc/systemd/system/multi-user.target.wants/etckeeper.timer → /lib/systemd/system/etckeeper.timer. etckeeper.service is a disabled or a static unit, not starting it. ... etckeeper is enabled and works out-of-the-box as well:\nsystemctl status etckeeper.timer ● etckeeper.timer - Daily autocommit of changes in /etc directory Loaded: loaded (/lib/systemd/system/etckeeper.timer; enabled; vendor preset: enabled) Active: active (waiting) since Sun 2022-01-16 02:28:44 EST; 2min 36s ago Trigger: Mon 2022-01-17 02:28:44 EST; 23h left Triggers: ● etckeeper.service Docs: man:etckeeper(8) Here\u0026rsquo;s what a typical log looks like:\n$ (cd /etc/etckeeper \u0026amp;\u0026amp; sudo git log) commit 8f9f5e31d9abb833cf645825c1cbda15336818b7 (HEAD -\u0026gt; master) Author: root \u0026lt;root@raspberry\u0026gt; Date: Sun Jan 16 06:25:28 2022 -0500 daily autocommit commit 5a6478711a1a1198535d5062ca309afb5c99c0eb Author: root \u0026lt;root@raspberry\u0026gt; Date: Sun Jan 16 02:29:01 2022 -0500 Initial commit References https://wiki.debian.org/UnattendedUpgrades ","permalink":"https://www.perrotta.dev/2022/01/debian-enable-unattended-upgrades/","summary":"\u003cp\u003eHere\u0026rsquo;s how we can enable automatic (unattended) package upgrades in Debian.\u003c/p\u003e","title":"Debian: Enable unattended upgrades"},{"content":"Use case: Given an Alpine Linux diskless1 installation meant for a Raspberry Pi setup, we would like to add a persistent storage component to it to make it survive across reboots.\nGoal The Alpine Linux Wiki covers most of the installation process, hence I will only document the bits that were lacking and/or confusing therein.\nMy use case is the following:\nGiven a Raspberry Pi 3B with an old 4GiB SD Card as CF storage2, install Alpine Linux in diskless mode. Find a way to preserve modifications in /etc and /var, as well as any installed packages through its apk package manager.\nLet\u0026rsquo;s follow the steps outlined in the wiki.\nCopy Alpine to the SD Card Grab the SD card and install Alpine Linux in it.\nAlpine provides officially supported images designed for the Raspberry Pi.\nMost Linux distributions provide an .iso or .img file to be installed with a tool like Balena Etcher, Rufus, Raspberry Pi Imager or plain dd3.\nAlpine is not like most Linux distributions: Instead, it provides a .tar.gz archive with files that should be copied directly to the SD card. Grab the latest version (3.15 at the time of this post) from https://alpinelinux.org/downloads/. There are 3 options:\narmhf: Works with all Pis, but may perform less optimally on recent versions.\narmv7: Works with the Pi 3B, 32-bit.\naarch64: Works with the Pi 3B, 64-bit.\nI opted for aarch64 to make it 64-bit, but armv7 would also have worked well for my setup. In fact, Raspberry Pi OS (Debian) uses armv7 (32-bit) at the time of this writing.\nBefore copying files over, format the SD Card. As I was doing this from a Windows machine because it was the only one I had readily available with a SD card slot, I just used the native Windows Disk Management tool to do so. I decided to allocate a 100MB4 FAT32 partition. The rest of the SD card would be blank for now. Alpine is surprisingly small, 100MB was more than enough for the kernel and other needed files.\nOnce the SD card is formatted, copy the files over to it. It turns out Windows cannot extract tarballs (.tar.gz); a tool like 7-zip should do the job. Copy the files over to the root of the newly allocated FAT32 partition, and then safely eject the SD card.\nBoot Alpine from the SD Card The next step is to insert the SD Card into the Pi and then boot. I had some trouble in this step and eventually figured out I didn\u0026rsquo;t mark the primary FAT32 partition as bootable. Unfortunately it\u0026rsquo;s not straightforward to mark the partition as bootable from Windows. On a Linux machine there\u0026rsquo;s a wide array of tools to do so: fdisk, cfdisk (TUI), sfdisk (scriptable fdisk), parted, gparted (GUI) are some of them. I worked around that by installing Raspberry Pi OS on the SD card with the Raspberry Pi imager, and then overwriting it with the Alpine files. This works because the Raspberry PI OS installation marks the FAT32 partition as bootable.\nInstall Alpine Installing Alpine is well documented in the wiki thus it won\u0026rsquo;t be covered here. It basically comes down to invoking setup-alpine, which then invokes other setup-* scripts.\nKeep in mind we\u0026rsquo;re not really \u0026ldquo;installing\u0026rdquo; Alpine as this is a diskless installation. A more accurate term here would be \u0026ldquo;configuring\u0026rdquo;.\nBefore invoking the installation script, I created a second primary partition in the SD card, set to ext4:\n# Configure networking to get working internet access. % setup-interfaces # Install some partitioning tools. % apk add cfdisk e2fsprogs # Create a second partition (mmcblk0p2) and write it. % cfdisk /dev/mmcblk0 # Format the partition as ext4. % mkfs.ext4 /dev/mmcblk0p2 # Mount the partition under /media. % mount /dev/mmcblk0p2 /media/mmcblk0p2 The installation is straightforward, we just need to pay attention to a few select steps:\nsetup-disk: Select none to ensure a diskless installation5. setup-apkcache: Select /media/mmcblk0p2/cache to persist downloaded apk packages. setup-lbu: Edit /etc/lbu/lbu.conf and set LBU_MEDIA=\u0026quot;mmcblk0p2\u0026quot;. Note: Do not add /media as it is implicit. Once the installation is complete, run lbu commit to persist the changes in the second partition. Once you do so, a \u0026lt;hostname\u0026gt;.apkovl.tar.gz6 file should materialize on /media/mmcblk0p2/.\nThis is a good moment to reboot. Before we do so, let\u0026rsquo;s cache the packages we had previously downloaded.\n# Cache packages. % apk cache download % reboot After the first reboot If everything worked as expected, once you reboot all your previously installed packages should have been preserved and automatically restored / reinstalled, as well as your modifications done to /etc.\nFrom this point on, whenever you install a new package that you want to be preserved for subsequent reboots, run lbu commit afterwards. For example:\n% apk add vim % lbu commit If you would like to see what is going to be committed, run lbu status or lbu diff before doing the actual commit. Whenever you commit, /media/mmcblk0p2/\u0026lt;hostname\u0026gt;.apkovl.tar.gz gets overwritten with your most recent modifications.\nIt\u0026rsquo;s possible to keep more than one backup file by changing BACKUP_LIMIT= in /etc/lbu/lbu.conf. This is specially handy if you decide to revert to an earlier system snapshot / state later on. The stock config looks like this:\n% cat /etc/lbu/lbu.conf # what cipher to use with -e option DEFAULT_CIPHER=aes-256-cbc # Uncomment the row below to encrypt config by default # ENCRYPTION=$DEFAULT_CIPHER # Uncomment below to avoid \u0026lt;media\u0026gt; option to \u0026#39;lbu commit\u0026#39; # Can also be set to \u0026#39;floppy\u0026#39; # LBU_MEDIA=usb # Set the LBU_BACKUPDIR variable in case you prefer to save the apkovls # in a normal directory instead of mounting an external media. # LBU_BACKUPDIR=/root/config-backups # Uncomment below to let lbu make up to 3 backups # BACKUP_LIMIT=3 Tip: You can find the list of all explicitly installed packages in /etc/apk/world.\nThe last piece: make /var persistent There are three natural ways that come to mind to make /var persistent:\nA) Separate partition (or file) Instead of two partitions (FAT32 and ext4), create 3 partitions: FAT32, ext4 and ext4. Use the latter one to mount /var on, saving this information in /etc/fstab. The main disadvantage of this setup is that you\u0026rsquo;ll need to allocate a fixed amount of space of each of the ext4 partitions and it may be difficult to figure out how to split the space between them.\nA variant of this approach is to just create the third partition as a file:\n# 500MB file % dd if=/dev/zero of=/media/mmcblk0p2/var.img bs=1M count=500 status=progress % mkfs.ext4 /media/mmcblk0p2/var.img % mount /media/mmcblk0p2/var.img /var This works because the Linux kernel supports mounting files as if they were device blocks, treating them as loop devices (pseudo-devices).\nI don\u0026rsquo;t like these approaches because they shadow the preexisting /var from the boot media, which in turn messes up with existing services that use it such as cron: % crontab -l would fail. One workaround would be to mount a /var subdirectory instead: for example, /var/lib/docker for docker.\nB) Bind mount This one is straightforward:\n% mount --bind /media/mmcblk0p2/var/lib/docker /var/lib/docker The actual partition lives in the SD card, however we make a bind mount under /var, which is like an alias. From Stack Exchange:\nA bind mount is an alternate view of a directory tree. Classically, mounting creates a view of a storage device as a directory tree. A bind mount instead takes an existing directory tree and replicates it under a different point. The directories and files in the bind mount are the same as the original. Any modification on one side is immediately reflected on the other side, since the two views show the same data.\nC) Overlay mount From ArchWiki:\nOverlayfs allows one, usually read-write, directory tree to be overlaid onto another, read-only directory tree. All modifications go to the upper, writable layer. This type of mechanism is most often used for live CDs but there is a wide variety of other uses.\nIt\u0026rsquo;s perfect for our use case, which uses a live bootable SD card for Alpine. It blends the preexisting, ephemeral, in-memory /var with the persistent in-disk /var.\nI wanted to mount /var directly but found it to be problematic for the same reasons mentioned earlier, therefore I just went with /var/lib/docker instead:\n# Create overlay upper and work directories. % mkdir -p /media/mmcblk0p2/var/lib/docker /media/mmcblk0p2/var/lib/docker-work # Add mountpoint entry to fstab. Note: The work dir must be an empty directory in the same filesystem mount as the upper directory. % echo \u0026#34;overlay /var/lib/docker overlay lowerdir=/var/lib/docker,upperdir=/media/mmcblk0p2/var/lib/docker,workdir=/media/mmcblk0p2/var/lib/docker-work 0 0\u0026#34; \u0026gt;\u0026gt; /etc/fstab # Mount all fstab entries, including our newly added one. % mount -a Conclusion I opted for the third approach, using an overlay mount, it was the most seamless one. A bind mount would have been fine as well.\nThe final setup works surprisingly well:\nAlpine Linux is very lightweight and runs mostly from RAM apk cache is persistent to the ext4 partition /var/ is persistent to the ext4 partition lbu commit persists changes in /etc/ and /home/ in the ext4 partition Every reboot fully resets the system sans persistent components above References https://vincentserpoul.github.io/post/alpine-linux-rpi0/ http://dahl-jacobsen.dk/tips/blog/2021-04-08-docker-on-alpine-linux/ http://dahl-jacobsen.dk/tips/blog/2018-03-15-alpine-on-raspberry-pi/ Running (almost) fully from RAM.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCF = Compact disk.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOn Linux I\u0026rsquo;d usually opt for dd, on Windows the Raspberry Pi Imager is a sensible choice.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n100MB is overly conservative, but keep in mind I had a very small SD Card, with only 4GiB storage. 250MB or even 500MB should be a more sensible default if you have a bigger SD Card (e.g. 32GiB).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAn alternative is to select data disk mode, but it didn\u0026rsquo;t work for me.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\novl is short for overlay. Not to be confused with vol for volume.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/01/alpine-linux-on-raspberry-pi-diskless-mode-with-persistent-storage/","summary":"\u003cp\u003eUse case: Given an Alpine Linux \u003cstrong\u003ediskless\u003c/strong\u003e\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e installation meant for\na Raspberry Pi setup, we would like to add a persistent storage component to it\nto make it survive across reboots.\u003c/p\u003e","title":"★ Alpine Linux on Raspberry Pi: Diskless Mode with persistent storage"},{"content":"It is possible to track/follow commits of git repositories on GitHub via RSS: https://github.com/\u0026lt;user\u0026gt;/\u0026lt;repo\u0026gt;/commits.atom.\nFor example, my dotfiles: https://github.com/thiagowfx/dotfiles/commits.atom\nI am using this to keep track of Miniflux commits from within Miniflux itself (a RSS webapp).\n","permalink":"https://www.perrotta.dev/2022/01/rss-follow-commit-updates-from-github/","summary":"\u003cp\u003eIt is possible to track/follow commits of \u003ccode\u003egit\u003c/code\u003e repositories\non \u003ca href=\"https://github.com/\"\u003eGitHub\u003c/a\u003e via RSS: \u003ccode\u003ehttps://github.com/\u0026lt;user\u0026gt;/\u0026lt;repo\u0026gt;/commits.atom\u003c/code\u003e.\u003c/p\u003e","title":"RSS: Follow commit updates from Github"},{"content":"This is a follow-up post of Keychron K2: Linux Setup. The previous post focused on the configuration of the keyboard, this one focuses on its user experience from the point of view of a Software Engineer.\nPreviously\u0026hellip; My first (and previous) mechanical keyboard was a Logitech G610 Orion Red1. It\u0026rsquo;s a full wired keyboard with Cherry MX Red switches (linear) and dedicated multimedia keys. In my opinion it is a solid choice for beginners because its interface is quite familiar thanks to the wide range of available keys. It is branded as a gaming keyboard but frankly it was a fine office keyboard as well. However after using it for a while I wanted to upgrade.\nThe natural upgrade path would contain one or more of the following features, in order of importance:\ngood support for both Linux and macOS, especially Linux compact: tenkeyless a.k.a. 80%, or 75%2 portable: wireless, either with a dongle or with bluetooth or both with decent battery life: should outlast at least a week of office work not overpriced: ~$200 CAD budget playful: with RGB backlight (instead of white LED) Given those, a natural upgrade path would have been the Logitech G915 TKL. Its main caveat is that it is relatively overpriced, ~$300 CAD. Above that $300 CAD point one should arguably be looking for ergonomic and/or fully programmable (QMK firmware) keyboards, like the Kinesis Advantage and/or the Ergodox EZ3. Even though the G915 TKL is rock solid, it didn\u0026rsquo;t have any fancy features to justify that investment.\nThe quest for the perfect keyboard\u0026hellip; I then proceeded to outsource my luck to the wisdom of the crowds, by asking for recommendations in a mailing list at $DAYJOB, giving them a subset of the requirements above.\nThe choice was then obvious: I\u0026rsquo;d get a Keychron. It fit all of my requirements perfectly. The issue was that Keychron had so many choices to pick from.\nXKCD Courtesy of Randall Munroe\nAfter some deliberation I had two options in mind: Keychron K2 and Keychron K1 TKL. The main difference between them is that the K1 is a low-profile keyboard. I didn\u0026rsquo;t know what low-profile meant at the time and had to do some research to figure it out4.\nIn the end I opted for the Keychron K2, red switches (linear), with RGB backlight.\nThe keyboard The keyboard met all my expectations, even surpassing them, I am quite satisfied overall:\nLinux support Great out-of-the-box support, it just works. Even though I tweaked a few configs, it wasn\u0026rsquo;t strictly necessary. In particular, there\u0026rsquo;s a physical toggle where you can choose between macOS (=Linux) or Windows mode. Furthermore they provide both macOS-style (command, option, etc) and Windows-style keycaps (super, alt, etc). For Linux I tend to stick with the Windows ones. Compact A 75% keyboard is compact by definition, what else could I add? I wouldn\u0026rsquo;t go lower than that though, in my opinion removing the function keys goes too far and makes the keyboard harder to use. A Tenkeyless / 80% option would also be compact enough while maybe increasing comfort a little bit, but I managed to adapt quickly to the 75% layout. Multimedia and OS keys are easily available by the means of Fn + F1, etc. Portable It has bluetooth, but can also be used while plugged in. There\u0026rsquo;s a toggle that controls which mode (wireless or wired) to use. The bluetooth has 3 channels and it\u0026rsquo;s very easy to switch between them: Fn + 1, Fn + 2, Fn + 3. This makes it easy to switch between laptops and/or workstation, work and/or personal. The cable connector is USB-C which in my opinion is a must these days (2020s). Battery life Battery lasts more than enough, to the point that I don\u0026rsquo;t even need to care about it. I tend to recharge it every 2 weeks or so. Fn + b will let me have a visual indication of how much juice is still left. The keyboard automatically sleeps after 10 minutes of inactivity in order to save battery, which I think is a nice bonus, I don\u0026rsquo;t need to worry about turning it off. This can be disabled if it ends up being annoying, though. Great value for money $90 USD at the time of this writing. Because I didn\u0026rsquo;t want to deal with international shipping, I ended up simply buying it from one of their official local retailers in Canada, OneOfZero. This slightly increased what I paid for it (~$150 CAD with taxes), on the other hand the shipping was really fast. Just beware, this particular retailer does not have a friendly return policy, if I recall correctly they charge a 25% fee and end up throwing the keyboard away (landfill), which is very depressing. Playful The RGB lighting is fluff and completely irrelevant in terms of productivity, however it adds a playful touch to the keyboard. I would say that white lighting is enough, but sometimes it\u0026rsquo;s just cool to change to different color(s). What can I say, we humans are visual creatures. You can easily adjust the light brightness and toggle it on/off (Fn + light), plus there are several patterns to choose from. I tend to use a still pattern because it isn\u0026rsquo;t distracting for programming or other type of work that requires focus. Finally: The keyboard keycaps are quite sturdy and stick well in place. I had some issues with my previous keyboard where some of its keycaps would easily fall off it when moving it within my backpack. I do not have this issue with the Keychron.\nFuture Mechanical keyboards are meant to last. I do not intend to upgrade it any time soon. However, if/when I ever do it, I will be looking for the following features:\nQMK firmware / programmable: would unlock more workflow possibilities. The Keychron Q1 would be a good candidate for this. With a dongle, in addition to bluetooth. Because sometimes bluetooth is just annoying and/or unreliable. The Logitech G915 TKL has a dongle. Other switches? So far I\u0026rsquo;ve only used red ones (linear). More silent switches could be useful. Ergonomic: Whether it\u0026rsquo;s a split, an ortholinear or just a curved keyboard, I figure that at some point it will be a good investment for my wrists. Adaptation is difficult but it may be necessary one day. I am not particularly attracted to custom keycaps, they are cute but not my cup of tea. And I also do not see the appeal of hot swappable keycaps. I can understand why some folks appreciate those features, customizability is powerful, but for me it\u0026rsquo;s less stressful to keep things simple.\nLinked to the .pdf because apparently the SKU isn\u0026rsquo;t listed in the Logitech product website anymore. At the time, it cost ~$120 CAD.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis requirement indirectly excluded all those gaming keyboards with dedicated macro and/or multimedia keys, if they ended up increasing the overall keyboard surface area.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIn the past, I had the opportunity to borrow these from coworkers for a week but couldn\u0026rsquo;t quite adapt to them, their learning curve is quite steep. Maybe I\u0026rsquo;ll try that again in the future.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIt basically means the keys are thinner than usual, comparable to laptop keyboard keys.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/01/keychron-k2-review/","summary":"\u003cp\u003eThis is a follow-up post of \u003ca href=\"https://www.perrotta.dev/2021/12/keychron-k2-linux-setup/\"\u003eKeychron K2: Linux Setup\u003c/a\u003e. The previous post focused on the configuration of the keyboard, this one focuses on its user experience from the point of view of a Software Engineer.\u003c/p\u003e","title":"★ Keychron K2: Review"},{"content":"apk(8) is the Alpine Linux package manager. Surprisingly, it lacks native logs. In this post we will learn how to work around this limitation.\nIn a distribution like Arch Linux that uses pacman(8), one would typically find logs in /var/log/pacman.log. You would expect Alpine Linux to follow suit and provide some /var/log/apk.log or similar, however that\u0026rsquo;s not the case.\nLogs are nowhere to be found, even in the apk-* man pages. I double-checked by asking on the #alpine-linux IRC and someone confirmed this is indeed the case, and there\u0026rsquo;s an (unconfirmed) possibility the next generation of apk may add logging support.\nMeanwhile, we will use etckeeper(8) to overcome this limitation.\netckeeper: set-up etckeeper is\na collection of tools to let /etc be stored in a git, mercurial, bazaar or darcs repository. This lets you use git to review or revert changes that were made to /etc. Or even push the repository elsewhere for backups or cherry-picking configuration changes.\nby Joey Hess.\nIt is available in the Alpine Linux repositories, just install it:\n% apk add etckeeper No configuration is needed, it works out-of-the-box, thanks to a post-install hook to initialize the git repository, and an apk commit hook to update it upon apk package operations.\netckeeper: viewing logs Just run git log as root. Root privilege is necessary because the git repository is initialized under /etc/etckeeper. Pick your poison:\n$ GIT_DIR=/etc/etckeeper doas git log $ doas git -C /etc/etckeeper log $ (cd /etc/etckeeper \u0026amp;\u0026amp; doas git log) Here\u0026rsquo;s what a typical log looks like, courtesy of apk-autoupdate(1)1:\ncommit f06255c4be4657481082406b2050ecd88e3da768 Author: root \u0026lt;root@localhost.localdomain\u0026gt; Date: Tue Jan 11 00:00:24 2022 -0500 committing changes in /etc after apk run Package changes: -libeconf-0.4.2-r0 -libeconf-doc-0.4.2-r0 +libeconf-0.4.4-r0 +libeconf-doc-0.4.4-r0 -mtools-4.0.36-r0 -mtools-doc-4.0.36-r0 +mtools-4.0.37-r0 +mtools-doc-4.0.37-r0 -perl-io-socket-ssl-2.073-r0 -perl-io-socket-ssl-doc-2.073-r0 +perl-io-socket-ssl-2.074-r0 +perl-io-socket-ssl-doc-2.074-r0 -py3-idna-3.3-r1 +py3-idna-3.3-r2 -py3-jinja2-3.0.1-r1 -py3-jinja2-doc-3.0.1-r1 +py3-jinja2-3.0.3-r0 +py3-jinja2-doc-3.0.3-r0 Which merely does apk upgrade, automatically. More details here.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/01/alpine-linux-apk-logs-with-etckeeper/","summary":"\u003cp\u003e\u003ccode\u003eapk(8)\u003c/code\u003e is the Alpine Linux package manager. Surprisingly, it lacks native\nlogs. In this post we will learn how to work around this limitation.\u003c/p\u003e","title":"Alpine Linux: apk logs with etckeeper"},{"content":"In the same spirit of my first PKGBUILD and Ebuild, herein I will describe my first APKBUILD.\nAt a glance Alpine Linux package management is very similar to Arch Linux, with tiny differences:\nPKGBUILD → APKBUILD: The filename is obviously different. Their format are very similar though, both of them are bash scripts with variables and functions. In particular, there\u0026rsquo;s check, patch, build and package. cp /usr/share/pacman/PKGBUILD.proto → newapkbuild: Template versus scaffolding. pacman → apk: The package manager is different. makepkg -s → abuild -r: makepkg drives all things package building for pacman. abuild drives package building for apk. makepkg -i → apk add \u0026lt;pkg\u0026gt;: makepkg can also drive package installations whereas abuild cannot, apk must be used. namcap → apkbuild-lint (from atools) + abuild sanitycheck1: Linters are different. updpkgsums → abuild checksum: Generate hashes for package sources. XKCD Courtesy of Randall Munroe\nOther than that, the process of writing an APKBUILD is very similar to writing a PKGBUILD. In fact, the Arch repositories (especially the AUR) tend to be much more comprehensive than Alpine\u0026rsquo;s in terms of number of packages, so chances are if you want to write a new package for Alpine, check in Arch\u0026rsquo;s repos first, it\u0026rsquo;s a good starting point.\nMy first package: fpp fpp stands for \u0026lsquo;Facebook Path Picker\u0026rsquo;.\nAs of the time of this post, I maintain fpp-git in the AUR. It looks like this:\npkgname=fpp-git pkgver=0.9.2.r130.ge0d5cfc pkgrel=1 pkgdesc=\u0026#39;TUI that lets you pick paths out of its stdin and run arbitrary commands on them\u0026#39; url=\u0026#39;https://facebook.github.io/PathPicker\u0026#39; license=(\u0026#39;MIT\u0026#39;) source=(\u0026#34;${pkgname%-git}::git+https://github.com/facebook/PathPicker.git\u0026#34;) sha256sums=(\u0026#39;SKIP\u0026#39;) arch=(\u0026#39;any\u0026#39;) makedepends=(\u0026#39;git\u0026#39;) depends=(\u0026#39;python\u0026#39;) conflicts=(\u0026#34;${pkgname%-git}\u0026#34;) provides=(\u0026#34;${pkgname%-git}\u0026#34;) prepare() { cd \u0026#34;$srcdir/${pkgname%-git}\u0026#34; rm -r \u0026#34;src/tests\u0026#34; } pkgver() { cd \u0026#34;$srcdir/${pkgname%-git}\u0026#34; git describe --long --tags | sed \u0026#39;s/\\([^-]*-g\\)/r\\1/;s/-/./g\u0026#39; } package() { cd \u0026#34;$srcdir/${pkgname%-git}\u0026#34; # library install -Dm755 \u0026#34;fpp\u0026#34; -t \u0026#34;$pkgdir/usr/share/fpp\u0026#34; cp -a src \u0026#34;$pkgdir/usr/share/fpp\u0026#34; # entrypoint install -dm755 \u0026#34;$pkgdir/usr/bin\u0026#34; ln -s \u0026#34;/usr/share/fpp/fpp\u0026#34; \u0026#34;$pkgdir/usr/bin\u0026#34; # documentation install -Dm644 LICENSE -t \u0026#34;$pkgdir/usr/share/licenses/$pkgname\u0026#34; install -Dm644 \u0026#34;debian/usr/share/man/man1/fpp.1\u0026#34; -t \u0026#34;$pkgdir/usr/share/man/man1\u0026#34; } PKGBUILD guidelines and instructions:\nhttps://wiki.archlinux.org/title/PKGBUILD https://wiki.archlinux.org/title/Arch_package_guidelines The equivalent APKBUILD I wrote looks like this:\npkgname=fpp pkgver=0.9.2 pkgrel=0 pkgdesc=\u0026#34;TUI that lets you pick paths out of its stdin and run arbitrary commands on them\u0026#34; url=\u0026#34;https://facebook.github.io/PathPicker\u0026#34; arch=\u0026#34;noarch\u0026#34; license=\u0026#34;MIT\u0026#34; depends=\u0026#34;bash python3\u0026#34; subpackages=\u0026#34;$pkgname-doc\u0026#34; source=\u0026#34;$pkgname-$pkgver.tar.gz::https://github.com/facebook/PathPicker/archive/$pkgver.tar.gz\u0026#34; builddir=\u0026#34;$srcdir/PathPicker-$pkgver\u0026#34; check() { fpp --version } prepare() { default_prepare rm -r \u0026#34;src/__tests__\u0026#34; } package() { # library install -Dm755 \u0026#34;fpp\u0026#34; -t \u0026#34;$pkgdir/usr/share/fpp\u0026#34; cp -a src \u0026#34;$pkgdir/usr/share/fpp\u0026#34; # entrypoint install -dm755 \u0026#34;$pkgdir/usr/bin\u0026#34; ln -s \u0026#34;/usr/share/fpp/fpp\u0026#34; \u0026#34;$pkgdir/usr/bin\u0026#34; # documentation install -Dm644 LICENSE -t \u0026#34;$pkgdir/usr/share/licenses/$pkgname\u0026#34; install -Dm644 \u0026#34;debian/usr/share/man/man1/fpp.1\u0026#34; -t \u0026#34;$pkgdir/usr/share/man/man1\u0026#34; } sha512sums=\u0026#34; 65b6b077f437bd642ebf94c55be901aabc73f7b9c89e4522c4f51970c4d63d744ad8fa29cac06816851f63bcb81d0480e61d405231c582e9aca0f4e650949a97 fpp-0.9.2.tar.gz \u0026#34; APKBUILD guidelines and instructions:\nhttps://wiki.alpinelinux.org/wiki/Creating_an_Alpine_package https://wiki.alpinelinux.org/wiki/APKBUILD_Reference Build Comparison Let\u0026rsquo;s highlight a few similarities and differences in them, excluding the fact that one is fetched from git and the other one fetches a point release directly2:\npackage metadata, by the means of bash variables, are almost equivalent one-to-one A notable difference is the architecture, Arch primarily supports x86_64 whereas Alpine has support for multiple architectures. When a package is architecture agnostic, Arch denotes it with any whereas alpine has both noarch and all, the latter is like any (=all architectures), the former means it\u0026rsquo;s agnostic (=e.g. a pure bash script or python package). APKBUILDs use flat strings, whereas PKGBUILDs use bash arrays Alpine encourages splitting larger packages into subpackages, as such APKBUILD has first-class support and syntactic sugar for that. -dev and -doc subpackages are very common. On the other hand, Arch tends to have monolithic packages in order to keep it simple, although it also supports subpackages. Alpine supports setting $builddir whereas Arch doesn\u0026rsquo;t. As a consequence, it\u0026rsquo;s often unneeded to cd in build() and package() in Alpine, whereas in Arch one does need to manually change directories to $srcdir/$pkgname before building. Alpine lacks optional dependencies, whereas Arch has optdepends. Alpine enforces the use of check in test packages, otherwise it needs to be explicitly disabled and documented with !check in options=. That\u0026rsquo;s not the case in Arch. check(), build() and package() are pretty much similar in both formats. $srcdir and $pkgdir are provided in both. The ArchWiki is way more documented in terms of packaging guidelines and examples than Alpine\u0026rsquo;s. If you use DuckDuckGo, you can query for !aw \u0026lt;foo\u0026gt; as a bang shortcut to search directly in the ArchWiki. Last but not least, in Arch one can install package tarballs3 with makepkg -i or pacman -U. In Alpine that approach doesn\u0026rsquo;t seem to be directly supported. The workflow is to add a local repository diretory in /etc/apk/repositories (notice the last two lines):\n$ cat /etc/apk/repositories # http://dl-cdn.alpinelinux.org/alpine/v3.15/main # http://dl-cdn.alpinelinux.org/alpine/v3.15/community # http://dl-cdn.alpinelinux.org/alpine/latest-stable/main # http://dl-cdn.alpinelinux.org/alpine/latest-stable/community http://dl-cdn.alpinelinux.org/alpine/edge/main http://dl-cdn.alpinelinux.org/alpine/edge/community http://dl-cdn.alpinelinux.org/alpine/edge/testing /home/$USER/packages/community /home/$USER/packages/testing abuild will place the resulting package tarball in ~/packages, in this case:\n$ ls ~/packages/testing/x86_64/fpp* /home/$USER/packages/testing/x86_64/fpp-0.9.2-r0.apk /home/$USER/packages/testing/x86_64/fpp-doc-0.9.2-r0.apk \u0026hellip;and then apk add fpp will automagically recognize it\u0026rsquo;s in there and install it. The advantage of this approach is that it keeps a local package repository around and it\u0026rsquo;s well integrated with apk, way differently from pacman that has no integration with the AUR at all. One could also possibly set up a local repository in Arch, for example, with ccm, but it takes extra steps and it\u0026rsquo;s not officially supported.\nUpstream Contributions On Arch, to contribute a PKGBUILD upstream one just needs to create an account in the AUR. Armed with a git + ssh infrastructure, all you need to do is git push. There are no ACLs involved, anyone can do that4.\nOn Alpine there\u0026rsquo;s a bit more of politics involved5: Anyone can send a patch(1), either via mailing list or via a Gitlab MR (merge request). Patch works well with git send-email -1, being automatically cross-posted to a Gitlab MR. On the other hand the MR workflow is easier to be followed up on feedback from developers and other contributors (git push --force), and it\u0026rsquo;s also cross-posted, to the mailing list. An Alpine developer with the appropriate permissions must approve your patch/MR before it becomes available to other Alpine users.\nSadly at the time of this writing my patch hasn\u0026rsquo;t yet been approved (2 weeks later), however we\u0026rsquo;re in holiday season. This wouldn\u0026rsquo;t have been a problem in the AUR, where I could have just pushed it immediately, without any review. On the other hand the Alpine approach at least gives me some hope that the submitted packages have slightly higher quality than the average ones in the AUR, since they need to be manually reviewed/approved/vetted by at least one Alpine developer.\nInstall spdx-licenses-list to lint the licenses, it\u0026rsquo;s used by abuild sanitycheck as an optional dependency.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe AUR tends to have both non-vcs and vcs versions of a software, whereas Alpine is focused a bit more on stability and tends to have non-vcs only. This is not a hard rule though, exceptions may exist.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n.tar.xz or, more recently, .tar.zstd.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAnd this is one of the reasons why you should always inspect every PKGBUILD you install from the Arch User Repository, as it could have been tampered with and/or contain malicious code.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI\u0026rsquo;ll leave it open-ended whether that\u0026rsquo;s a bug or a feature. Depending on the lens you see through, it could be considered either gatekeeping (bureaucracy, control) or sanity (quality, stability). It has pros and cons, and even those are arguable.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/01/my-first-apkbuild/","summary":"\u003cp\u003eIn the same spirit of my first PKGBUILD and \u003ca href=\"https://www.perrotta.dev/2014/09/my-first-ebuild/\"\u003e\u003ccode\u003eEbuild\u003c/code\u003e\u003c/a\u003e, herein I will describe my first \u003ccode\u003eAPKBUILD\u003c/code\u003e.\u003c/p\u003e","title":"★ My First APKBUILD"},{"content":"Alternative title:\nVentoy: A keychain for all your live operating systems\nFrom the project website, ventoy is an:\nopen source tool to create bootable USB drive for ISO/WIM/IMG/VHD(x)/EFI files.\nWith ventoy, you don\u0026rsquo;t need to format the disk over and over, you just need to copy the ISO/WIM/IMG/VHD(x)/EFI files to the USB drive and boot them directly.\nBefore The typical linux desktop user workflow to try out new distros1 is to:\nDownload an .iso or .img file. Grab a USB stick2 and format/erase it. Run an application3 to write the image to the block device. Boot your workstation, press some key to change your BIOS/UEFI setup to boot via USB. Profit. This works well if you only need to do it once or twice, but carries a few caveats:\nThe USB flash drive contents are completely erased. Extra measures need to be taken in order to use it the rest of its storage persistently. You\u0026rsquo;ll have to redo this whole process whenever there\u0026rsquo;s a new release of your OS. Even for rolling release OSes like Arch Linux, Gentoo or Alpine Linux, you\u0026rsquo;ll typically want to have a more up-to-date image anyway, otherwise the installation process will eventually become problematic4, as the system gets more and more out-of-date. You can only keep one bootable image at a time in your USB stick. Unless you have multiple USB sticks, suddenly you will find that your 32 GB (or even 128 GB) storage is useless to hold multiple operating systems. Afterwards Compare that with Ventoy\u0026rsquo;s workflow:\nDownload Ventoy, and install it to your USB flash drive. This needs to be done only once5. Download an .iso or .img file. Copy it to the USB storage, using your file manager. You could use cp from the command line, you could also drag and drop using your file manager, whether it\u0026rsquo;s from Windows, Linux or macOS, doesn\u0026rsquo;t really matter. The destination is just an ordinary directory, nothing fancy. Boot your workstation, press some key to change your BIOS/UEFI setup to boot via USB. Profit. The number of steps is the same, but Ventoy really shines in the following aspects:\nThere\u0026rsquo;s no need to use any special software to write your images to the USB stick. You just copy and paste a file. Really! And you can do that from any OS. You can add as many images as you want. For example, you could keep your favorite desktop OS therein, alongside your favorite ARM OS (for your raspberry pi), alongside your favorite system rescue utility. They will all co-exist, and once you boot with the USB you\u0026rsquo;ll be able to choose which image you want to boot to, with a nice bootloader menu, à la GRUB or systemd-boot. Upgrading an OS is just a matter of deleting the old one and copying the new one over, exactly like you would do with a simple document file. Your USB flash drive is formatted in such a way that it\u0026rsquo;s possible to use its unused storage as persistent storage. So for example you could boot into your desktop OS, save your work, then reboot into your system rescue utility6 and pick up the leftover files therein. Ventoy is magic. It allows you to carry a single USB stick with all of your favorite operating systems - and they don\u0026rsquo;t even have to be Linux, most operating systems are supported:\nMost type of OS supported (Windows/WinPE/Linux/ChromeOS/Unix/VMware/Xen\u0026hellip;)\nThe full list of supported systems is here.\nIf you need some inspiration to fill in your Ventoy USB stick, head over to distrowatch.\nObviously excluding virtualized solutions like containers (like docker or lxc) and virtual machines. I am also excluding chroots like systemd-boot.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNo one burns CDs anymore, right? Right?\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIn 2020s, Balena Etcher is a pretty popular application to do so. Linux users often just resort to the command-line instead: % dd if=mydisk.img of=/dev/sdb status=progress; sync.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTry installing Arch from a 2010 .iso in 2020 and let me know how it goes.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nYou may want to occasionally re-image Ventoy, say, in 5, 10 years? It will probably keep working even if you don\u0026rsquo;t, though, at least with the operating systems supported at the time you installed it.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMy favorite one is GRML, which is debian-based.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/01/ventoy-automate-your-distro-hopping/","summary":"\u003cp\u003eAlternative title:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eVentoy: A keychain for all your live operating systems\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eFrom the project website, \u003ca href=\"https://www.ventoy.net/en/index.html\"\u003e\u003cstrong\u003eventoy\u003c/strong\u003e\u003c/a\u003e is an:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eopen source tool to create bootable USB drive for ISO/WIM/IMG/VHD(x)/EFI files.\u003c/p\u003e\n\u003cp\u003eWith ventoy, you don\u0026rsquo;t need to format the disk over and over, you just need to copy the ISO/WIM/IMG/VHD(x)/EFI files to the USB drive and boot them directly.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Ventoy: Automate your Distro hopping"},{"content":"In this post we will learn how to define a command-not-found hook to the apk(8) package manager in Alpine Linux.\nSneak peek Before:\n$ podman zsh: correct \u0026#39;podman\u0026#39; to \u0026#39;pod2man\u0026#39; [nyae]? n zsh: command not found: podman After:\n$ podman zsh: correct \u0026#39;podman\u0026#39; to \u0026#39;pod2man\u0026#39; [nyae]? n podman may be found in the following packages: \u0026lt;cmd:podman\u0026gt; podman-3.4.4-r1 x86_64 {podman} (Apache-2.0) Preamble Whenever you type a command that is not in your $PATH, usually your shell will yell at you that it wasn\u0026rsquo;t found.\nThe typical workflow in this scenario is to use the search functionality of your package manager in order to find which package provides the binary you\u0026rsquo;re interested in.\nIn Alpine Linux, one would do:\n$ apk search podman podman-doc-3.4.4-r1 podman-remote-3.4.4-r1 podman-docker-3.4.4-r1 openscap-1.3.5-r3 podman-3.4.4-r1 podman-compose-0.1.5-r4 podman-bash-completion-3.4.4-r1 podman-zsh-completion-3.4.4-r1 py3-podman-3.2.1-r1 podman-docker-doc-3.4.4-r1 podman-openrc-3.4.4-r1 podman-fish-completion-3.4.4-r1 The output is a bit noisy, but with a bit of experience you could easily figure out the package you\u0026rsquo;re looking for is simply called podman, given the output above.\nSurely this was an easy example, what if we tried something less obvious?\n$ vidir zsh: correct \u0026#39;vidir\u0026#39; to \u0026#39;vdir\u0026#39; [nyae]? n zsh: command not found: vidir There\u0026rsquo;s no vidir binary, then surely there\u0026rsquo;s a vidir package, right?\n$ doas apk add vidir ERROR: unable to select packages: vidir (no such package): required by: world[vidir] Er, no. You\u0026rsquo;ll need to use search again:\n$ apk search vidir moreutils-0.67-r0 There it is, moreutils. Great piece of software, by the way1.\nWhat if we could automate this?\nAutomating command-not-found: 1st try In bash, one can define a command_not_found_handle function. In zsh, one can define a command_not_found_handler function. I know, why can\u0026rsquo;t it be the same function, right? Just one r in the way. Regardless of whichever shell you use, the point is that the function is invoked whenever you run a command that is not in the $PATH (or that isn\u0026rsquo;t a shell built-in).\nIn principle, you could do:\ncommand_not_found_handle() { local cmd=\u0026#34;$1\u0026#34; apk search \u0026#34;$cmd\u0026#34; } It\u0026rsquo;s a good first try, and it surely works as expected, but it can be a bit noisy sometimes. Look at the podman output above, it outputs several unrelated packages, none of which provide the podman binary other than its homonym.\nAutomating command-not-found: 2nd try In Alpine, we can do slightly better. apk(8) has the concept of providers:\n$ apk list -P | awk \u0026#39;{print $1}\u0026#39; | egrep \u0026#39;\u0026lt;\\w+:\u0026#39; | cut -f 1 -d \u0026#39;:\u0026#39; | cut -c 2- | sort -u cmd dbus pc so -P above stands for --providers. This roughly means one can search for a package that provides a given shared library (so), or a package that provides a given binary (cmd), and so on. We\u0026rsquo;re interested in the cmd: provider.\nIf we tried it with podman, we would get the following output:\n$ apk list -P -- \u0026#34;cmd:podman\u0026#34; \u0026lt;cmd:podman\u0026gt; podman-3.4.4-r1 x86_64 {podman} (Apache-2.0) Look at how much shorter and direct it is, compared to the 1st approach!\nHere\u0026rsquo;s what it looks like if we try it with a binary provided by multiple packages:\n$ apk list -P -- \u0026#34;cmd:docker\u0026#34; \u0026lt;cmd:docker\u0026gt; docker-cli-20.10.11-r0 x86_64 {docker} (Apache-2.0) [installed] \u0026lt;cmd:docker\u0026gt; podman-docker-3.4.4-r1 x86_64 {podman} (Apache-2.0) It\u0026rsquo;s very easy to see that both docker-cli and podman-docker provide docker. If you just did a simple search, you\u0026rsquo;d get a lot of noise:\n$ apk search docker docker-bash-completion-20.10.11-r0 docker-cli-20.10.11-r0 docker-machine-driver-kvm2-1.24.0-r0 x11docker-6.9.0-r2 docker-volume-local-persist-1.3.0-r5 podman-docker-3.4.4-r1 openvswitch-2.12.3-r4 docker-engine-20.10.11-r0 docker-openrc-20.10.11-r0 dockerize-0.6.1-r9 docker-fish-completion-20.10.11-r0 openscap-1.3.5-r3 docker-py-5.0.3-r1 openvswitch-ovn-2.12.3-r4 docker-registry-openrc-2.7.1-r5 docker-doc-20.10.11-r0 rsyslog-imdocker-8.2108.0-r0 lazydocker-0.12-r2 docker-compose-bash-completion-1.29.2-r2 docker-compose-1.29.2-r2 py3-dockerpty-0.4.1-r4 docker-compose-zsh-completion-1.29.2-r2 docker-registry-2.7.1-r5 docker-credential-ecr-login-0.5.0-r2 dockerpy-creds-0.4.0-r3 docker-cli-compose-2.1.1-r0 docker-credential-ecr-login-doc-0.5.0-r2 podman-docker-doc-3.4.4-r1 docker-20.10.11-r0 docker-compose-fish-completion-1.29.2-r2 flannel-contrib-cni-0.15.1-r0 docker-zsh-completion-20.10.11-r0 docker-volume-local-persist-openrc-1.3.0-r5 docker-cli-buildx-0.7.1-r0 Packaging2 it all together I wrote the following scripts, which I source in my respective interactive shells, to achieve this behavior out-of-the-box:\n$ cat apk-command-not-found.bash #!/bin/bash # apk(8) from Alpine Linux command not found hook for bash command_not_found_handle () { local cmd=\u0026#34;$1\u0026#34; pkgs mapfile -t pkgs \u0026lt; \u0026lt;(apk list -P -- \u0026#34;cmd:$cmd\u0026#34; 2\u0026gt;/dev/null) if (( ${#pkgs[*]} )); then echo \u0026#34;$cmd may be found in the following packages:\u0026#34; printf \u0026#39; %s\\n\u0026#39; \u0026#34;${pkgs[@]}\u0026#34; else echo \u0026#34;bash: command not found: $cmd\u0026#34; fi 1\u0026gt;\u0026amp;2 return 127 } $ cat apk-command-not-found.zsh #!/bin/zsh # apk(8) from Alpine Linux command not found hook for zsh command_not_found_handler() { local cmd=\u0026#34;$1\u0026#34; local pkgs=(${(f)\u0026#34;$(apk list -P -- \u0026#34;cmd:$cmd\u0026#34; 2\u0026gt;/dev/null)\u0026#34;}) if [[ -n \u0026#34;$pkgs\u0026#34; ]]; then echo \u0026#34;$cmd may be found in the following packages:\u0026#34; printf \u0026#39; %s\\n\u0026#39; \u0026#34;${pkgs[@]}\u0026#34; else echo \u0026#34;zsh: command not found: $cmd\u0026#34; fi 1\u0026gt;\u0026amp;2 return 127 } The snippets above are snapshots intended for this post. I keep up-to-date versions of these files in my dotfiles repository, try out this query in case I ever move them elsewhere.\nhttps://joeyh.name/code/moreutils/: moreutils is a collection of the unix tools that nobody thought to write long ago when unix was young.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\npun intended\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/01/alpine-linux-apk-command-not-found-hook/","summary":"\u003cp\u003eIn this post we will learn how to define a command-not-found hook to the \u003ccode\u003eapk(8)\u003c/code\u003e package manager in Alpine Linux.\u003c/p\u003e\n\u003ch2 id=\"sneak-peek\"\u003eSneak peek\u003c/h2\u003e\n\u003cp\u003eBefore:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e$ podman\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ezsh: correct \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;podman\u0026#39;\u003c/span\u003e to \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;pod2man\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e[\u003c/span\u003enyae\u003cspan style=\"color:#f92672\"\u003e]\u003c/span\u003e? n\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ezsh: command not found: podman\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eAfter:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e$ podman\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ezsh: correct \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;podman\u0026#39;\u003c/span\u003e to \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;pod2man\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e[\u003c/span\u003enyae\u003cspan style=\"color:#f92672\"\u003e]\u003c/span\u003e? n\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epodman may be found in the following packages:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u0026lt;cmd:podman\u0026gt; podman-3.4.4-r1 x86_64 \u003cspan style=\"color:#f92672\"\u003e{\u003c/span\u003epodman\u003cspan style=\"color:#f92672\"\u003e}\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e(\u003c/span\u003eApache-2.0\u003cspan style=\"color:#f92672\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"★ Alpine Linux: apk command not found hook"},{"content":"Direnv is a tool to automate your shell to automatically load and unload environment variables on-the-fly, on a per-project (per-directory) basis.\nPreliminaries: Is it worth it? Questions I like to ask myself before deciding whether to invest my time into learning and potentially adopting a foreign tool are the following:\nIs it popular and stable enough? Is it abandonware?\nPopularity Popularity is relative, it doesn\u0026rsquo;t need to be worthy of the Hacker News frontpage nor Hotness on Reddit, but it needs to be widely available in popular Linux distributions and/or package managers, one install command away from my development environment.\nRepology is a good proxy for popularity. Looking at direnv therein, it\u0026rsquo;s available for Alpine, Arch, Debian, Fedora, FreeBSD, HomeBrew, Nix, OpenBSD, Ubuntu\u0026hellip;that\u0026rsquo;s more than enough, we can safely conclude direnv is widely popular.\nThe main takeaway we want to confirm is whether the project isn\u0026rsquo;t too niche and/or an one-man show. Seeing signs of a small-ish community and/or occasional contributions from external users/developers helps build confidence and give credibility to the project.\nStability and Abandonware Stability is easier to define than popularity and can often be determined just by taking a quick glance at the github (or whichever other forge it\u0026rsquo;s hosted in) page of the project.\nAt the time of this writing, the latest release of direnv was about a week ago (2.30.2, Dec 28th 2021). It\u0026rsquo;s definitely not abandonware and it\u0026rsquo;s well maintained. A few signs that help corroborate that:\nSeveral PRs were merged recently Its issue tracker is quite active, with a good mix of feature requests and bugs I don\u0026rsquo;t like to judge the project based on the number of issues it has, especially if it\u0026rsquo;s popular. Chromium has 60k+ issues at the time of this writing, yet I wouldn\u0026rsquo;t call it bleeding edge. Common sense applies. Since direnv has been around for a while and it\u0026rsquo;s relatively popular, 150+ open issue seems acceptable to me. Now that direnv passed the Litmus test for adoption1, let\u0026rsquo;s get our hands dirty.\nInstallation There\u0026rsquo;s nothing special here, as direnv is widely packaged. Pick your poison:\n$ sudo pacman -Syu direnv # Arch Linux $ doas apk add direnv # Alpine Linux $ sudo apt install direnv # Debian-based distros Is it lightweight?\n$ apk info -L direnv direnv-2.30.1-r0 contains: usr/bin/direnv Hell yes! More lightweight than that? Impossible. It\u0026rsquo;s a single binary thanks to Golang. No tons of files or dependencies. I mean:\n$ du -sh /usr/bin/direnv 7.5M /usr/bin/direnv \u0026hellip;it\u0026rsquo;s a 7MB binary, let\u0026rsquo;s not get ahead of ourselves. But that\u0026rsquo;s fine, really, it\u0026rsquo;s just a dev tool, we don\u0026rsquo;t really deploy it to prod.\nUse Cases Everything is controlled with a .envrc file within a repository root. A typical file could look like this:\nexport HOUSE=\u0026#34;ATREIDES\u0026#34; layout python3 The upstream website does a great job at summarizing use cases. I am not here to duplicate documentation, so please go ahead and read it. That said, here are some example use cases I found useful:\nUse Case: Python Python developers often need to create different virtual environments for different projects. For example, I was participating in Advent of Code last year and wrote my solutions in Python 3: https://github.com/thiagowfx/adventofcode.\nEach day2 I would cd ~/projects/adventofcode, and then do source ~/.venv/bin/activate. And guess what, that\u0026rsquo;s for the first terminal where I\u0026rsquo;d run make, I\u0026rsquo;d also spawn a second one with vim, thereby needing to activate the virtual environment twice.\nAnd this is assuming the virtual environment already exists. If it didn\u0026rsquo;t - for example, after a vanilla git clone, I\u0026rsquo;d have to do python -m venv .venv first.\nQuickly all of this became repetitive and annoying. I kinda \u0026ldquo;cheated\u0026rdquo; and stopped using the virtualenv for a few days, relying on my Linux distribution package manager instead:\n% apk add py3-{autopep8,pyflakes,numpy,pylint} This way, my\nimport numpy would correctly work and not yell that numpy was nowhere to be found.\nIt\u0026rsquo;s not very clean, but it worked. However eventually I wanted to become cleaner and leaner and automate my virtual environment setup. I uninstalled the aforementioned packages after a few days:\n% apk del py3-{autopep8,pyflakes,numpy,pylint} \u0026hellip;therefore forcing me to come up with a better setup. I always had direnv in my TODO list, and this was the perfect moment to try it out.\nHow does direnv address this?\nAdd the direnv hook to your shell. I actively use two shells3, bash and zsh, so I did it twice and then added it to my dotfiles: Bash:\n$ cat ~/.bashrc.d/direnv.bash #!/bin/bash # https://direnv.net/ if hash direnv \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; then eval \u0026#34;$(direnv hook bash)\u0026#34; fi Zsh:\n$ cat ~/.zshrc.d/direnv.zsh #!/bin/zsh # https://direnv.net/ if (( $+commands[direnv] )); then eval \u0026#34;$(direnv hook zsh)\u0026#34; fi Set up direnv in the AOC repository: $ cat ~/projects/adventofcode/.envrc layout python3 $ direnv allow # Only needs to be done once That\u0026rsquo;s it: It\u0026rsquo;s a single line of configuration. Now what does it do? All of the above. No magic: whenever you cd into the project directory or any of its subdirectories with one of the configured shells, if the venv doesn\u0026rsquo;t exist:\nit will be automatically created; then it will be sourced Now you may ask yourself: Why go through all this trouble? Why not simply create a shell script to do exactly that for you automatically? That\u0026rsquo;s perfectly fine, it\u0026rsquo;s a matter of taste. But then you\u0026rsquo;ll have to maintain that script. The python ecosystem keeps changing - a few years ago I was using virtualenvwrapper to manage virtual environments, these days it doesn\u0026rsquo;t exist anymore, people use either python -m env or pyenv or poetry or\u0026hellip;it never ends. Drew DeVault wrote a good piece about that.\nXKCD Courtesy of Randall Munroe\nMaintenance is not the only burden, scalability is also one: If you use python in several repositories, you\u0026rsquo;ll now have to include your script in all of them.\nConsidering that direnv is flexible enough in other scenarios, I consider its adoption in this situation a good trade-off to make.\nUse Case: Hugo This blog is written in Hugo. I have a Makefile with a bunch of environment variables to manage its setup:\n$ make dev Whenever I am working in my VPS, for reasons outside of the scope of this post I need to use a different port other than the default one for Hugo (1313). Since I am using variables, I could just do:\n$ make PORT=1234 dev However, to make this change permanent (\u0026ldquo;fire-and-forget\u0026rdquo;), I could also do:\n$ echo \u0026#39;export PORT=1234\u0026#39; | tee -a .envrc $ direnv allow # Only needs to be done once $ make dev This way, whenever I run make I wouldn\u0026rsquo;t even need to think twice about which port to use.\nOf course, a small improvement that should be done in this scenario is to add direnv related files to your .gitignore:\n$ git ignore direnv \u0026gt;\u0026gt; .gitignore # Created by https://www.toptal.com/developers/gitignore/api/direnv # Edit at https://www.toptal.com/developers/gitignore?templates=direnv ### direnv ### .direnv .envrc # End of https://www.toptal.com/developers/gitignore/api/direnv Other use cases? I don\u0026rsquo;t have other real use cases to share because only recently I became familiarized with direnv. That said, the direnv docs are very comprehensive of its full potential usage.\nSome use cases that I like:\ndotenv Automatically sources .env (note: not to confuse with .envrc) files, which are widely common in projects managed with docker-compose. source_env + env_vars_required Alongside .gitignore, this is a great way to source secrets (e.g. API keys or tokens) and not accidentally check them into your repository. fetchurl bash | curl is a cancer4 that should arguably be stopped due to its inherent security risks. That said, direnv provides a safer way to work with it because you can specify a hash to ensure you\u0026rsquo;re downloading the same script - if an attacker or malicious actor modified it, direnv would throw an error. path_add If your project outputs to e.g. build/\u0026lt;...\u0026gt;/bin or similar (typical in cmake projects and AFAIK in Rust ones too), you could add that directory to your PATH so that you could easily execute your binaries, without having to write the full subdirectory path each time. layout Besides python, direnv supports several other programming languages out-of-the-box. Popular examples include go, nix, node, perl and ruby. Downsides? One could call direnv bloated because of all of the aforementioned capabilities. If it doesn\u0026rsquo;t spark joy for your taste, consider using autoenv which is basically a leaner version of direnv, meant mostly for doing one thing and doing it well: setting and unsetting variables.\nOther than that, direnv is pretty much a great piece of software.\nOne thing I didn\u0026rsquo;t cover is how secure it is: You need to run direnv allow explicitly in order to tell direnv that you trust a given .envrc file. If you don\u0026rsquo;t do it, direnv will refuse to source it:\n$ touch .envrc direnv: error ~/projects/foo/.envrc is blocked. Run `direnv allow` to approve its content If you run direnv allow but later on the file is modified (for example, after git pull, whereby you retrieve a modification from a teammate), direnv will once again refuse to operate. You\u0026rsquo;ll need to whitelist it again by re-running direnv allow. Direnv will snapshot/hash the file contents of .envrc remember it across sessions.\nReferences Tools You Should Know About: direnv Obviously the aforementioned list was non-exhaustive. There are a few other questions that you may want to ask, out of scope of this article, such as: (i) does the project have an OSS or FLOSS license? (ii) does the project depend on Java?\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAdvent of code challenges are released one by one, thereby forcing you to wait until the next day in order to get the next challenge.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nmore on this another day\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nc.f. https://curlpipesh.tumblr.com/, https://gnu.moe/wallofshame.md\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/01/direnv-automate-your-environment-variables/","summary":"\u003cp\u003e\u003ca href=\"https://direnv.net/\"\u003eDirenv\u003c/a\u003e is a tool to automate your shell to automatically load and unload environment variables on-the-fly, on a per-project (per-directory) basis.\u003c/p\u003e","title":"★ Direnv: Automate your Environment Variables"},{"content":"This article describes my experience transitioning to, setting up and using the Miniflux RSS reader for the first time.\nPreamble I always kind of enjoyed following people and blogs via RSS, even though it has never been a key part of my workflow (nor of the mainstream web). That said, I am not here to convince you why RSS is great, there are good existing resources12 for that already.\nInitially I had used Commafeed, Feedly and Inoreader, which are hosted solutions. They are mostly OK, especially if you only have a handful of feeds. Their free offerings are quite decent, with a limit of a hundred or so feeds. They also have mobile clients (Android, iOS) which are a must these days. I was never fully converted to them though, and my workflow therein would only last for a few weeks or months. Some common barriers were:\ntheir recommendations and \u0026lsquo;machine learning\u0026rsquo; fluff were a consistent source of stress, with a fear of missing out (FOMO) akin to social media. I felt pressured to keep following new blogs just like I am pressured to constantly \u0026rsquo;like\u0026rsquo; and \u0026lsquo;follow\u0026rsquo; new pages in traditional American social media.\nthere was a lot of context switching: many upstream RSS feeds aren\u0026rsquo;t great, for example, by providing excerpts (summaries) only I\u0026rsquo;d always have to visit the website directly if I wanted to read full articles. This doesn\u0026rsquo;t scale well long term, attention is a precious resource and our brains aren\u0026rsquo;t great at keeping steady and focused attention if we constantly context switch. A classical feed like this is Paul Graham\u0026rsquo;s.\nthere was no ability to filter out (exclude) posts from feeds. For example, deleting posts with a certain title (Sponsor, Ad, some boring mainstream topic). A classical example is John Gruber\u0026rsquo;s Daring Fireball sponsored posts which usually have \u0026lsquo;Sponsor\u0026rsquo; in their titles. Why do I have to manually skip these posts, why can\u0026rsquo;t I teach my RSS reader to do it automatically for me?\nlock-in: whenever I starred/saved posts that I liked for future reference, they would be stuck in the specific cloud provider I chose.\nThese were some of my gripes.\nDuring those years I had also tried to self-host TinyTinyRSS but it didn\u0026rsquo;t really last for me:\nfirst, its stack is relatively bloated: hosting and maintaining a typical LAMP stack takes some considerable amount of effort — TinyTinyRSS requires a full PHP installation alongside a webserver (apache, nginx or similar) and a database. Suddenly there was a lot of complexity to maintain all that.\nsecond, I didn\u0026rsquo;t have any cloud resources (VPS), nor a local server in my home (e.g. a Raspberry Pi or a NUC or a NAS Appliance). An instance in my personal laptop wouldn\u0026rsquo;t really scale either as I would have needed it to be always on if I wanted to have continuous access to it (e.g. from my phone).\nthird, I wasn\u0026rsquo;t a seasoned sysadmin at the time and wasn\u0026rsquo;t really looking forward to self-host.\nThen 2020 and the COVID-19 pandemic came along with all of its imposed government lockdowns worldwide. Suddenly many people had a lot of free time on their hands.\nSelf-hosting at home Having an Arch Linux workstation at home, it felt natural to try out Miniflux there first.\nMiniflux has great upstream documentation already, therefore it\u0026rsquo;s just a matter of following it. It\u0026rsquo;s out of scope of this post to duplicate the installation process here, however I will add a bit of color regarding my initial setup.\nDisclaimer: Those instructions will probably get out-of-date at some point.\nThankfully there\u0026rsquo;s already a miniflux package for Arch, making my job much easier. Installing miniflux alone isn\u0026rsquo;t enough though, we will also need to install a database server (PostgreSQL):\n$ sudo pacman -Syu miniflux postgresql The next step is to configure the PostgreSQL server. Refer to the upstream documentation for that, but the TL;DR is:\ncreate a miniflux user create a miniflux database owned by the miniflux user perform a few tweaks (extension hstore) Then configure miniflux:\n$ cat /etc/miniflux.conf # Purge articles after a few days: These values are actually the default. Listed here just for reference. CLEANUP_ARCHIVE_READ_DAYS=30 CLEANUP_ARCHIVE_UNREAD_DAYS=90 # Database configuration DATABASE_URL=user=miniflux password=\u0026lt;password\u0026gt; dbname=miniflux sslmode=disable RUN_MIGRATIONS=yes We will also need to create an admin user for miniflux with miniflux --create-admin.\nThen we start the database server and miniflux:\n$ sudo systemctl enable --now postgresql miniflux Afterwards it\u0026rsquo;s just a matter of navigating to http://localhost:8080 and logging in with your newly created admin user.\nMiniflux is a pleasure to use, and it\u0026rsquo;s very easy to get acquainted with it.\nIt\u0026rsquo;s also possible to add custom CSS in its Settings. I added the following tweaks3 for improved typography:\n:root { --system-font-family: system-ui, -apple-system, BlinkMacSystemFont, \u0026#34;Segoe UI\u0026#34;, Roboto, Helvetica, Arial, \u0026#34;Noto Sans\u0026#34;, sans-serif, \u0026#34;Apple Color Emoji\u0026#34;, \u0026#34;Segoe UI Emoji\u0026#34;, \u0026#34;Segoe UI Symbol\u0026#34;, \u0026#34;Noto Color Emoji\u0026#34;; --font-family: var(--system-font-family); --entry-content-font-family: var(--system-font-family); } body { max-width: 900px; } textarea[name=\u0026#34;custom_css\u0026#34;] { min-height: 300px; width: -webkit-fill-available; } Update(2022-02-18): It turns out the CSS above isn\u0026rsquo;t really needed as a system font stack is set out-of-the-box.\nThe one main limitation of running Miniflux this way is that you\u0026rsquo;ll need your workstation to be always on if you want to have continuous access to it. This means if you want to read late at night you\u0026rsquo;ll need to leave your computer on. Not only this is impractical and inconvenient, it\u0026rsquo;s also not much environmentally friendly.\nAnother limitation is that in principle you\u0026rsquo;ll only be able to access Miniflux from home, unless you take extra measures4 to make your workstation accessible from outside your home network.\nOther resources Miniflux clients:\nMiniflux web app (PWA), works well enough Unread (iOS) via Fever API, gesture based Reeder (iOS) via Fever API newsboat (CLI) Common self-hosted alternatives to miniflux:\nFreshRSS TinyTinyRSS Feedbin (harder to self-host) https://kevq.uk/please-add-rss-support-to-your-site/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://laurakalbag.com/subscribe/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://css-tricks.com/snippets/css/system-font-stack/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nfor example: a VPN like OpenVPN, Wireguard or tailscale; or a tool like ngrok.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2022/01/miniflux-rolling-my-own-rss-reader/","summary":"\u003cp\u003eThis article describes my experience transitioning to, setting up and using the\n\u003ca href=\"https://miniflux.app/\"\u003eMiniflux\u003c/a\u003e RSS reader for the first time.\u003c/p\u003e","title":"★ Miniflux: Rolling my own RSS Reader"},{"content":"If you ever find yourself in a hurry to get access to a Linux shell but don\u0026rsquo;t easily have one at hand (e.g. a workstation or a raspberry pi or a VPS), it\u0026rsquo;s possible to quickly get access to an ephemeral instance in the cloud.\nHere\u0026rsquo;s a non-exhaustive list of trusted providers:\nGoogle Cloud Shell Welcome to Google Cloud Shell, a tool for managing resources hosted on Google Cloud Platform! The machine comes pre-installed with the Google Cloud SDK and other popular developer tools.\nYour 5GB home directory will persist across sessions, but the VM is ephemeral and will be reset approximately 20 minutes after your session ends. No system-wide change will persist beyond that.\nType \u0026ldquo;gcloud help\u0026rdquo; to get help on using Cloud SDK. For more examples, visit https://cloud.google.com/shell/docs/quickstart and https://cloud.google.com/shell/docs/examples\nType \u0026ldquo;cloudshell help\u0026rdquo; to get help on using the \u0026ldquo;cloudshell\u0026rdquo; utility. Common functionality is aliased to short commands in your shell, for example, you can type \u0026ldquo;dl \u0026rdquo; at Bash prompt to download a file. Type \u0026ldquo;cloudshell aliases\u0026rdquo; to see these commands.\nType \u0026ldquo;help\u0026rdquo; to see this message any time. Type \u0026ldquo;builtin help\u0026rdquo; to see Bash interpreter help.\nSee also: https://cloud.google.com/shell\nGithub Codespaces 👋 Welcome to Codespaces! You are on our default image.\nIt includes runtimes and tools for Python, Node.js, Docker, and more. See the full list here: https://aka.ms/ghcs-default-image Want to use a custom image instead? Learn more here: https://aka.ms/configure-codespace 🔍 To explore VS Code to its fullest, search using the Command Palette (Cmd/Ctrl + Shift + P or F1).\n📝 Edit away, run your app as usual, and we\u0026rsquo;ll automatically make it available for you to access.\nSee also: https://github.com/codespaces\nTip: Replace https://github.com/username/repository with https://github.dev/username/repository to automatically open the repository within a github codespace. Alternatively, if you have github shortcuts enabled, press . while in the repository page.\n","permalink":"https://www.perrotta.dev/2022/01/ephemeral-linux-shell-access-in-the-cloud/","summary":"\u003cp\u003eIf you ever find yourself in a hurry to get access to a Linux shell but don\u0026rsquo;t\neasily have one at hand (e.g. a workstation or a raspberry pi or a VPS), it\u0026rsquo;s\npossible to quickly get access to an ephemeral instance in the cloud.\u003c/p\u003e","title":"Ephemeral Linux Shell Access in the Cloud"},{"content":" Ufw stands for Uncomplicated Firewall, and is a program for managing a netfilter firewall. It provides a command line interface and aims to be uncomplicated and easy to use.\nThe firewall makes justice to its name as it is really uncomplicated, and a pleasure to set up.\nInstall Install and set up ufw1, which should be packaged for most linux distributions:\nOpenRC-based (Alpine Linux, Gentoo) # Install ufw and ufw-extras $ doas apk install ufw{,-extras} # Enable ufw daemon $ doas rc-update add ufw # Start ufw daemon $ doas rc-service ufw start # Enable firewall $ doas ufw enable Systemd-based (Arch Linux, Debian) # Install ufw and ufw-extras $ sudo pacman -Syu ufw{,-extras} # Enable and start ufw daemon $ sudo systemctl enable --now ufw # Enable firewall $ sudo ufw enable Add rules Firewall rules can be added with ufw allow [port] or ufw allow [name]. Named profiles (for example: ssh, http) live in /etc/ufw/applications.d/, or you can query all of them with ufw app list.\n% ufw app list Available applications: AIM Bonjour CIFS DNS Deluge IMAP IMAPS IPP KTorrent Kerberos Admin Kerberos Full Kerberos KDC Kerberos Password LDAP LDAPS LPD MSN MSN SSL Mail submission NFS POP3 POP3S PeopleNearby SMTP SSH Socks Telnet Transmission Transparent Proxy VNC WWW WWW Cache WWW Full WWW Secure XMPP Yahoo qBittorrent svnserve Ufw also supports ufw limit [port | name] which is like add but with the added ability to \u0026ldquo;deny connections from IP addresses that attempt to initiate 6 or more connections in the last 30 seconds\u0026rdquo;. It\u0026rsquo;s a good measure to mitigate brute-force and/or DDOS attacks.\n# either use named profiles % ufw allow http-alt % ufw limit ssh # or port numbers % ufw allow 8080/tcp % ufw limit 22 Remove rules Firewall rules can be removed by merely adding \u0026lsquo;delete\u0026rsquo; between ufw and the verb.\n% ufw delete allow 8080/tcp % ufw delete limit ssh Check status One status command to rule them all, \u0026ldquo;verbose\u0026rdquo; is optional:\n% ufw status verbose Status: active Logging: on (low) Default: deny (incoming), allow (outgoing), disabled (routed) New profiles: skip To Action From -- ------ ---- 22/tcp LIMIT IN Anywhere 8080 ALLOW IN Anywhere 22/tcp (v6) LIMIT IN Anywhere (v6) 8080 (v6) ALLOW IN Anywhere (v6) Ufw uses iptables under the hood. Inspect the underlying iptables rules:\n% iptables -S | egrep \u0026#39;\\b(22|8080)\\b\u0026#39; -A ufw-user-input -p tcp -m tcp --dport 22 -m conntrack --ctstate NEW -m recent --set --name DEFAULT --mask 255.255.255.255 --rsource -A ufw-user-input -p tcp -m tcp --dport 22 -m conntrack --ctstate NEW -m recent --update --seconds 30 --hitcount 6 --name DEFAULT --mask 255.255.255.255 --rsource -j ufw-user-limit -A ufw-user-input -p tcp -m tcp --dport 22 -j ufw-user-limit-accept -A ufw-user-input -p tcp -m tcp --dport 8080 -j ACCEPT References https://help.ubuntu.com/community/UFW https://wiki.archlinux.org/title/Uncomplicated_Firewall ufw-extras is optional, it contains additional rules (e.g. mosh, tailscale).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2021/12/ufw-firewall/","summary":"\u003cblockquote\u003e\n\u003cp\u003eUfw stands for Uncomplicated Firewall, and is a program for managing a netfilter firewall. It provides a command line interface and aims to be uncomplicated and easy to use.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Ufw: Firewall"},{"content":"Every modern terminal emulator supports ANSI OSC-52 out-of-the-box. xterm is not one of those1.\nOSC-52 allows one to copy text into the system clipboard. It\u0026rsquo;s a very handy escape sequence to be used alongside terminal emulators and terminal multiplexers such as tmux/screen.\nIt\u0026rsquo;s also possible to enable OSC-52 in vim, making copy-and-paste a first-class citizen therein.\nAs mentioned in the intro, most modern applications already support it out-of-the-box, as such there\u0026rsquo;s no need to configure them. We would like to configure xterm as well though, because it is widely available in pretty much every Unix out there.\n$ grep -B 1 -i allowWindowOps ~/.Xresources ! osc-52 support *.allowWindowOps: true Then apply:\n$ xrdb -merge ~/.Xresources All new xterm applications should then pick up the new resource.\nmaybe because it\u0026rsquo;s not modern, and it\u0026rsquo;s not decent? ;)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2021/12/xterm-enable-ansi-osc-52/","summary":"\u003cp\u003eEvery modern terminal emulator supports \u003ca href=\"https://invisible-island.net/xterm/ctlseqs/ctlseqs.html#h3-Operating-System-Commands\"\u003eANSI OSC-52\u003c/a\u003e out-of-the-box. \u003ccode\u003exterm\u003c/code\u003e is not one of those\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e","title":"Xterm: Enable ANSI OSC-52"},{"content":"Whenever I connect/disconnect my bluetooth headphones to my Linux workstation, I need to manually switch to/off them.\nEvery commercial OS I\u0026rsquo;m aware of does that automatically, including Chrome OS, macOS and Windows.\nTo automate this behavior on Linux, it depends whether we\u0026rsquo;re using PulseAudio or Pipewire. It boils down to loading the module-switch-on-connect pulseaudio module.\nNote: Tested on Arch Linux only.\nPulseAudio $ sudoedit /etc/pulse/default.pa ... load-module module-switch-on-connect ... Then restart pulseaudio:\n$ systemctl --user restart pulseaudio PipeWire $ sudo mkdir -p /etc/pipewire $ sudo cp /usr/share/pipewire/pipewire-pulse.conf /etc/pipewire/pipewire-pulse.conf $ sudoedit /etc/pipewire/pipewire-pulse.conf ... # Extra modules can be loaded here. Setup in default.pa can be moved here context.exec = [ { path = \u0026#34;pactl\u0026#34; args = \u0026#34;load-module module-switch-on-connect\u0026#34; } ] ... Alternatively, ~/.config/pipewire/pipewire-pulse.conf should also work. We should not edit the file in /usr because it will not survive package upgrades.\nThen restart pipewire:\n$ systemctl --user restart pipewire{,-pulse} References https://wiki.archlinux.org/title/PulseAudio#Switch_on_connect https://gitlab.freedesktop.org/pipewire/pipewire/-/wikis/Config-PulseAudio#configuration-file-pipewire-pulseconf ","permalink":"https://www.perrotta.dev/2021/12/linux-auto-switch-to-bluetooth-headset/","summary":"\u003cp\u003eWhenever I connect/disconnect my bluetooth headphones to my Linux workstation, I need to manually switch to/off them.\u003c/p\u003e","title":"Linux: Auto Switch to Bluetooth Headset"},{"content":"I recently purchased a Keychron K2 mechanical keyboard. It is a 75% keyboard that has bluetooth. This article describes some quality-of-life improvements to use it with a Linux system.\nNote: Tested on Arch Linux only.\nKernel Module Keychron keyboards are recognized out-of-the-box as Apple keyboards on Linux systems.\nThe corresponding kernel module is hid_apple.\n$ modinfo hid_apple Ensure the module is loaded within the initram for setups that use LUKS:\n$ grep MODULES -B 1 /etc/mkinitcpio.conf # hid_apple for Keychron K2 MODULES=(hid_apple) This works for wired mode but alas it\u0026rsquo;s not possible to use the keyboard wirelessly to type in your LUKS password unless extra steps are taken:\nInstall the mkinitcpio-bluetooth package, currently available in the AUR. Add the bluetooth hook to your mkinitcpio, ensuring it comes before encrypt. $ grep HOOKS -B 1 /etc/mkinitcpio.conf # bluetooth for Keychron K2 HOOKS=(base udev autodetect keyboard bluetooth modconf block encrypt filesystems resume fsck) Regenerate it: % mkinitcpio -P. Bluetooth There are 3 bluetooth slots, corresponding to the 1, 2 and 3 keys.\nTo put the keyboard in pairing mode, hold Fn + \u0026lt;n\u0026gt; for a few seconds. The key will persistently blink. From the computer, connect to the device named Keychron K2. Trust the keyboard.\nSwitch slots by pressing Fn + \u0026lt;n\u0026gt; once. There is some light feedback to indicate the switch.\nBluetooth works well out-of-the-box, but the keyboard automatically sleeps after 10 minutes of inactivity to save energy. While it is possible to disable this behavior, I find it welcome. It is a hassle though because the bluetooth device refuses to reconnect once the keyboard is awaken. The naive solution is to pair it again from scratch, but a better user experience is to change bluetooth settings:\n$ cat /etc/bluetooth/main.conf ... [General] FastConnectable=true [Policy] UserspaceHID=true ... And then restart bluetooth to apply them:\n% systemctl restart bluetooth This way the keyboard always automatically reconnects to the computer.\nI also find it convenient to leave bluetooth enabled on startup:\n# Enable the bluetooth daemon. % systemctl enable bluetooth # Enable the bluetooth adapter. $ cat /etc/bluetooth/main.conf ... [Policy] AutoEnable=true ... Finally, another tweak is to make the bluetooth adapter stay awake otherwise it may periodically disconnect:\n$ cat /etc/modprobe.d/btusb.conf # Turn off bluetooth autosuspend. options btusb enable_autosuspend=0 Function Keys and Multimedia Keys The default fnmode is set to multimedia keys:\n0 = disabled 1 = normally media keys, switchable to function keys by holding Fn key (Default) 2 = normally function keys, switchable to media keys by holding Fn key I prefer to set it to function keys. One of the reasons for that is to make switching TTYs (Ctrl+Alt+Fn) possible. For some reason, Fn doesn\u0026rsquo;t seem to work in TTYs.\nChange it in the current session only:\n% echo 2 \u0026gt;\u0026gt; /sys/module/hid_apple/parameters/fnmode If you do not have a root shell, use sudo / doas + tee:\n% echo 2 | sudo tee /sys/module/hid_apple/parameters/fnmode Change it permanently:\n$ cat /etc/modprobe.d/hid_apple.conf # Use function keys by default. Press Fn to use multimedia keys. options hid_apple fnmode=2 And then reload the kernel module:\n% modprobe -r hid_apple \u0026amp;\u0026amp; modprobe hid_apple Insert Key By default there is no native Insert key. Use Fn + Del to trigger Insert. For example, Fn + Shift + Del works like Shift + Insert in X11, yielding paste selection.\nBattery Check the battery level programatically:\n% pacman -S upower $ upower --dump | grep -i keyboard -A 7 | grep percentage percentage: 71% This only works in bluetooth mode (not in wired mode).\nShortcuts Hold Fn + [1 | 2 | 3] for a few seconds: Put bluetooth slot in pairing mode. The corresponding LED will persistently blink until pairing is complete. Fn + [1 | 2 | 3]: Toggle bluetooth slot. The corresponding LED will briefly blink. Fn + b: Check battery level visually. Green is more than 70%, blue is more than 30%, otherwise it will flash. Fn + Light: Toggle keyboard lights on/off. Fn + [Left | Right]: Browse keyboard light color schemes. References https://wiki.archlinux.org/title/Apple_Keyboard https://github.com/kurgol/keychron https://gist.github.com/andrebrait/961cefe730f4a2c41f57911e6195e444 https://mikeshade.com/posts/keychron-linux-function-keys/ https://github.com/kurgol/keychron/blob/master/k2.md ","permalink":"https://www.perrotta.dev/2021/12/keychron-k2-linux-setup/","summary":"\u003cp\u003eI recently purchased a \u003ca href=\"https://www.keychron.com/products/keychron-k2-wireless-mechanical-keyboard\"\u003eKeychron K2\u003c/a\u003e mechanical keyboard. It is a 75% keyboard that has bluetooth. This article describes some quality-of-life improvements to use it with a Linux system.\u003c/p\u003e","title":"★ Keychron K2: Linux Setup"},{"content":"Hello World from Hugo!\nI am joining the indieweb and the JAMStack by rolling my own blog backed by a static site generator (SSG) and deployed with git.\nPreviously I used to have a blog hosted on Wordpress called Everyday Serendipity.\n","permalink":"https://www.perrotta.dev/2021/12/hello-world-from-hugo/","summary":"Hello World from Hugo!\nI am joining the indieweb and the JAMStack by rolling my own blog backed by a static site generator (SSG) and deployed with git.\nPreviously I used to have a blog hosted on Wordpress called Everyday Serendipity.","title":"Hello World from Hugo"},{"content":"Recentemente participei de uma CTF promovida pelo ELT (Epic Leet Team). Uma das challs que consegui resolver completamente foi a matroshka, e aqui está um breve write-up sobre a mesma.\nDado um arquivo matroshka.tar.gz, precisávamos encontrar a flag.\nNão era difícil desconfiar do que esse arquivo / chall se tratava: matroshkas são aquelas bonecas russas que se encaixam umas dentro das outras. Então\u0026hellip;de cara, logo já desconfiei: provavelmente existe um arquivo compactado dentro de outro, dentro de outro, dentro de outro, e assim por diante\u0026hellip;\nPor experiência, não valeria a pena tentar descompactar tudo manualmente, pois sabe-se lá quantos níveis de compactação esse negócio iria ter (provavelmente mais do que 100).\nDe cara logo pensei em usar o dtrx, que é um excelente programa (não perco tempo e sempre rodo um port install dtrx) para extrair arquivos sem ter que ficar se lembrando das sintaxes individuais de cada programa. Nesse caso, não iria rolar: os arquivos eram renomeados de forma a trickear o dtrx, que funciona através de heurísticas, uma delas é a \u0026rsquo;extensão\u0026rsquo; do nome do arquivo. Por exemplo, vários arquivos (após descompactados) eram renomeados na forma *.elt.\nA segunda alternativa foi (serendipidade, não conhecia essa ferramenta antes) tentar utilizar o atool. Por motivos similares ao dtrx, não rolou.\nPois bem, então o jeito ia ser descompactar tudo na marra. Pensei em escrever um programa que faria o seguinte:\ntry { unzip \u0026lt;file\u0026gt; } catch { try { tar xf \u0026lt;file\u0026gt; } catch { // ...e assim por diante } } Obviamente eu utilizaria os programas diretamente, então a coisa poderia ficar um pouco mais simples, utilizando os return codes dos mesmos para detectar se descompactaram o arquivo com sucesso. Por exemplo, tar xf \u0026lt;file\u0026gt; retorna 0 se rodou corretamente, do contrário ele retorna algo diferente de zero. Isso se mostrou válido para todos os programas de descompactação que utilizei, exceto o lha, que insistia em retornar 0 de qualquer jeito, mesmo quando falhava.\nPara automatizar essa tarefa, resolvi utilizar python2. C/C++ provavelmente também seriam bons candidatos, mas eu queria praticar o meu python.\nApós algumas inspeções, notei que cada arquivo continha um e somente um arquivo dentro dele, então a ideia base seria:\nmantenha uma lista com todos os arquivos conhecidos até então (no começo, só haveria um); descompacte esse arquivo; detecte qual arquivo acabou de ser descompactado continue fazendo isso até encontrar a flag Meu código ficou assim:\n#!/usr/bin/env python import os import subprocess TARGET_DIR = \u0026#39;mat\u0026#39; def uncompress_kgb(file): return subprocess.call([\u0026#34;kgb\u0026#34;, file]) def uncompress_gzip(file): return subprocess.call([\u0026#34;gunzip\u0026#34;, \u0026#34;-S\u0026#34;, \u0026#39;.\u0026#39; + file.split(\u0026#39;.\u0026#39;)[-1], file]) def uncompress_tar(file): return subprocess.call([\u0026#34;tar\u0026#34;, \u0026#34;xvf\u0026#34;, file]) def uncompress_rar(file): return subprocess.call([\u0026#34;unrar\u0026#34;, \u0026#34;x\u0026#34;, file]) def uncompress_lha(file): return subprocess.call([\u0026#34;lha\u0026#34;, \u0026#34;e\u0026#34;, file]) def uncompress_zip(file): return subprocess.call([\u0026#34;unzip\u0026#34;, file]) def uncompress_arj(file): subprocess.call([\u0026#34;cp\u0026#34;, file, file + \u0026#34;.arj\u0026#34;]) err = subprocess.call([\u0026#34;arj\u0026#34;, \u0026#34;x\u0026#34;, file]) subprocess.call([\u0026#34;rm\u0026#34;, file + \u0026#34;.arj\u0026#34;]) return err def uncompress_7z(file): subprocess.call([\u0026#34;7z\u0026#34;, \u0026#34;x\u0026#34;, file]) def colorprint(s): print \u0026#39;\\033[93m\u0026#39; + repr(s) + \u0026#39;\\033[0m\u0026#39; os.chdir(TARGET_DIR) base = set() while True: newbase = set(os.listdir(\u0026#39;.\u0026#39;)) diff = newbase - base colorprint(diff) if len(diff) \u0026gt; 1: raise Exception(\u0026#34;len(diff) \u0026gt; 1\u0026#34;) elif len(diff) == 0: print \u0026#34;len(diff) == 0\u0026#34; break for file in diff: err = uncompress_kgb(file) if err != 0: err = uncompress_gzip(file) if err != 0: err = uncompress_tar(file) if err != 0: err = uncompress_rar(file) if err != 0: err = uncompress_zip(file) if err != 0: err = uncompress_arj(file) if err != 0: err = uncompress_7z(file) if err != 0: err = uncompress_lha(file) if err != 0: print \u0026#34;lha fail\u0026#34; base = newbase Essa ideia funcionou bastante bem. A única coisa overkill foi que eu não deletei arquivos anteriores; isso poderia ter simplificado significativamente o problema (e os sets no python).\nAdemais, uma das coisas chatas do arj é que ele só é capaz de extrair arquivos que terminam em *.arj, então fui obrigado a renomear/copiar um arquivo antes de tentar utilizá-lo para extrair seu conteúdo.\n","permalink":"https://www.perrotta.dev/2016/10/matroshka/","summary":"\u003cp\u003eRecentemente participei de uma\n\u003ca href=\"https://ctf.tecland.com.br/Pwn2Win/game/scoreboard/\"\u003eCTF\u003c/a\u003e promovida pelo\n\u003ca href=\"https://ctf-br.org/elt\"\u003eELT\u003c/a\u003e (Epic Leet Team). Uma das \u003cem\u003echalls\u003c/em\u003e que consegui\nresolver completamente foi a \u003cstrong\u003ematroshka\u003c/strong\u003e, e aqui está um breve \u003cem\u003ewrite-up\u003c/em\u003e\nsobre a mesma.\u003c/p\u003e\n\u003cp\u003eDado um arquivo \u003ccode\u003ematroshka.tar.gz\u003c/code\u003e, precisávamos encontrar a \u003cem\u003eflag\u003c/em\u003e.\u003c/p\u003e","title":"Matroshka"},{"content":"Chromebooks are excellent for testing and playing with Linux userland[1] stuff. And, even better, (almost) every change you make to it can be reset back to its factory state — even in the worst case of completely wiping ChromeOS from your computer. As long as you don\u0026rsquo;t mess up with its firmware, you can do whatever you want and still be safe. Most models are cheap, battery life is amazing for a cheap Linux laptop (8+ hours, depending on the device and on usage)[2], Linux support is great (well, it ships with a Linux OS, right?) — even for most touchscreen models [citation needed] — and it is simple (as in KISS).\nNow, the previous paragraph sounded a little repetitive, but I\u0026rsquo;d like to focus on two of my previous points here:\nYou can do whatever you want and still remain safe. Yes. Indeed, you can reset your chromebook to its factory state with just a few clicks — this is still hard to do with Windows and OS X, as far as I know. I think Windows 10 is introducing a feature to ease this process, however I am not sure about it yet. This is super important (and convenient!) for people playing with a ChromeOS device — like me. My usual way to reset the system back to \u0026ldquo;factory state\u0026rdquo; was, usually, to install a Linux distro with the btrfs filesystem so I could create a snapshot upon the installation was finished, and then revert back to it at anytime I wanted. However, this solution is a little cumbersome. The second usual solution would be to simply use a virtual machine, but I don\u0026rsquo;t need to reiterate that this is slow and limited in many ways. Simple. This is really important if you don\u0026rsquo;t want to have headaches in the long run. For starters, simplicity is always a trade-off that involves performance: it is hard to be simple with a core i7 CPU, so if you choose the KISS path, you are deliberately sacrificing some of your performance. Once you accept that, we can move forward: Chromebooks are very simple devices. They\u0026rsquo;re almost directly comparable to a tablet with a keyboard. Sometimes they feel like a full-featured arduino. These were some small highlights about my decision to get one. Now here\u0026rsquo;s the good stuff:\nThere be dragons Disclaimer/Warning: these will potentially void the warranty of your device, and they are only recommended if you know what you\u0026rsquo;re doing. Be a good user and research before messing up with your system. I will provide upstream links with documentation as much as possible.\nEnable developer mode — it\u0026rsquo;s a must; however, it weakens the security of your device. Beware and research! This can be totally reverted with a single keystroke (SPACE) at the login screen that will appear in every subsequent boot of your system. dev_install — to get fdisk, python, tmux, nano, emerge (for binaries), and so on. These are just a few developer tools, they won\u0026rsquo;t scale and you won\u0026rsquo;t get whatever package you want in your vanilla ChromeOS, however these might help a lot with simple tasks. For example, you can format a SD card or an USB Flash Drive directly from ChromeOS upon running this tool — otherwise, you\u0026rsquo;d usually have to use another computer, with another OS to do it. It uses just a little space (~100M+ or so), and it can be totally reverted with devinstall \u0026lt;dash\u0026gt;\u0026lt;dash\u0026gt;uninstall(replace with -). Switch to the beta channel if you want to be slightly more into the bleeding edge — but not too much as in the dev channel. In fact, at the time of this post, select devices in beta channel have support for Android apps in ChromeOS. Open a new shell (crosh) with Ctrl + Alt + T. The default username is \u0026lsquo;chronos\u0026rsquo;, without any password. User-writable locations are: /usr/local (in particular, /usr/local/bin is in your PATH env variable out-of-the-box (echo $PATH)), /tmp and /home/chronos/user/Downloads. There might be more, but these are enough. You can also insert removable media (USB flash drives, SD cards), which would show up on /media. ChromeOS can read/write filesystems formatted as ext{2,3,4} and FAT{,32} at least. These are the basics. However, you will soon find out that you cannot do too much with ChromeOS, even with these modifications. In this case, there are three options, and all of them involve getting a Linux distribution in some way:\n(RECOMMENDED) Install a Linux distro on either a SD card or an USB Flash Drive and dual-boot from it. It is really easy to dual-boot, you just press C-d to boot from the internal eMMC / SSD, or C-u to boot from external storage. There\u0026rsquo;s no need to play with grub, EFI, or any (other) fancy bootloader. If you change your mind later on, just format your SD card: there is no need to change anything on ChromeOS. (NOT RECOMMENDED BY ME) Completely wipe ChromeOS by installing another Linux distribution on its place. Although it\u0026rsquo;s easy to revert this process through recovery media, it completely defeats the purpose of getting a Chromebook in the first place, especially now that Android apps are starting to become available on them (for touchscreen devices, of course). (PROBABLY WOULDN\u0026rsquo;T RECOMMEND, BUT YOU MAY LIKE IT) Install a chroot alongside your ChromeOS, such as crouton. I tried it however I hated it. It is very buggy and opinionated, somewhat similar to oh-my-zsh: both of them promise to deliver a good framework and they\u0026rsquo;re kinda large open source projects, however they do lots of automatic things for you, and you end up not knowing what is happening to your system. It opposes the Arch Linux philosophy, being user-friendly rather than user-centered. Some people like it, I won\u0026rsquo;t argue this, it can work for them; but not for me. An alternative would be to install a chroot manually, without the help of crouton. I\u0026rsquo;ve chosen the first option, installing Arch Linux ARM on my Chromebook. So far, so good: it\u0026rsquo;s working very well. Storage is easily shared between the two OSes (by using the SD card). There\u0026rsquo;s more to get out of my system, and I intend to share new findings in this blog. In particular, the most important one: to find an easy way (not crouton!) of running whatever Linux-compatible application I want from within Chrome OS, without dual-booting. I have a few ideas (well, it\u0026rsquo;s just a matter of creating a KISS chroot), I just want to polish them a little more before trying them out.\nSee also Generic Chrome OS Troubleshooter Chart v0.3 Footnotes I wouldn\u0026rsquo;t say the same for kernel stuff though. It is not trivial to install a bootloader (such as grub) or to boot a custom kernel in a Chromebook. It\u0026rsquo;s possible indeed, it\u0026rsquo;s just not as nice or as easy as an average PC or Mac laptop from today, especially if you have the intention of keeping the upstream OS (Chrome OS) in the device. People probably would cite a Macbook, a Surface Pro, or something else similar here. For starters, did you see how much they cost, in comparison to an average Chromebook? Such a comparison wouldn\u0026rsquo;t be fair. ","permalink":"https://www.perrotta.dev/2016/09/linux-goodness-in-your-chromebook/","summary":"\u003cp\u003e\u003ca href=\"https://www.google.com.br/chromebook/\"\u003eChromebooks\u003c/a\u003e are excellent for testing\nand playing with Linux userland[1] stuff. And, even better, (almost) every\nchange you make to it can be reset back to its factory state — even in the worst\ncase of completely wiping ChromeOS from your computer. As long as you don\u0026rsquo;t mess\nup with its \u003ca href=\"https://en.wikipedia.org/wiki/Firmware\"\u003efirmware\u003c/a\u003e, you can do\nwhatever you want and still be safe. Most models are cheap, battery life is\namazing for a cheap Linux laptop (8+ hours, depending on the device and on\nusage)[2], Linux support is great (well, it ships with a Linux OS, right?) —\neven for most touchscreen models [citation needed] — and it is simple (as in\nKISS).\u003c/p\u003e","title":"Linux goodness in your Chromebook"},{"content":"What’s Haiku in the first place? It is an operating system, period. What might cause a small surprise is that it is not based either on Linux or on BSD — and yet it is (probably) runnable in your modern computer/laptop!\nWhat I most like on it is that the system is integrated with the GUI, so the end user gets a nice experience. This is not so common as it sounds: most Linux distributions are simply a collection of programs and utilities put together in one place, but they are not necessarily integrated — however, you can integrate them. This makes all the difference between user-friendly and user-centered paradigms1.\nAnyway, Haiku is simple, so for me this post is more a hobby than something useful that I will use in the future; however, I find that knowing about more and more about different operating systems has its own advantages.\nFor more, see https://www.haiku-os.org/about.\nInstalling Haiku Note: most of those instructions come from here. Also, thanks David Couzelis for kindly giving me some advice and pointing me out to them.\nGet Haiku. You can do it either from here, or get nightly releases from here. I personally recommend the nightly releases; I first installed the latest non-nightly one, but later on I discovered that is was very old (circa 2012): it didn’t even have a package manager. Which image should you download? In this post, I am assuming we’ll install Haiku directly to a (real) disk, so I’m downloading the raw image. Actually, the anyboot image can also be downloaded — and this is the one you will get if you opted for a non-nightly version; however, the anyboot image will be converted to a raw one in step 3 before we proceed. So, skip the next step if you downloaded the raw image. Got your anyboot image? Now, convert it to raw (run this from a bash compatible shell): $ dd if=haiku-anyboot.image of=haiku.raw bs=1M skip=$(expr $(od -j 454 -N4 -i -A n haiku-anyboot.image) / 2048) $ dd if=/dev/zero of=haiku.raw bs=1 seek=506 count=4 conv=notrunc Update (2022): These days dd supports status=progress to display the image writing progress.\nNow that we got a raw image, we are writing it directly to a disk partition. First things first: find 3GB or more of free space in your disk. Then create room for a partition in there (for example, with fdisk or gparted). Got it? Now create a partition in there. I’ll assume the partition is /dev/sda42. Please change 42 for the appropriate number in your case. Copy the raw image to the partition (this should be done as ROOT); WARNING: double check the partition and the disk number, otherwise you might lose data. $ dd if=haiku.raw of=/dev/sda42 bs=1M conv=notrunc Make the installation bootable: you’ll need to compile and run the makebootabletiny program, which can be downloaded from here. It is a simple C program, so: $ gcc makebootabletiny.c -o makebootabletiny % ./makebootabletiny /dev/sda42 # this one: as ROOT Make your bootloader know about Haiku. If you’re using grub2z, you can add something such as the following lines to /etc/grub.d/40_custom: menuentry \u0026#34;Haiku OS\u0026#34; { set root(hd0,42) chainloader +1 } Then run as root: grub-mkconfig -o /boot/grub/grub.cfg Done! Now you should be able to boot into Haiku.\nNow what? This post is not a review of Haiku, so I’m stopping here. However, if I write a review about Haiku, I’ll do that from Haiku 🙂.\nI’m just leaving this here: https://www.haiku-os.org/slideshows/haiku-1\nBy the way, Arch Linux is user-centered, which means that you’re supposed to integrate the system as you wish. If you don’t wish that then you’re screwed anyway, so go away 🙂\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2015/04/installing-haiku-from-an-existing-linux/","summary":"\u003ch2 id=\"what8217s-haiku-in-the-first-place\"\u003eWhat’s Haiku in the first place?\u003c/h2\u003e\n\u003cp\u003eIt is an operating system, period. What might cause a small surprise is that it is not based either on Linux or on BSD — and yet it is (probably) runnable in your modern computer/laptop!\u003c/p\u003e","title":"Installing Haiku from an existing linux"},{"content":"Update: I stopped at task 06 on February. Little seems to have stopped responding e-mails. Which is sad, because I was liking those tasks =/\nI am participating in the Eudyptula Challenge, which is not something exactly new, but it is as able to get you out of your comfort zone as if it were1.\nIt puts you into the mind of a Linux Kernel developer, where you are required to write and submit patches and to complete some tasks.\nSubscribing to the challenge is a mini challenge itself2: you must write to little (the superuser behind the thing) an e-mail in plain text. This means: NO HTML. This looks like a silly requirement, but it’s the way how the (real) Linux developers work, and it is actually slightly fancier than you probably imagine. Little himself recommends that the participants don\u0026rsquo;t use either the Gmail web interface or Outlook.\nI intend to document here my own experiences with Eudyptula3.\nSetting up The first thing I went after was a decent and updated Linux distro; doing the challenge within a Linux rather than anything else offers a better experience. Zero effort here, as I’ve already had Arch Linux4 installed.\nNext: a good text editor. Wait…why not two? I am using vim to compose e-mails, and emacs to write code.\nWhat’s next? Oh yes…a decent sendmail program. It is not mandatory to have one of those, but it eases a lot the process of writing e-mail in plain text. I grabbed msmtp plus its MTA. Writing a config file for it is pretty straightforward.\nAnd now I was ready: I subscribed to the challenge.\nThe task #01 Hours later, little5 acknowledged my e-mail and sent me the instructions for the first task.\nI should write my first module for the Linux Kernel.\nA good start. I didn’t even know how to begin, oh no!\nThe task was to write a module that would write ‘Hello World’ to the debug level log of the running kernel when loaded.\nFirst, I had to install some dependencies to be able to compile a couple of C programs. Actually, the base-devel group from Arch already contained most of the things I would need. linux-headers and bc were also relevant.\nMy first module, written in C6, looked like this in the end:\n#include \u0026lt;linux/init.h\u0026gt;; #include \u0026lt;linux/kernel.h\u0026gt;; #include \u0026lt;linux/module.h\u0026gt;; MODULE_AUTHOR(\u0026#34;Thiago Perrotta\u0026#34;); MODULE_DESCRIPTION(\u0026#34;A gentle Hello World module\u0026#34;); MODULE_LICENSE(\u0026#34;GPL\u0026#34;); static int __init hello_init(void) { printk(KERN_DEBUG \u0026#34;Hello world!\\n\u0026#34;); return 0; } static void __exit hello_cleanup(void) { printk(KERN_DEBUG \u0026#34;Goodbye World!\\n\u0026#34;); } module_init(hello_init); module_exit(hello_cleanup); I didn’t get it right in my first trial, of course. In particular, I wrote ‘Hello World’ to the INFO log level, using KERN_INFO, instead of KERN_DEBUG.\nAnd I also needed a Makefile to be able to compile it:\nobj-m += hello.o KERNEL ?= /lib/modules/$(shell uname -r)/build all: make -C $(KERNEL) M=$(PWD) modules clean: make -C $(KERNEL) M=$(PWD) clean The Makefile wasn’t right in my first trial, either. Another requirement of this task was that this file should provide an environment variable, as an alternative location for the build directory of the kernel, opposed to being a hard coded value. I provided one from the beginning, but the way I wrote it was slightly wrong.\nNow, this code didn’t just pop out of my head out of nothing. Although I know how to program in C, I never did kernel programming before. I had to look up for some documentation about how to write a module. It wasn’t super straightforward, because I found many documentation sources about writing modules for the Linux 2.6.x version, which is outdated now7, and the modern version of writing modules is a bit different from it.\nAlso, I was careful not to accidentally find a solution for this task directly. Of course, some people have already done it, and they could have written something about their experiences, as I am doing right now.\nAnyways, after writing those two files and testing that they really worked as expected, I loaded my freshly built module. And yay, a simple dmesg would reveal my hello message out there. It was really cool to see it worked:\nthiago@archpad ~//01 % sudo modinfo hello.ko filename: /home/thiago//01/hello.ko license: GPL description: A gentle Hello World module author: Thiago Perrotta depends: vermagic: 3.17.6-1-ARCH SMP preempt mod_unload modversions After that, I’ve sent everything I should to little, and today I got its reply, about the second task. So, let’s go…\nWhat have I learned with this task? how to write a Makefile to compile a kernel module how to execute shell commands within a Makefile how to use an environment variable within a Makefile how to write e-mails in plain text and send them with msmtp how to send attachments without base64 encoding8 how to load, unload and get information about modules modules are so dynamic and easy to load, huh? Like USB devices. I would never imagine that. It is also a excuse for me to write something here :-)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPun intended.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI can’t write this in one shot. Really. Eudi←yptull←a. EUDYPTULA.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe distro that will conquer the moon.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWhich turned out to be a bot.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDamn, everything will be written in C from now on. That’s what we get with kernel programming, I guess.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOnly lazy or paranoid Linux sysadmins will tell you otherwise 😉\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis turned out to be the hardest thing of this task, yet it wasn’t even part of it.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.perrotta.dev/2015/01/the-eudyptula-challenge/","summary":"Update: I stopped at task 06 on February. Little seems to have stopped responding e-mails. Which is sad, because I was liking those tasks =/\nI am participating in the Eudyptula Challenge, which is not something exactly new, but it is as able to get you out of your comfort zone as if it were1.\nIt puts you into the mind of a Linux Kernel developer, where you are required to write and submit patches and to complete some tasks.","title":"The Eudyptula Challenge"},{"content":" Ebuilds are not evil\n— Larry, the Cow\nFrom now on, this will be my overlay repository: https://github.com/thiagowfx/overlay\nI might change its name in the future, however it will probably remain on GitHub. From my experience with this blog, I realized it would be better to leave a copy of this ebuild here:\n# Copyright 1999-2014 Gentoo Foundation # Distributed under the terms of the GNU General Public License v2 # $Header: $ EAPI=\u0026#34;5\u0026#34; inherit font DESCRIPTION=\u0026#34;A font specially designed for comfortably reading on any computer or device\u0026#34; HOMEPAGE=\u0026#34;http://www.huertatipografica.com/fonts/bitter-ht\u0026#34; SRC_URI=\u0026#34;http://www.fontsquirrel.com/fonts/download/${PN}\u0026#34; LICENSE=\u0026#34;OFL\u0026#34; SLOT=\u0026#34;0\u0026#34; KEYWORDS=\u0026#34;amd64 ~arm ~arm64 ~ppc ~ppc64 x86\u0026#34; IUSE=\u0026#34;\u0026#34; HDEPEND=\u0026#34;app-arch/unzip\u0026#34; FONT_SUFFIX=\u0026#34;otf\u0026#34; S=\u0026#34;${WORKDIR}\u0026#34; src_unpack() { mv \u0026#34;${DISTDIR}/${A}\u0026#34; \u0026#34;${DISTDIR}/${A}.zip\u0026#34; unpack \u0026#34;${A}.zip\u0026#34; } src_install() { FONT_S=\u0026#34;${WORKDIR}\u0026#34; font_src_install } My conclusion? Writing ebuilds is nice, very nice (yeah, more complex than writing PKGBUILDs, I know, but at least this complexity is justified).\nEdit: Thanks Buss for pointing me out a small error (RDEPEND, instead of HDEPEND).\n","permalink":"https://www.perrotta.dev/2014/09/my-first-ebuild/","summary":"Ebuilds are not evil\n— Larry, the Cow\nFrom now on, this will be my overlay repository: https://github.com/thiagowfx/overlay\nI might change its name in the future, however it will probably remain on GitHub. From my experience with this blog, I realized it would be better to leave a copy of this ebuild here:\n# Copyright 1999-2014 Gentoo Foundation # Distributed under the terms of the GNU General Public License v2 # $Header: $ EAPI=\u0026#34;5\u0026#34; inherit font DESCRIPTION=\u0026#34;A font specially designed for comfortably reading on any computer or device\u0026#34; HOMEPAGE=\u0026#34;http://www.","title":"My first ebuild"},{"content":"TL;DR: pequena TODO list sobre como instalar o Gentoo. Você vai passar 90% do seu tempo olhando para texto dando scroll na tela (processo demorado…).\nEsse é o método mais tradicional possível (e que me interessa) que pude constatar. Adapte-o para as suas próprias necessidades:\nBoote em um ambiente com o Arch (na verdade, você pode fazer isso a partir de qualquer distro decente).\nCrie uma partição /, do tipo ext4, para acomodar a instalação do Gentoo. De preferência, coloque um label decente lá. Sugestões de ferramentas para isso: cfdisk ou gparted.\nMonte essa partição em /mnt/gentoo.\nContinue a partir do handbook oficial do Gentoo – nesse caso, para a arquitetura amd64. No entanto, atenção. Use esse guia para perceber quais etapas de instalação e configuração são diferentes (em relação a se estivéssemos instalando pelo método oficial, a partir do live environment do gentoo), mais especificamente da parte 4 à parte 6.\nBaixe o PKGBUILD gentoo-mirrorselect do AUR.\nUse mirrorselect -i -o e então selecione interativamente os mirrors mais próximos da sua localização atual. Copie o valor da variável GENTOO_MIRRORS para o final do seu /mnt/gentoo/etc/portage/make.conf.\nCopie o seu /etc/resolv.conf para /mnt/gentoo/etc/resolv.conf. Isso é importante para ter conectividade dentro do ambiente de chroot.\nCFLAGS que recomendo para o /etc/portake/make.conf (helper):\n-march=native -O2 -pipe A partir daqui, nada especial, apenas continue seguindo o handbook do gentoo, até a parte do bootloader.\nChegando na parte do bootloader, optei por deixar o Arch gerenciá-lo. Nesse caso, basta rodar um típico grub-mkconfig -o /boot/grub/grb.cfg que o Gentoo deverá ser automaticamente detectado (supondo que o pacote os-prober esteja instalado).\nConfigure a rede no Gentoo. Isso é bastante específico, mas o procedimento é bem parecido com a configuração da rede no Arch. A única questão é que, ao dar emerge no wpa_supplicant (no caso de você utilizar Wi-fi), vai demorar bastante até todas as dependências serem instaladas (esse é o ponto principal que me afastou do Gentoo até hoje. Sem pacotes binários, ter que compilar tudo localmente…ao menos, se no final a otimização do sistema for maior, poderia ter valido mais a pena.)\nTeste o Gentoo durante uma semana e diga o que você achou dele 😉\nDecida se você gosta mais do Gentoo ou do Arch. Isso vale tanto para o sistema, tanto para a comunidade. Não tenho ideia de como seja a comunidade do Gentoo (a do Arch eu já tinha uma boa noção de como era mesmo antes de ter o sistema instalado).\nValeu pessoal. Como sou 100% newbie no Gentoo, apreciarei quaisquer dicas! (Enquanto as dicas não aparecem, Wiki Pages e Forums Threads me esperam). Sabe, é bom, de vez em quando, ser newbie em alguma coisa. Claro que ser expert / muito bom / hacker em algumas aplicações é ótimo, mas eu acredito na importância de possuir uma boa base de conhecimento em geral (não necessariamente apenas para fins acadêmicos ou financeiros, mas também para satisfazer a mente: desafios são importantes!).\nAh, mais dois comentários:\nTerminando de escrever esse post e de editá-lo, a compilação dos (130) pacotes que o wpa_supplicant puxou não chegou nem na metade… (eu teria instalado uns três ou quatro Archs automatizados nesse tempo – tá bom, não vale comparar pacotes binários com código-fonte, eu sei)\nEsse é o meu terceiro ou quarto post utilizando o org2blog (do emacs). Já ficou bastante claro para mim que vou passar a usá-lo de maneira fixa. O único problema é que eu ainda não sei a sintaxe de formatação dele direito, é muita informação. Não é nem que não seja intuitiva, mas já tem LaTeX, BBCode, Markdown, aí eu tenho que aprender mais uma markup language. Mas, provavelmente vou me submeter a isso, o orgmode é sensacional para management em geral.\n","permalink":"https://www.perrotta.dev/2014/05/instalando-o-gentoo-a-partir-do-arch/","summary":"\u003cp\u003e\u003cstrong\u003eTL;DR\u003c/strong\u003e: pequena TODO list sobre como instalar o Gentoo. Você vai passar 90% do seu tempo olhando para \u003cstrong\u003etexto\u003c/strong\u003e dando scroll na tela (processo demorado…).\u003c/p\u003e","title":"Instalando o Gentoo a partir do Arch"},{"content":" Meu computador não boota! E agora? Possíveis sintomas: tela preta congelada, tela de splash congelada, systemd travado, upstart travado, corrupção (fsck não ficou satisfeito), bootloader (grub, syslinux, EFI) mal configurado (ou não configurado) Obter uma distro de Linux e gravá-la num USB Flash Drive (pendrive). Meu gosto pessoal: System rescue cd, Parted magic, Slitaz ou Arch Linux. Bootar a distro e imediatamente abrir um console / emulador de terminal. Com interface gráfica ou não, a gosto. fdisk -l para detectar os discos do computador. Detectar o disco cujo sistema está com problema. Geralmente o que contém a partição /boot ou /. Identificar as partições também é usualmente importante. Se você precisar formatar alguma partição, use cfdisk ou parted. Exemplo: cfdisk /dev/sda1. Se você precisar (re)criar algum filesystem, use mkfs (por exemplo, mkfs.ext4 -L \u0026quot;archroot\u0026quot; /dev/sda1). Para (re)montar o seu sistema de arquivos: (por exemplo) mount /dev/sda1 /mnt. chroot no sistema que você acabou de montar: chroot /mnt. Para recuperar (na verdade, gerar novamente) o arquivo de configuração do grub dentro do chroot: grub-mkconfig -o /boot/grub/grub.cfg. Para reinstalar o grub (fora do chroot!), use grub-install. Explorar o diretório /etc/systemd/system. Usualmente um desses passos é um caminho para resolver o problema. No final das contas, as coisas são bastante específicas, dependem do contexto.\n","permalink":"https://www.perrotta.dev/2014/04/recovery-t%C3%ADpico-via-usb/","summary":"Meu computador não boota! E agora? Possíveis sintomas: tela preta congelada, tela de splash congelada, systemd travado, upstart travado, corrupção (fsck não ficou satisfeito), bootloader (grub, syslinux, EFI) mal configurado (ou não configurado) Obter uma distro de Linux e gravá-la num USB Flash Drive (pendrive). Meu gosto pessoal: System rescue cd, Parted magic, Slitaz ou Arch Linux. Bootar a distro e imediatamente abrir um console / emulador de terminal. Com interface gráfica ou não, a gosto.","title":"Recovery típico via USB"},{"content":"Não imagino que seja incomum o seguinte cenário:\nNews: uma nova versão da distro Debisuse está disponível. Usuário: vou baixar a ISO, criar uma máquina virtual no VirtualBox (ou no VMWare, vai que), Bootar a ISO a partir dela. Isso tudo é muito mais prático do que gravar a ISO num Flash (Pen) Drive e então testá-la com um novo boot. No entanto, podemos ser mais práticos ainda se utilizarmos, para isso, um único comando, com o qemu! O comando típico é:\nqemu-system-x86_64 --enable-kvm -m 512M -cdrom ~/Downloads/debisuse-latest.iso Se sua arquitetura for de 32 bits, você vai querer qemu-system-i386. O parâmetro m regula a quantidade de memória a ser alocada para a máquina virtual.\nPara utilizar o QEMU, você vai precisar encontrar o pacote adequado na sua distro.\nNo Debian/Ubuntu e openSUSE: qemu e kvm (não testei, mas tudo indica que são esses) No Arch: qemu (veja https://wiki.archlinux.org/index.php/Kvm e https://wiki.archlinux.org/index.php/QEMU) OBS.: O KVM é para deixar a execução ainda mais rápida. Mas ele não é obrigatório, OK? Para poder usá-lo existe uma série de peculiaridades, tais como habilitar as opções de virtualização na sua BIOS (isso é mais comum em laptops) e assegurar-se de que o módulo adequado do kernel foi carregado (geralmente kvm\\_intel ou kvm\\_amd). Você pode conferir isso com o comando:\nlsmod | grep kvm Se ver alguma saída, existem boas chances de o módulo correto do KVM já ter sido carregado pelo seu kernel. Para fins de comparação, essa é a minha saída:\nkvm_intel 131191 3 kvm 388773 1 kvm_intel Happy hacking!\n","permalink":"https://www.perrotta.dev/2014/01/testando-uma-iso-no-linux-sem-o-virtualbox/","summary":"Não imagino que seja incomum o seguinte cenário:\nNews: uma nova versão da distro Debisuse está disponível. Usuário: vou baixar a ISO, criar uma máquina virtual no VirtualBox (ou no VMWare, vai que), Bootar a ISO a partir dela. Isso tudo é muito mais prático do que gravar a ISO num Flash (Pen) Drive e então testá-la com um novo boot. No entanto, podemos ser mais práticos ainda se utilizarmos, para isso, um único comando, com o qemu!","title":"Testando uma ISO no Linux sem o VirtualBox"}]